---
layout: archive
title: Zulip Chat Archive
permalink: /stream/270676-lean4/topic/Data-level.20parallelism.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/index.html">lean4</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html">Data-level parallelism</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="291835439"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291835439" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Andrés Goens <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291835439">(Aug 03 2022 at 11:35)</a>:</h4>
<p>Is there anything out there for data-level parallelism? E.g. some kind of parallel map (map/reduce style) or similar? The way I've found so far is to use <code>Task</code>, which from <a href="#narrow/stream/270676-lean4/topic/Another.20IO.20Task.20question">this discussion</a> sounds like it would be a lot of overhead to spawn a new task for every single computation in a <code>map</code>.</p>



<a name="291835991"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291835991" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Sebastian Ullrich <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291835991">(Aug 03 2022 at 11:40)</a>:</h4>
<p>That's a pretty long thread, can you be more specific?</p>



<a name="291836979"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291836979" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Andrés Goens <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291836979">(Aug 03 2022 at 11:49)</a>:</h4>
<p><span class="user-mention silent" data-user-id="110024">Sebastian Ullrich</span> <a href="#narrow/stream/270676-lean4/topic/Data-level.20parallelism/near/291835991">said</a>:</p>
<blockquote>
<p>That's a pretty long thread, can you be more specific?</p>
</blockquote>
<p>Haha, indeed it is! I can try to be more specific: From the discussion it seems that if we have n <code>Task</code>s, then the runtime system will spawn <code>n</code> worker threads. My first thought was (without testing it though) that this would not be very efficient if we wanted to run a computation (like a <code>map</code>) on a very large <code>Array</code>, as the overhead of spawning a new thread for every value would not be worth it. Is that wrong, e.g. does the runtime system not spawn OS threads for that but rather be much more lightweight and it might be worth it?</p>
<p>There's obviously other ways around this otherwise, like splitting the computation manually into m <code>Task</code>s, but I wanted to ask if there was any code out there that had done something like this</p>



<a name="291837297"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291837297" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Sebastian Ullrich <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291837297">(Aug 03 2022 at 11:52)</a>:</h4>
<p>That's not true, non-dedicated tasks (the default) are backed by a thread pool bounded by the number of hardware threads</p>



<a name="291841879"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291841879" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> James Gallicchio <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291841879">(Aug 03 2022 at 12:34)</a>:</h4>
<p>for the collections library, I plan to implement parallelized versions of map/whatever that chunk the data appropriately to minimize the overhead; I think even with a dedicated thread pool you'd want some granularity</p>



<a name="291841954"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291841954" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> James Gallicchio <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291841954">(Aug 03 2022 at 12:34)</a>:</h4>
<p>(but, very happy to hear that the overhead will not be the same as spawning a system thread)</p>



<a name="291850103"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291850103" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuri de Wit <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291850103">(Aug 03 2022 at 13:34)</a>:</h4>
<p>Curious: is there a <code>Channel</code> abstraction to go with <code>Task</code>?</p>



<a name="291882928"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291882928" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Andrés Goens <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291882928">(Aug 03 2022 at 17:19)</a>:</h4>
<p>For future reference, I did some experimenting/measuring around and the performance is pretty decent even for a lot of threads and small-ish computation! Here's a version of this naive parallel map that I described</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="kd">def</span> <span class="n">parallelMap</span> <span class="o">:</span> <span class="o">(</span><span class="n">α</span> <span class="bp">→</span> <span class="n">β</span><span class="o">)</span> <span class="bp">→</span> <span class="n">Array</span> <span class="n">α</span> <span class="bp">→</span> <span class="n">IO</span> <span class="o">(</span><span class="n">Array</span> <span class="n">β</span><span class="o">)</span>
  <span class="bp">|</span> <span class="n">f</span><span class="o">,</span> <span class="n">as</span> <span class="bp">=&gt;</span>
  <span class="k">let</span> <span class="n">ts</span> <span class="o">:=</span> <span class="n">as.map</span> <span class="bp">λ</span> <span class="n">a</span> <span class="bp">=&gt;</span> <span class="n">Task.spawn</span> <span class="o">(</span><span class="bp">λ</span> <span class="n">_</span> <span class="bp">=&gt;</span> <span class="n">f</span> <span class="n">a</span><span class="o">)</span><span class="bp">;</span>
  <span class="k">let</span> <span class="n">rs</span> <span class="o">:=</span> <span class="n">ts.map</span> <span class="n">Task.get</span><span class="bp">;</span>
<span class="n">return</span> <span class="n">rs</span>
</code></pre></div>
<p>Using that I tested the following setup on an x86 machine with 24 HW threads and compared the two variants:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="kd">def</span> <span class="n">someComputation</span> <span class="o">:</span> <span class="n">Nat</span> <span class="bp">→</span> <span class="n">Nat</span>
 <span class="bp">|</span> <span class="n">n</span> <span class="bp">=&gt;</span> <span class="n">List.range</span> <span class="n">n</span> <span class="bp">|&gt;.</span><span class="n">foldl</span> <span class="o">(</span><span class="n">init</span> <span class="o">:=</span> <span class="mi">42</span><span class="o">)</span> <span class="o">(</span><span class="bp">λ</span> <span class="n">x</span> <span class="n">y</span> <span class="bp">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="bp">+</span><span class="mi">1</span><span class="o">)</span> <span class="bp">*</span> <span class="o">(</span><span class="n">y</span><span class="bp">+</span><span class="mi">1</span><span class="o">))</span>

<span class="kd">def</span> <span class="n">main</span> <span class="o">:</span> <span class="n">IO</span> <span class="n">Unit</span> <span class="o">:=</span> <span class="k">do</span>
  <span class="k">let</span> <span class="n">foo</span> <span class="o">:=</span> <span class="n">List.range</span> <span class="mi">10000</span> <span class="bp">|&gt;.</span><span class="n">toArray</span> <span class="bp">|&gt;.</span><span class="n">map</span> <span class="n">someComputation</span>
  <span class="c">/-</span><span class="cm"></span>
<span class="cm">  real  1m21.293s</span>
<span class="cm">  user  1m21.244s</span>
<span class="cm">  sys   0m0.036s</span>
<span class="cm">  -/</span>
  <span class="k">let</span> <span class="n">foo</span> <span class="bp">&lt;-</span> <span class="n">List.range</span> <span class="mi">10000</span> <span class="bp">|&gt;.</span><span class="n">toArray</span> <span class="bp">|&gt;</span> <span class="n">parallelMap</span> <span class="n">someComputation</span>
  <span class="c">/-</span><span class="cm"></span>
<span class="cm">  real  0m6.240s</span>
<span class="cm">  user  2m22.260s</span>
<span class="cm">  sys   0m0.372s</span>
<span class="cm">  -/</span>
  <span class="n">IO.println</span> <span class="n">s</span><span class="bp">!</span><span class="s2">"res: {foo.size}"</span>
</code></pre></div>
<p>That's a speedup of almost 14x (in a 24 thread machine, for this compute-bound problem we can probably think of HW threads as cores), which is surprisingly good.  Or put differently, there's factor ~1.7 in terms of CPU (user) time, which for this embarrassingly parallel problem should roughly correspond to the thread overhead (of ~70%).  With a nicely chunked version like <span class="user-mention" data-user-id="407274">@James Gallicchio</span> 's upcoming one this should be even better <span aria-label="wink" class="emoji emoji-1f609" role="img" title="wink">:wink:</span></p>



<a name="291979742"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291979742" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Tomas Skrivan <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291979742">(Aug 04 2022 at 08:38)</a>:</h4>
<p>Would it be possible to have <code>parallelMap</code> outside of <code>IO</code> monad? And other parallel algorithms too? If you are careful, you can make sure they are fully deterministic i.e. the result does not depend on the order of thread execution.</p>



<a name="291979891"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291979891" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Sebastian Ullrich <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291979891">(Aug 04 2022 at 08:40)</a>:</h4>
<p>It does look like the code is using the monad for <code>return</code> only :) ...</p>



<a name="291980128"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291980128" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Sebastian Ullrich <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291980128">(Aug 04 2022 at 08:43)</a>:</h4>
<p>For the sake of completeness, the main optimization we're currently missing regarding task overhead is local queues with work stealing. All scheduling inside the current thread pool goes through a big global mutex.</p>



<a name="291980141"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291980141" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Andrés Goens <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291980141">(Aug 04 2022 at 08:43)</a>:</h4>
<p>Oh, I've no idea why I thought <code>Task</code> lived in <code>IO</code>,  just dropping them works indeed, thanks for that observation!</p>



<a name="291980392"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291980392" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Tomas Skrivan <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291980392">(Aug 04 2022 at 08:46)</a>:</h4>
<p>Ohh I also thought that <code>Task</code> lives in <code>IO</code> :)</p>



<a name="291984410"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/291984410" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Gabriel Ebner <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#291984410">(Aug 04 2022 at 09:31)</a>:</h4>
<p><span class="user-mention silent" data-user-id="463095">Yuri de Wit</span> <a href="#narrow/stream/270676-lean4/topic/Data-level.20parallelism/near/291850103">said</a>:</p>
<blockquote>
<p>Curious: is there a <code>Channel</code> abstraction to go with <code>Task</code>?</p>
</blockquote>
<p>Soon.. <a href="https://github.com/gebner/lean4/blob/10983d956cca532fcd3bdd6e7cd30ee1b87532de/src/Lean/Mutex.lean">https://github.com/gebner/lean4/blob/10983d956cca532fcd3bdd6e7cd30ee1b87532de/src/Lean/Mutex.lean</a></p>



<a name="292031343"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/292031343" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuri de Wit <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#292031343">(Aug 04 2022 at 16:23)</a>:</h4>
<p><span class="user-mention" data-user-id="110043">@Gabriel Ebner</span> re: <code>Mutex.lean</code>, a notice a few patterns (?) I don't understand.</p>
<p>Consider:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="kn">private</span> <span class="n">opaque</span> <span class="n">BaseMutexImpl</span> <span class="o">:</span> <span class="n">NonemptyType.</span><span class="o">{</span><span class="mi">0</span><span class="o">}</span>

<span class="kd">def</span> <span class="n">BaseMutex</span> <span class="o">:</span> <span class="kt">Type</span> <span class="o">:=</span> <span class="n">BaseMutexImpl.type</span>
</code></pre></div>
<ol>
<li>What is <code>BaseMutexImpl.type</code>? It seems that elaboration is adding this to the environment when using opaque, but why?</li>
<li>Why the indirection from <code>BaseMutex</code> to <code>BaseMutexImpl.type</code>? The same is used in Condvar.</li>
<li>I see that BaseMutexImpl is generated as a lean_box(0): is this just so it can be threaded through the API calls like "World" is in main?</li>
</ol>
<p>(sorry for the tangent in this thread)</p>



<a name="292046032"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/292046032" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Gabriel Ebner <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#292046032">(Aug 04 2022 at 17:49)</a>:</h4>
<p>That's just the boilerplate required for FFI types.  What <code>opaque BaseMutexImpl : NonemptyType</code> does is declare a constant of type <code>{ α : Type // Nonempty α }</code>, i.e. a type about which we only know a single thing, namely that it is nonempty.  (We need this so that we can declare <code>opaque BaseMutex.new : BaseIO BaseMutex</code> later: for consistency Lean needs to check that <code>BaseIO BaseMutex</code> is nonempty, which is only true if <code>BaseMutex</code> is nonempty.)</p>



<a name="292046089"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/292046089" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Gabriel Ebner <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#292046089">(Aug 04 2022 at 17:49)</a>:</h4>
<p>The reason why we don't write <code>opaque BaseMutex : NonemptyType</code> is because we don't want to see/type <code>BaseMutex.type</code> afterwards.</p>



<a name="292046369"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/292046369" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Gabriel Ebner <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#292046369">(Aug 04 2022 at 17:52)</a>:</h4>
<blockquote>
<p>I see that BaseMutexImpl is generated as a lean_box(0)</p>
</blockquote>
<p>This is true for every type, and it's called erasure.  For example if you write <code>def foo := (Nat, 42)</code> then the first component will also be the scalar value 0 (which is generated with the C code <code>lean_box(0)</code>).</p>



<a name="292047349"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/292047349" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuri de Wit <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#292047349">(Aug 04 2022 at 17:59)</a>:</h4>
<p><span class="user-mention silent" data-user-id="110043">Gabriel Ebner</span> <a href="#narrow/stream/270676-lean4/topic/Data-level.20parallelism/near/292046369">said</a>:</p>
<blockquote>
<blockquote>
<p>I see that BaseMutexImpl is generated as a lean_box(0)</p>
</blockquote>
<p>This is true for every type, and it's called erasure.  For example if you write <code>def foo := (Nat, 42)</code> then the first component will also be the scalar value 0 (which is generated with the C code <code>lean_box(0)</code>).</p>
</blockquote>
<p>Ok. So there is still a tuple with two slots at runtime. It is just that the first one will be lean_object(0)? I was naively assuming that erasure meant that it would completely disappear from the runtime (e.g. a function with 2 parameters where one param is erased would become a function with 1 parameter at runtime).</p>



<a name="292048566"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/270676-lean4/topic/Data-level%20parallelism/near/292048566" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Gabriel Ebner <a href="https://leanprover-community.github.io/archive/stream/270676-lean4/topic/Data-level.20parallelism.html#292048566">(Aug 04 2022 at 18:09)</a>:</h4>
<p>No, the memory representation of an inductive does not depend on the parameter (<code>Prod α β</code> has the same memory representation no matter what <code>α</code> and <code>β</code> are).  Otherwise you'd run into huge and ugly issues with polymorphism.  (Should <code>def blah : α × β → α</code> be compiled differently depending on <code>α</code>/<code>β</code>, or get a flag?  And what about <code>def hmm : (α : Type) × (α × Nat) → Nat := fun ⟨_, _, n⟩ =&gt; n</code>?)</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>