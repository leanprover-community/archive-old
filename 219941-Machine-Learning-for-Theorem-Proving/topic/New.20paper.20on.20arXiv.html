---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html">New paper on arXiv</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="267306286"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267306286" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jeremy Avigad <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267306286">(Jan 08 2022 at 17:32)</a>:</h4>
<p>Someone pointed me to this paper posted on arXiv:<br>
<a href="https://arxiv.org/abs/2112.15594">https://arxiv.org/abs/2112.15594</a><br>
The abstract begins: "We demonstrate that a neural network pre-trained on text and fine-tuned on code solves Mathematics problems by program synthesis." It ends: "This is the first work to automatically solve, grade, and generate university-level Mathematics course questions at scale. This represents a milestone for higher education."<br>
I haven't read it yet.</p>



<a name="267306565"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267306565" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267306565">(Jan 08 2022 at 17:39)</a>:</h4>
<p>Comments:   128 pages, 250 tables</p>



<a name="267306818"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267306818" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Zygimantas Straznickas <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267306818">(Jan 08 2022 at 17:45)</a>:</h4>
<p>Reddit discussion about the paper: <a href="https://www.reddit.com/r/MachineLearning/comments/rutbpv/r_a_neural_network_solves_and_generates/">https://www.reddit.com/r/MachineLearning/comments/rutbpv/r_a_neural_network_solves_and_generates/</a> .</p>



<a name="267308073"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267308073" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267308073">(Jan 08 2022 at 18:13)</a>:</h4>
<p>It’s really only 8 pages with a really long appendix made up of what looks like example math problems.</p>



<a name="267349875"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267349875" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Kunhao Zheng <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267349875">(Jan 09 2022 at 09:57)</a>:</h4>
<p>They used Codex to translate math problems (maybe heavy prompt engineering here, they didn't fine-tune the model) into Python code and executed the code to get the answers. <br>
(Background: One can hint Codex with some descriptions or docs of a function, and let it generate the function that satisfies the descriptions.) <br>
Here the descriptions happens to be the math problems itself.<br>
The same idea of auto-formalization can be also tried on lean: hinting the Codex the math problems and let it generate lean code w/ proof or only a <code>sorry</code> there.</p>



<a name="267356863"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267356863" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Kunhao Zheng <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267356863">(Jan 09 2022 at 12:55)</a>:</h4>
<p>I remember reading another quite similar paper on arXiv months ago. Found it here:</p>
<blockquote>
<p>Solving Probability and Statistics Problems by Program Synthesis<br>
<a href="https://arxiv.org/pdf/2111.08267.pdf">https://arxiv.org/pdf/2111.08267.pdf</a></p>
</blockquote>
<p>It doesn't surprise me much that it's from the same group of people.</p>



<a name="267378456"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267378456" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Junyan Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267378456">(Jan 09 2022 at 21:23)</a>:</h4>
<p>On p.4 it says</p>
<blockquote>
<p>Interaction. he original question may not be a prompt that synthesizes a program whose execution results in the correct answer. In addition, the answer may require multiple steps with clear plots or other modalities. We therefore may interactively prompt Codex until reaching the correct answer or visualizations, making the minimum necessary changes from the original question. Panel D in Figure 2 shows an example of interaction to produce multiple plots.</p>
</blockquote>
<p>So this step (prompt generation) isn't quite automated yet.</p>



<a name="267379450"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267379450" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Junyan Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267379450">(Jan 09 2022 at 21:48)</a>:</h4>
<p>This paragraph</p>
<blockquote>
<p>C. Questions and Prompts.We classify the transformations from the original course questions to the Codex prompts resulting in correct solutions into the following three classes: (i) As-is prompt: Original question and Codex prompt are the same, (ii) Automatic prompt transformation: Original question and Codex prompt are different, and the Codex prompt is generated automatically by Codex itself, (iii) Manual prompt transformation: Original question and Codex prompt are different, and the Codex prompt is generated by a human.</p>
</blockquote>
<p>as <a href="https://www.reddit.com/r/MachineLearning/comments/rutbpv/comment/hr33znc/?utm_source=share&amp;utm_medium=web2x&amp;context=3">quoted on reddit</a>, appears <a href="https://arxiv.org/abs/2112.15594v1">in the first version</a> of the paper but no longer in the latest version. Not sure what changed from version to version.</p>



<a name="267381756"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/267381756" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Michael R Douglas <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#267381756">(Jan 09 2022 at 22:46)</a>:</h4>
<p>Iddo Drori, lead author on these papers, will speak in the New Technologies seminar on March 9.</p>



<a name="274735450"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/274735450" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Junyan Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#274735450">(Mar 09 2022 at 18:59)</a>:</h4>
<p>starting in 30min</p>



<a name="274736400"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/274736400" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#274736400">(Mar 09 2022 at 19:04)</a>:</h4>
<p>Actually it has already started right now.</p>



<a name="274737777"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/274737777" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Junyan Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#274737777">(Mar 09 2022 at 19:13)</a>:</h4>
<p>Oh, my mistake, thanks</p>



<a name="274746168"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/274746168" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#274746168">(Mar 09 2022 at 20:17)</a>:</h4>
<p><span class="user-mention" data-user-id="201575">@Michael R Douglas</span> The speaker said he would be able to give a citation for their work on proof synthesis.  Would you be able to follow up with him and post it here?  (He said it was on arXiv, but I can't seem to find any examples of proofs in say <a href="https://arxiv.org/pdf/2112.15594.pdf">https://arxiv.org/pdf/2112.15594.pdf</a>, although it is possible I'm just not searching for the correct keywords or looking in the right papers.)</p>



<a name="283101195"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/283101195" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Adam Topaz <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#283101195">(May 20 2022 at 17:22)</a>:</h4>
<p>Not really lean related, but might still be interesting for folks in this stream:<br>
<a href="https://arxiv.org/abs/2203.08913">https://arxiv.org/abs/2203.08913</a></p>



<a name="283101476"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/283101476" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Adam Topaz <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#283101476">(May 20 2022 at 17:25)</a>:</h4>
<p>Related HN discussion: <a href="https://news.ycombinator.com/item?id=31448360">https://news.ycombinator.com/item?id=31448360</a></p>



<a name="293461225"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/293461225" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#293461225">(Aug 15 2022 at 03:56)</a>:</h4>
<p>This paper was <a href="https://www.pnas.org/doi/full/10.1073/pnas.2123433119">recently published</a> in PNAS (which is similar to Science or Nature).  I haven’t looked at the new version of the paper but hopefully thorough review helped clarify a lot of the iffy and too-good-to-be-true sounding results from the original arXiv paper.</p>



<a name="311753168"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/311753168" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Adam Topaz <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#311753168">(Nov 23 2022 at 04:33)</a>:</h4>
<p>Just mentioning the following paper I came across, in case anyone finds it interesting:<br>
<a href="https://arxiv.org/abs/2211.08671">https://arxiv.org/abs/2211.08671</a></p>



<a name="312249498"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/New%20paper%20on%20arXiv/near/312249498" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Junyan Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/New.20paper.20on.20arXiv.html#312249498">(Nov 25 2022 at 19:46)</a>:</h4>
<p>This isn't an arXiv paper and was published a while ago, but still:<br>
<a href="https://www.pnas.org/doi/10.1073/pnas.2115730119">Socially situated artificial intelligence enables learning from human interaction</a><br>
From the abstract (emphasis mine):</p>
<blockquote>
<p>Humans have long demonstrated an ability to learn from interactions with others. However, AI agents learn in <strong>social isolation</strong>. To create intelligent systems that understand more than a fixed slice of the world, our article formalizes socially situated AI—a framework that enables agents to interact with people as they simultaneously learn <strong>new concepts</strong> about the world around them. Using our framework, we deploy a field experiment on a photo-sharing social network where our agent interacts with hundreds of thousands of people to learn concepts about the visual world. We combine advances in deep learning, <strong>computer vision, natural language processing, and human–computer Interaction</strong> to deliver a human-centered AI that learns from interactions with people in social environments.</p>
</blockquote>
<blockquote>
<p>We manifest our framework as an interactive agent that learns how to ask natural language questions about photos as it broadens its visual intelligence on a large photo-sharing social network. Unlike active-learning methods, which implicitly assume that humans are oracles willing to answer any question, our agent adapts its behavior based on <strong>observed norms of which questions people are or are not interested to answer</strong>. Through an <strong>8-month</strong> deployment where our agent <strong>interacted with 236,000 social media users</strong>, our agent improved its performance at recognizing new visual information by 112%. A controlled field experiment confirmed that our agent outperformed an active-learning baseline by 25.6%.</p>
</blockquote>
<p>I think the method could be very relevant when it comes to accelerating AI's absorption of mathematical and scientific knowledge. It could open an avenue of teaching mathematics to machines other than formalizing/mathlib building/simp lemma and tactic writing.</p>
<p>Meta AI has recently released Galactica and <a href="https://www.science.org/doi/10.1126/science.ade9097">Cicero</a>, and it would be wonderful if we could combine them :) Galactica has apparently generated a lot of activities during the short time it was online, and I think people are willing to hold long form conversations with AI if it could conceivably pay back; if one mathematician demonstrates interest/curiosity about one math topic during conversation, AI may be motivated to acquire it from another person knowledgeable about the topic. Some account system may be needed to keep track of every correspondent's preferences, so as to communicate in a personalized way and avoid triggering adverse reactions (AI is now probably good enough to infer sentiments but an upvote/downvote system could also be nice to have).</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>