---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html">PACT Paper</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="235899462"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/235899462" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#235899462">(Apr 23 2021 at 19:54)</a>:</h4>
<p>Hi, I'm reviewing the <a href="https://arxiv.org/abs/2102.06203">PACT paper</a> and I'd love to clarify a few questions I have. <span class="user-mention" data-user-id="115715">@Jason Rute</span> <span class="user-mention" data-user-id="116045">@Jesse Michael Han</span> </p>
<p>1 - For the diagram in Figure 1, how are the expressions in the boxes generated? Also I understand the tactic proof and I can see how the proof term it generates corresponds to it and proves the theorem, but what exactly are the arrows in the syntax tree pointing at? </p>
<p>2 - Section 3.2 describes a recursion into the structure of each proof term. Are the sub-terms simply exprs within the expr of the whole proof term? i.e. this would imply that it is a recursion into an expr-tree as defined <a href="https://github.com/leanprover-community/lean/blob/65ad4ffdb3abac75be748554e3cbe990fb1c6500/library/init/meta/expr.lean#L85">here</a>?</p>



<a name="235907043"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/235907043" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jesse Michael Han <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#235907043">(Apr 23 2021 at 20:55)</a>:</h4>
<p>re: 2, it's a recursion through the proofterm <code>expr</code>, but it's not quite <code>expr.mfold</code> --- we skip lambda-abstractions and instead accumulate them in a list of binders for creating synthetic tactic states later on</p>
<p>in the <code>lean-step</code> codebase the core function is <code>expr.mfold_with_binders</code></p>



<a name="235966494"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/235966494" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#235966494">(Apr 24 2021 at 12:25)</a>:</h4>
<p>re 1: I agree it is a confusing picture.   Each edge represents a context + goal and the child tree of that edge is the term which completes that goal.  So let's work from the top.  Here is the tactic proof:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="kd">lemma</span> <span class="n">sub_ne_zero_of_ne</span> <span class="o">(</span><span class="n">a</span> <span class="n">b</span> <span class="o">:</span> <span class="n">int</span><span class="o">)</span> <span class="o">(</span><span class="n">h</span> <span class="o">:</span> <span class="n">a</span> <span class="bp">≠</span> <span class="n">b</span><span class="o">)</span> <span class="o">:</span> <span class="n">a</span> <span class="bp">-</span> <span class="n">b</span> <span class="bp">≠</span> <span class="mi">0</span> <span class="o">:=</span>
<span class="kd">begin</span>
  <span class="n">intro</span> <span class="n">hab</span><span class="o">,</span>
  <span class="n">apply</span> <span class="n">h</span><span class="o">,</span>
  <span class="n">apply</span> <span class="n">int.eq_of_sub_eq_zero</span> <span class="n">hab</span><span class="o">,</span>
  <span class="n">tactic.trace_result</span>
<span class="kd">end</span>
</code></pre></div>
<p>I add the <code>tactic.trace_result</code> at the end so we can see what the proof being generated is.  It is:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="n">id</span> <span class="o">(</span><span class="bp">λ</span> <span class="o">(</span><span class="n">hab</span> <span class="o">:</span> <span class="n">a</span> <span class="bp">-</span> <span class="n">b</span> <span class="bp">=</span> <span class="mi">0</span><span class="o">),</span> <span class="n">h</span> <span class="o">(</span><span class="n">int.eq_of_sub_eq_zero</span> <span class="n">hab</span><span class="o">))</span>
</code></pre></div>
<p>Ignore the extra <code>id</code> so we get </p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">λ</span> <span class="o">(</span><span class="n">hab</span> <span class="o">:</span> <span class="n">a</span> <span class="bp">-</span> <span class="n">b</span> <span class="bp">=</span> <span class="mi">0</span><span class="o">),</span> <span class="n">h</span> <span class="o">(</span><span class="n">int.eq_of_sub_eq_zero</span> <span class="n">hab</span><span class="o">)</span>
</code></pre></div>
<p>Now, like in any mathematical formalism, this expression is a tree.  That is the tree drawn in figure one.  Specifically, the constructors are <code>lam</code> for  lambda abstraction and <code>app</code> for function application, so we can write this expression as something like</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="o">(</span> <span class="n">lam</span> <span class="o">(</span><span class="n">hab</span> <span class="o">:</span> <span class="n">a</span> <span class="bp">-</span> <span class="n">b</span> <span class="bp">=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">(</span> <span class="n">app</span> <span class="n">h</span> <span class="o">(</span> <span class="n">app</span> <span class="n">int.eq_of_sub_eq_zero</span> <span class="n">hab</span> <span class="o">)</span> <span class="o">)</span> <span class="o">)</span>
</code></pre></div>
<p>Now, for every subexpression (or at least the ones in the body of the lambda and on both sides of the application), we have a context and goal.  The context is what variables have occurred in outer lambda binders and the goal is the type of the subexpression.  These form the edges of the tree.</p>



<a name="235966502"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/235966502" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#235966502">(Apr 24 2021 at 12:25)</a>:</h4>
<p>Does that make sense?</p>



<a name="235972386"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/235972386" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#235972386">(Apr 24 2021 at 14:05)</a>:</h4>
<p><span class="user-mention" data-user-id="116045">@Jesse Michael Han</span> Thanks - I'm going to dig into the code</p>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> Ahh yes that makes sense. Thank you for the clarification.</p>



<a name="236398153"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/236398153" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#236398153">(Apr 27 2021 at 19:54)</a>:</h4>
<p><span class="user-mention" data-user-id="116045">@Jesse Michael Han</span> Is it fair to say that the PACT data pipeline as described in Section 3.2 is best described by <code>lean_step_main_core_aux</code>? I've been reading the code and I noticed that <code>mfold_with_binders</code> is used only in <code>gather_used_premises</code> as a step before starting <code>lean_step_main_core</code>.</p>



<a name="236415445"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/236415445" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jesse Michael Han <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#236415445">(Apr 27 2021 at 21:43)</a>:</h4>
<p>oops yeah, <code>mfold_with_binders</code> was the first implementation of the idea but <code>lean_step_main_core_aux </code> is the more general version that gathers all the data i needed at once</p>



<a name="236551084"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/236551084" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#236551084">(Apr 28 2021 at 17:57)</a>:</h4>
<p>The paper mentions the use of the fairseq API:</p>
<p>"The fairseq backend supports querying a locally hosted Transformer via the Fairseq CLI (Ott et al., 2019), returning a list of candidates found by beam search, ranked by cumulative log-probabilities."</p>
<p>I'm assuming you trained some models using the fairseq API as a quick baseline to compare against OpenAI's models. I'm assessing different libraries for training LMs, and I'm getting mixed signals about fairseq. I'm curious if you had problems with it or if you would recommend it. Thanks!</p>



<a name="236660014"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/236660014" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#236660014">(Apr 29 2021 at 11:52)</a>:</h4>
<p><span class="user-mention" data-user-id="240875">@Yuhuai Tony Wu</span> would know best about FairSeq.</p>



<a name="236660703"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/236660703" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#236660703">(Apr 29 2021 at 11:59)</a>:</h4>
<p>If your question is more generally which LM library to use for generating Lean term proofs, I generally don’t know enough from the practical side to be super helpful, but one concern I think you should consider is whether to start with a pretrained model.  I think it is fairly clear now that pretrained models, especially if they contain math, but even if they don’t, are good starting points.  This will of course limit the options of models and libraries you can use and limit your ability to do domain specific encoding.  Also, if you work with a model trained on the Pile or another dataset containing GitHub, you should double check if any Lean repos are in that dataset.</p>



<a name="236669788"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/236669788" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#236669788">(Apr 29 2021 at 13:11)</a>:</h4>
<p>Thanks, definitely planning to use a pre-trained model. Was planning to pre-train on LIME (<a href="https://arxiv.org/abs/2101.06223">https://arxiv.org/abs/2101.06223</a>) also.</p>



<a name="239632988"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/239632988" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Brando Miranda <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#239632988">(May 20 2021 at 17:52)</a>:</h4>
<p>hi <span class="user-mention" data-user-id="115715">@Jason Rute</span>  <span class="user-mention" data-user-id="116045">@Jesse Michael Han</span>!  I was also reading the paper and saw one term that was used that had no reference I could use to figure out what it meant. In particular, it said: </p>
<blockquote>
<p>...it is possible to augment a seed dataset of human proofs with new successful trajectories using rein-forcement learning or expert iteration.<br>
What is expert iteration? Is there a paper or reference to learn about it? Thanks for time in advance</p>
</blockquote>



<a name="239633331"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/239633331" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jesse Michael Han <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#239633331">(May 20 2021 at 17:54)</a>:</h4>
<p>expert iteration is described in this paper: <a href="https://arxiv.org/abs/1705.08439">https://arxiv.org/abs/1705.08439</a></p>



<a name="239641818"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/239641818" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Brando Miranda <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#239641818">(May 20 2021 at 18:55)</a>:</h4>
<p>I'm curious, is expert iteration not very popular? (or does not work very well) I've never heard of it before and have not seen it before mentioned by name before PACT. Does someone know? (Thanks for the reference!)</p>



<a name="239644927"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/239644927" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuhuai Tony Wu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#239644927">(May 20 2021 at 19:18)</a>:</h4>
<p>Expert iteration can be understood as a very general concept, also known as iterated amplification and distillation.  Essentially the idea is that you start with a weak agent, and apply some amplifier (usually some search method) to obtain better data ("expert trajectories"), and then you distill the data back to the agent by supervised learning. AlphaGo/AlphaZero also relies on the same idea .</p>



<a name="248914306"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/248914306" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Brando Miranda <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#248914306">(Aug 09 2021 at 22:01)</a>:</h4>
<p>Hi PACT authors! I was curious about the details of the optimizer you used. What type of optimizer did you use (+ scheduler) and what hyperparameter search did you do to find them (if any)?</p>



<a name="248920590"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/248920590" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Aidan Swope <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#248920590">(Aug 09 2021 at 23:04)</a>:</h4>
<p>The optimization strategy is described on page 6 of the <a href="https://arxiv.org/pdf/2102.06203.pdf">PACT paper</a><br>
(I am not an author of the work)</p>



<a name="248920920"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/248920920" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#248920920">(Aug 09 2021 at 23:08)</a>:</h4>
<p>If you still have questions after looking at the paper, I think <span class="user-mention" data-user-id="249373">@Stanislas Polu</span> would know best.</p>



<a name="277123431"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277123431" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277123431">(Mar 30 2022 at 10:30)</a>:</h4>
<p>Hi <span class="user-mention" data-user-id="115715">@Jason Rute</span>  <span class="user-mention" data-user-id="116045">@Jesse Michael Han</span>! I was curious about the details of co-training when reading the paper. When training the model on the <code>tactic</code> and <code>mix1</code> <code>mix2</code> datasets, will the language modeling loss of the prompt part (for instace, the lm losses computed from <code>GOAL &lt;TacticState&gt; PROOFSTEP</code> part in proof step samples) get masked? Or would the model be optimized from the loss calculated on the whole sample?</p>



<a name="277123906"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277123906" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277123906">(Mar 30 2022 at 10:36)</a>:</h4>
<p>(deleted)</p>



<a name="277123971"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277123971" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277123971">(Mar 30 2022 at 10:36)</a>:</h4>
<p>We used the whole sample, no masking at training <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>



<a name="277124277"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277124277" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277124277">(Mar 30 2022 at 10:40)</a>:</h4>
<p>Is it also the same for the validation loss we report in the charts?  Whole string, no masking?  <span class="user-mention" data-user-id="249373">@Stanislas Polu</span></p>



<a name="277124561"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277124561" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277124561">(Mar 30 2022 at 10:43)</a>:</h4>
<p>Validation loss is masked <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>



<a name="277125930"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277125930" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277125930">(Mar 30 2022 at 10:59)</a>:</h4>
<p>A lot of thanks!</p>



<a name="277181151"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277181151" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277181151">(Mar 30 2022 at 17:40)</a>:</h4>
<p>Hi <span class="user-mention" data-user-id="249373">@Stanislas Polu</span> <span class="user-mention" data-user-id="115715">@Jason Rute</span>! Thanks again for the clarification. I have another small question about the <code>tactic</code> dataset: I followed the instructions from <a href="https://github.com/jasonrute/lean_proof_recording">lean_proof_recording</a> and run the full pipeline to extract the dataset. While the B.2 section of the paper said that the dataset has around 128K examples, I got over 200K examples finally. I am wondering is the difference of runtime environment causing this? Or would PACT clean/filter the generated samples before training the model on them?</p>



<a name="277182536"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277182536" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Arthur Paulino <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277182536">(Mar 30 2022 at 17:52)</a>:</h4>
<p>mathlib has grown since then. That's probably why</p>



<a name="277182812"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277182812" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277182812">(Mar 30 2022 at 17:54)</a>:</h4>
<p>And it seems that the local context classification task is missing from the <a href="https://github.com/jesse-michael-han/lean-step-public">lean-step-public</a> repo. Is it because the released extraction script for <code>mix1</code> and <code>mix2</code> is not that up-to-date? :D</p>
<p><em>EDIT: I realised that the <code>Proof step classification</code> task is actually the <code>local context classifcation</code> task. Sorry for the noise!</em></p>



<a name="277182846"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277182846" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277182846">(Mar 30 2022 at 17:54)</a>:</h4>
<p><span class="user-mention silent" data-user-id="451983">Arthur Paulino</span> <a href="#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper/near/277182536">said</a>:</p>
<blockquote>
<p>mathlib has grown since then. That's probably why</p>
</blockquote>
<p>Ah yes! That looks reasonable.</p>



<a name="277183669"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/277183669" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#277183669">(Mar 30 2022 at 18:00)</a>:</h4>
<p>So the dataset size nearly doubled after one year, which may result in better model performance even without the changes in algorithms! It is exciting to see that the machine learning models somewhat grow together with humans. :D</p>



<a name="279490998"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/279490998" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#279490998">(Apr 20 2022 at 02:15)</a>:</h4>
<p>Hey <span class="user-mention" data-user-id="249373">@Stanislas Polu</span>! Recently I have been trying to reproduce the results of PACT on a GPT model trained by our lab. I am curious about the technical details about decoding strategy: is PACT using nuclear sampling? and how would temperature affect the performance?</p>



<a name="279491632"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/279491632" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#279491632">(Apr 20 2022 at 02:26)</a>:</h4>
<p>And we found that our model is really vulnerable to overfitting on mix1 and mix2. Did this also happen in your settings?</p>



<a name="279521594"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/279521594" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#279521594">(Apr 20 2022 at 09:56)</a>:</h4>
<p><span class="user-mention" data-user-id="489362">@Yichen Xu</span> we generally simply sample at T=1.0; which is rather high but the diversity of samples overall helps performance we found.</p>



<a name="279541638"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/PACT%20Paper/near/279541638" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yichen Xu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper.html#279541638">(Apr 20 2022 at 13:16)</a>:</h4>
<p>Tons of thanks! :^)</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>