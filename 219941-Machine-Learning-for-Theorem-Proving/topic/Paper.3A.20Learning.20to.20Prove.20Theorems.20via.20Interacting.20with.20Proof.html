---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html">Paper: Learning to Prove Theorems via Interacting with Proof</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="197021213"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021213" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021213">(May 09 2020 at 23:49)</a>:</h4>
<p><a href="https://arxiv.org/pdf/1905.09381.pdf" title="https://arxiv.org/pdf/1905.09381.pdf">Learning to Prove Theorems via Interacting with Proof Assistants</a> by Kaiyu Yang and Jia Deng<br>
The paper with CoqGym and ASTactic.  (It's been around for a while, but I want to talk about it now.)</p>



<a name="197021269"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021269" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021269">(May 09 2020 at 23:50)</a>:</h4>
<p><span class="user-mention" data-user-id="246156">@Brando Miranda</span> I'm continuing the ASTacti/CoqGym conversation from <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Paper.3A.20Tactic.20Learning.20and.20Proving.20for.20the.20Coq.20Proof.20Assista" title="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Paper.3A.20Tactic.20Learning.20and.20Proving.20for.20the.20Coq.20Proof.20Assista">here</a>.</p>



<a name="197021391"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021391" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021391">(May 09 2020 at 23:54)</a>:</h4>
<p><span class="user-mention" data-user-id="246156">@Brando Miranda</span> asked:</p>
<blockquote>
<p>Can someone shed me some light on why generation of tactics via tree expansion is not a popular method? Do people know what are it's drawbacks?</p>
</blockquote>
<p>Ok, I looked at the <a href="https://arxiv.org/pdf/1905.09381.pdf" title="https://arxiv.org/pdf/1905.09381.pdf">CoqGym/ASTactic paper</a> more.  First, the "tree" in "tree expansion on tactic grammar" is neither of the above trees I mentioned <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Paper.3A.20Tactic.20Learning.20and.20Proving.20for.20the.20Coq.20Proof.20Assista" title="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Paper.3A.20Tactic.20Learning.20and.20Proving.20for.20the.20Coq.20Proof.20Assista">in the other thread</a>.  As far as I can tell their approaches to those two trees are similar to the other papers.  What they mean by "tree expansion" on the tactic grammar refers to how to build a single tactic command like <code>rewrite IHaâ€™</code>.  Now let's first talk about how DeepHOL does this (and other similar AI agents).  DeepHOL take the goal state and then uses that goal state to compute a tactic such as REWRITE.  This step is called tactic selection.  Then if the tactic takes premise arguments,  they take that same goal state (and in some models they also use the chosen tactic) to rank the available premises and they take the best one.  (Or maybe they that ranking as a probability distribution and randomly choose one.  I forget the details.)  If there is a list of premises (like in <code>SIMP_TAC</code>), DeepHOL takes a fixed number (if I remember correctly) of the highest ranked premises.  Now, in my mind, the big difference in the ASTactic paper is that they fill in more of the tactic arguments.</p>



<a name="197021394"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021394" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021394">(May 09 2020 at 23:54)</a>:</h4>
<p>Whereas DeepHOL only supplies a tactic and premises, ASTactic might supply more arguments.  Some of this might just be necessary for some of the Coq tactics, but it is in the long run important to be able to fill in more arguments to tactics so that the AI is not constrained by the types of proofs it can find.  (Currently DeepHOL and most of the other Machine Leaning proof AIs can't generate the witness terms for existential proofs.)  The CoqGym/ASTactic folks call this "tree expansion on the tactic grammar", but as a concept it seems similar to the other papers, but with more tactic arguments filled in (and maybe more concern about the distinction between different types of arguments, e.g. not all lemmas can be used for rewriting).</p>



<a name="197021402"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021402" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021402">(May 09 2020 at 23:54)</a>:</h4>
<p>Now, I'm really confused how extensive their AI actually is at filling in tactic arguments.  They give a complicated context free grammar in the appendix, but the paper only lists three types of arguments:</p>
<ul>
<li>identifiers of premises.  Note like DeepHOL, they only use premises which have already been named with identifiers.  (Note that in Coq, or Lean it is essential to take local premise arguments in addition to global ones.  Some tactics also only take local premises.)</li>
<li>numbers.   I don't know enough about Coq to know how numbers are used in tactics, but they have a model for choosing the numbers 1 through 4.</li>
<li>variable names.  For example, the induction tactic needs a variable name.  They randomly pick a universally quantified variable in the goal.  (I assume other Coq AIs do something similar.  Of course this could also be considered a form of premise selection with the premise <code>n : nat</code>.)</li>
</ul>



<a name="197021419"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021419" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021419">(May 09 2020 at 23:55)</a>:</h4>
<p>That is it!  They also show an example of choosing the location of the rewrite.  Like in Lean, Coq can apply a rewrite to the local context.  Maybe they consider this to be part of tactic tree generation process where they choose the rewrite tactic, they have to choose out of the four rewrite location types.  Then if one chooses to rewrite the local context, then one has to choose which local hypothesis to rewrite.</p>



<a name="197021420"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021420" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021420">(May 09 2020 at 23:55)</a>:</h4>
<p>My (slightly uninformed) conclusion is this.  Yes, it is important to fill in more of the tactic arguments and to use the tactics to their fullest.  In HOL-Light, I don't think the tactic were as complicated so they aren't missing as much by not using all the tactic possibilities. (However, it still may be good to filter down, say, the rewrite premises to those which are of the correct form.  Also, they probably should eventually fill in variable names.)  As for other Coq agents, I'd have to look more at how they do things.  These other agents generally are more powerful, so I don't think they are missing out too much on Coq's tactic features if they choose not to use a tactic option.  Maybe they apply heuristics for some things like variable names, or maybe they just treat problem as premise selection as well.  (Since in Coq and Lean, <code>n : nat</code> and <code>h : n = n</code> are both a form of "premise".  It may be easy for the AI to realize that induction needs a local premises of the first type and rewrite needs a local or global premise of the second type.  Also, I don't know that it is even a good idea to use the grammar for human tactics.  If it is important to rewrite the goal and rewrite the local hypotheses, one could just break that into two tactics.  Honestly, the "context free grammar" is really flat and is mostly just a list of tactics with very few options for the tactics.  (For example, they don't use any of the tactic combinators like <code>repeat</code>, <code>try</code>, <code>&lt;|&gt;</code>, <code>;</code>, etc.)  Last, none of the current AIs can yet handle the trickiest arguments, like existential proofs.  (ASTactic limits existential arguments to local hypotheses.)  Once we start supplying term arguments, then we will definitely need something like this.  So if that is where your thoughts are headed, I agree we will need ways to build arbitrary term trees.</p>



<a name="197021423"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Learning%20to%20Prove%20Theorems%20via%20Interacting%20with%20Proof/near/197021423" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof.html#197021423">(May 09 2020 at 23:55)</a>:</h4>
<p>One more comment about the ASTactic paper.  They use a very complicated model for predicting the tactic and arguments, in particular they use  the premises and the goal state and previous hidden states in their RNN for building process.  As usually with more complicated models, especially RNNs, I think they could be more difficult to train and they didn't do an abolition to show that the GRU RNN added anything.  Indeed this is not the best performing Coq AI (maybe the worst performing!), and I am even skeptical about the way they handle premise selection, since they only choose 10 premises from the global context.  Are those the 10 used in the human proof or 10 random ones?  Anyway, I think these ideas hold a lot of promise but I think they are currently not adding value for the added complexity.</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>