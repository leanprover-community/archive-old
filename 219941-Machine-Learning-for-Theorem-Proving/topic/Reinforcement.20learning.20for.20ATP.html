---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html">Reinforcement learning for ATP</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="222202113"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/222202113" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Ivan Bolotnikov <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#222202113">(Jan 10 2021 at 02:53)</a>:</h4>
<p>Hello! As far as I understand, the currently dominating idea in this community regarding building ML models for ATP (and for IMO grand challenge in particular) is to expand the Mathlib library with new handwritten proofs and then to use them for training models in a supervised manner. Meanwhile, I'm curious whether the reinforcement learning (RL) approach could be more promising for ATP. I mean, it seems to be a very natural idea to run the following open ended two "component" procedure: the first component generates theorems starting from simple ones and proceeding with more and more complex; and the second component tries to prove these theorems continuously retraining on self-generated proofs. Moreover, it might be possible to expand a pool of available tactics in runtime with incrementally more complex tactics by learning to automatically generate tactics from currently generated proofs.<br>
Actually, I'm just interested in opinion of community members regarding the prospects of the above ideas.</p>



<a name="222207732"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/222207732" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Zygimantas Straznickas <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#222207732">(Jan 10 2021 at 05:47)</a>:</h4>
<p>This is actually something I'm trying to work on! As to why the RL approach isn't more popular, my impression is that it's (maybe correctly) perceived to be very very hard. Not that the supervised methods are easy -- but at least there are good pre-existing ideas to build on top of, "proofs of existence" that show achieving decent results is possible, and infrastructure+datasets to train the models. Whereas for RL, it seems that even state-of-the-art foundational RL techniques aren't yet good enough  for this problem. I actually know two experienced researchers who tried to do RL for ATP, and both quickly gave up and refocused on other areas of RL.  So it might just be that we need to wait for more RL knowledge to build up :) .</p>
<p>One interesting challenge for the adversarial RL setup you describe is, how do we avoid an equilibrium of boring-complicated problems? Mathematicians can easily feel the difference between the Riemann Hypothesis and a random 5000-clause boolean satisfiability problem. But a sufficiently big SAT problem can be harder to solve than any currently-open math problem. How do we stop the pair of agents from just getting better and better at (solving/generating) random boring SAT problems, instead of "interesting", math-y problems? (btw, afaik this problem is still hypothetical -- we're far away from caring about agents learning the wrong math, we still can't get them to learn any :) )</p>



<a name="222220279"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/222220279" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#222220279">(Jan 10 2021 at 12:34)</a>:</h4>
<p>Add to that the fact that the action space is enumerable. Most vanilla methods such as UCT+MCTS won't work well in that setup which requires more research in RL. Note that simple expert iteration which is easily applicable here has been demonstrated to work and count as RL as well :)</p>



<a name="222229550"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/222229550" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Vladislav Bezhentsev <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#222229550">(Jan 10 2021 at 16:30)</a>:</h4>
<blockquote>
<p>One interesting challenge for the adversarial RL setup you describe is, how do we avoid an equilibrium of boring-complicated problems?</p>
</blockquote>
<p>I like this point. Here are some thoughts on this matter:</p>
<ol>
<li>
<p>Though it's indeed not obvious how to generate meaningful theorems in general, we can try to solve this problem in some limited setting. For example, we can consider only combinatorics. Then we can automatically design increasingly complex constructions consisting only of finite sets (including mappings and other relations on them) and ask a predefined set of questions about them, like "count number of elements in &lt;construction&gt;", "find a minimal / maximal (in some sense) construction in a given family", "how big must some substructure be to guarantee that a particular property holds? (citation from wiki on Ramsey theory)". Of course, we will still get a lot of "strange" theorems from a human point of view, but at least 1) all of them will indeed have some combinatorics essence (by construction) and 2) a lot of freshman level, math competition level and even research level combinatorics problems seems to be possible to generate in such a simple framework.</p>
</li>
<li>
<p>Let's imagine that we somehow were able to formalize statements of the majority of currently existing interesting / important theorems and conjectures. Let's call them "reference theorems". But then, back to the adversarial setup, we can try to generate only those "artificial" theorems which in some sense are similar to "reference theorems". That is we can try to use "references" as anchor points which hinder us from from trying to prove some weird stuff and sticks us to useful parts of mathematics. In such setting we can even transfer the concept of usefulness on the artificially generated theorems. That is, useful theorems are those which frequently occur as lemmas in the proofs of "reference theorems".</p>
</li>
<li>
<p>We can go a little bit further and train a model on the formalized "reference theorems" to generate new theorems in the same vein.</p>
</li>
</ol>
<p>All in all, the 2. and 3. points are intended to say that it might be possible to gain some major benefits from only formalizing the statements of theorems without formalizing their proofs.</p>



<a name="235116251"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235116251" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> mikey liu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235116251">(Apr 19 2021 at 01:23)</a>:</h4>
<p>i think some sort of model-based RL is needed. dreamcoder, a paper i suggested in another thread, is similar to muzero at the top level, for example. but i think we need all the tricks in the bags (nlp, type theory, etc.) to "actually" do math.</p>



<a name="235176033"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235176033" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235176033">(Apr 19 2021 at 13:02)</a>:</h4>
<p><span class="user-mention silent" data-user-id="378484">Ivan Bolotnikov</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/222202113">said</a>:</p>
<blockquote>
<p>Hello! As far as I understand, the currently dominating idea in this community regarding building ML models for ATP (and for IMO grand challenge in particular) is to expand the Mathlib library with new handwritten proofs and then to use them for training models in a supervised manner.</p>
</blockquote>
<p>I'm 3 months too late to this conversation, so you may already know this now, but for the IMO grand challenge specifically, there is only one team working on that, lead by <span class="user-mention" data-user-id="230999">@Daniel Selsam</span> and my understanding (which he is free to correct) is that this is decidedly not the approach his team is taking.  In particular, what I can piece together is that they are working on basically a big super IMO tactic which will solve most (all?) IMO problems.  What they have realized is that to solve a problem, like prove an inequality, there is a general algorithm one follows, but also one has to make a bunch of choices, such as which of the X number of common inequality lemmas to appl.  Similarly, in geometry problems, one has to make a number of choices when constructing geometry diagrams, which if not done well will lead to combinatorial explosion.  Their tactic framework is specifically designed around these choices.  One way to hand these choice points is to try them all in some sort of search algorithm, and that is what a lot of the stuff they have presented so far does.  Another is to train a NN to make the choice, possibly using a universal choice-making NN.  See the following resources for more on Daniel and team's ideas:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=GtAo8wqWHHg">Daniel's AITP Talk</a> where he talks about all this stuff.</li>
<li><span class="user-mention" data-user-id="116045">@Jesse Michael Han</span>'s <a href="https://jesse-michael-han.github.io/blog/imo-gc-geo/">blog post</a> which came out at the same time and goes into more details about this <code>SearchT</code> tactic framework.</li>
<li>Two papers on solving geometry problems <a href="https://arxiv.org/pdf/2012.02590">here</a> and <a href="https://arxiv.org/pdf/2102.04633">here</a>.  I have not yet read either closely, and the second I only discovered today.</li>
<li>A <a href="https://arxiv.org/pdf/2012.11401">paper on a universal oracle for making choices in software</a>.  See the discussion at <a class="stream-topic" data-stream-id="219941" href="/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/universal.20oracle">#Machine Learning for Theorem Proving &gt; universal oracle</a> including a summary by me.</li>
</ul>



<a name="235199858"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235199858" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235199858">(Apr 19 2021 at 15:24)</a>:</h4>
<p>I may have also given the idea that Daniel's IMO approach is something like "hard coding" solution heuristics.  While I think that is somewhat true, there does seem to be something very general in their approach that I don't yet understand.  For example, they can add more "abstractions" into the mix to solve more problems with little or no change to the overall algorithm.  There is a discussion in <a class="stream-topic" data-stream-id="219941" href="/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder">#Machine Learning for Theorem Proving &gt; dreamcoder</a> that provides some more details.</p>



<a name="235227078"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235227078" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235227078">(Apr 19 2021 at 18:31)</a>:</h4>
<p>Now, as for general automation for interactive theorem proving, I think reinforcement learning is actually quite common.  But unlike supervised learning, it requires more work to set up.  Also, reinforcement learning is a broad concept, so what you have in mind for reinforcement learning is not what possibly others have in mind.  Most current work on RL in theorem proving seems to follow the following pattern which is not unlike AlphaGo:<br>
(0) (Optional) Bootstrap your prover by supervised learning on human proofs or simple heuristics (like TFIDF for premise selection).<br>
(1) Loop over all training theorems (not the proofs, but the theorem statements) and use your learned theorem prover (with tree search) to find proofs.<br>
(2) Save the proofs as training data and use that to refine your prover.<br>
(3) Loop starting at step (1) until convergence or timeout.</p>



<a name="235227139"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235227139" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235227139">(Apr 19 2021 at 18:32)</a>:</h4>
<p>Papers which do this sort of RL loop include:</p>
<ul>
<li><a href="https://arxiv.org/abs/1805.07563">Reinforcement Learning for Theorem Proving </a>  a tableau-based FOL prover for Mizar statements using XGBoost.  I think the final result can solve about half of the theorems in the library (both testing and training) with no proof supervision.  (They also have a followup paper just for Robinson Arithmetic which is more of toy example demonstrating the power of curriculum learning in this area.)</li>
<li><a href="https://arxiv.org/abs/1807.08058">Learning Heuristics for Quantified Boolean Formulas through Deep Reinforcement Learning</a></li>
<li><a href="https://arxiv.org/pdf/1905.10501.pdf">Learning to Reason in Large Theories without Imitation</a> is one of the <a href="https://sites.google.com/view/holist/home">HOList project</a> papers.  With out using human proofs, I think they can solve about 56% of test theorems (and more when starting with human proofs and then improving via RL).</li>
<li><a href="https://arxiv.org/abs/2009.03393">Generative Language Modeling for Automated Theorem Proving</a> for MetaMath.  The prover starts off with supervised learning (in a language model), but then improves via Expert Iteration (which is basically the algorithm above, even though they don't call it RL).</li>
</ul>



<a name="235227164"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235227164" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235227164">(Apr 19 2021 at 18:32)</a>:</h4>
<p>I personally feel that the RL ideas are still in the AlphaGoZero era and can be improved a lot.  There is no uses of competing populations of agents, no adversarial games, no learned world models (although <a href="https://arxiv.org/abs/1909.11851">Mathematical Reasoning in Latent Space</a> starts in this direction), no hierarchical RL, no novelty search, no learned curriculum.  Also, specific to theorem proving, they only explore a very narrow part of the mathematical landscape, namely proofs of theorems already in the library.  There has not been a lot of work done on conjecturing new theorems which I think is what you mean by RL?</p>



<a name="235227200"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235227200" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235227200">(Apr 19 2021 at 18:32)</a>:</h4>
<p>One very small step in that direction might be counterexample exploration.  In tactic theorem proving, when an agent reaches a goal, it could try to prove that goal true and simultaneously try to construct a counterexample.  This could be a way to instill the agent with concrete knowledge about specific numbers, spaces, operations, etc.</p>



<a name="235227254"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235227254" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235227254">(Apr 19 2021 at 18:33)</a>:</h4>
<p>Also, it shouldn't be underestimated how hard it is to get into neural theorem proving.  I've seen researchers asking about hooking up their RL algorithm to HOList only to discover that the HOList action space is very different from anything they have seen before.  It is easy to come up with ideas, but hard to test them.</p>



<a name="235237603"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235237603" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235237603">(Apr 19 2021 at 19:46)</a>:</h4>
<p>I'll share some thoughts I've had about the problem of how to use RL to generate new theorems to help train a neural prover (fyi I've shared similar thoughts previously in the dreamcoder thread). </p>
<p>I think being able to generate new theorems is critical to enable open-ended improvement in neural provers. I also think it may be possible to get a feedback loop working between a neural generator and a neural prover (basically a form of asymmetric self-play, unlike the symmetric forms of self-play seen in systems like AlphaZero/MuZero). One of the main problems is how you optimize the theorem generator. A purely adversarial objective (as in GANs) is probably not going to work. One idea is to optimize the generator to produce theorems that are near the limit of the prover's capability. Since the prover uses tree search (let's assume a gpt-f like model), you can use the branch-factor of the prover's search tree as a metric for how hard the proof was. If the prover fails completely there's nothing you can say, except that it was too hard. But if it succeeds, then branch factor is a measure of how much search was required. An easy proof requires little search (branch factor could be as low as 1, if the search is linear though in practice I suppose this might never happen). The prover's search times out after a certain number of steps, so one could optimize the generator (via RL) to produce theorems that when the prover tries to prove them result in a large branch factor but are still provable within the timeout limit.</p>
<p>I think the approach I sketched is very limited and could be improved in various ways. One thing not addressed is how to bias the generator in the direction of generating "interesting" theorems. It's easy to imagine a failure mode like Vladislav described above in which it generates strange theorems. Arguably, we might still learn interesting things from the dynamics of such a system, and it could serve as a basis for experiments that optimize the generator for different objectives. As Jason noted, more experimentation and new ideas are definitely needed.</p>
<p>I've been working on the problem of how one could generate theorems within Lean, and happy to go into more details with anyone who's curious.</p>



<a name="235238172"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235238172" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235238172">(Apr 19 2021 at 19:50)</a>:</h4>
<p>If the generator at the same time tries to minimize the pretty-printed length of the theorem, that might be one constraint pushing it towards "interesting" theorems.</p>



<a name="235238268"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235238268" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235238268">(Apr 19 2021 at 19:51)</a>:</h4>
<p>Otherwise I fear you'll quickly end up with 5-kilobyte long trivialities about intersections and unions of subsets.</p>



<a name="235240919"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235240919" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235240919">(Apr 19 2021 at 20:09)</a>:</h4>
<p>Here is an interesting paper on asymmetric self play in RL.  There are three agents.  Agent C creates problems which are hard for agent A but easy for agent B.  Agents A and B both try to solve the problems and learn from them.  This keeps the problems just on the doable side since ideally only one of the two agents can solve them.  <a href="https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html?m=1">https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html?m=1</a></p>



<a name="235241089"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235241089" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235241089">(Apr 19 2021 at 20:10)</a>:</h4>
<p>But <span class="user-mention" data-user-id="112680">@Johan Commelin</span> may be correct and other constraints would be needed to both (1) make the problems useful and (2) make the problems explore all the math topics in the library.</p>



<a name="235241401"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235241401" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235241401">(Apr 19 2021 at 20:12)</a>:</h4>
<p>Maybe the generator agent needs to make problems which look like theorems in the library and maybe it has to contain a randomly sampled declaration.</p>



<a name="235243251"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235243251" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235243251">(Apr 19 2021 at 20:26)</a>:</h4>
<p><span class="user-mention silent" data-user-id="249373">Stanislas Polu</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/222220279">said</a>:</p>
<blockquote>
<p>Add to that the fact that the action space is enumerable. Most vanilla methods such as UCT+MCTS won't work well in that setup which requires more research in RL. Note that simple expert iteration which is easily applicable here has been demonstrated to work and count as RL as well :)</p>
</blockquote>
<p>David Silver's team at Deepmind just released a whole bunch of papers on arXiv about MCTS including one on this subject (which I haven't read yet): <a href="https://arxiv.org/abs/2104.06303">Learning and Planning in Complex Action Spaces</a>.</p>



<a name="235256273"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235256273" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235256273">(Apr 19 2021 at 21:58)</a>:</h4>
<p><span class="user-mention" data-user-id="337523">@Joe Palermo</span> The dual of a nondeterministic tactic <code>tac</code> is a nondeterministic theorem generator <code>gen</code> such that <code>tac</code> proves all theorems generated by <code>gen</code>. For some tactics, it is trivial to deduce a deterministic generator that can enumerate the theorems that the tactic proves, but for others, the obvious generator is <em>constrained</em>, i.e. it cannot easily be represented as a deterministic program and so requires search to execute. A sweet-spot for RL-based data-augmentation may be to map a nondeterministic tactic, plus an input to its dual generator, to successful executions of the generator, i.e. to a theorem that the tactic proves. By coincidence, a successful episode will produce a supervised datapoint in exactly the form one would want in order to learn heuristics for nondeterministic tactics.</p>



<a name="235257690"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235257690" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235257690">(Apr 19 2021 at 22:12)</a>:</h4>
<p><span class="user-mention" data-user-id="230999">@Daniel Selsam</span> when you say nondeterministic tactic, you mean a tactic with choice points, right?   So something like <code>gptf</code> which is stochastic, would it be clear what the dual is (maybe if the choice points are assumed to be every place where is chooses a tactic)?  Also, I assume you are thinking more along the lines of something similar to your work?  Do you have a simpler example of a nondeterministic tactic and dual for which this might work?</p>



<a name="235258040"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235258040" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235258040">(Apr 19 2021 at 22:15)</a>:</h4>
<p>Also, what is the input to a dual generator?  Is this like some index of the nth output, or a source or randomness?</p>



<a name="235258189"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235258189" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235258189">(Apr 19 2021 at 22:17)</a>:</h4>
<p>Or the path through the choice points?</p>



<a name="235259240"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235259240" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235259240">(Apr 19 2021 at 22:27)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> Here are some notes I took ~2 years ago introducing the primal-vs-dual distinction in the context of inequalities <a href="/user_uploads/3121/XEGASHKrR7DDp4h0EjIaK0S7/doc1_primal_trick_vs_dual_trick.html">doc1_primal_trick_vs_dual_trick.html</a> (HTML with MathJax)</p>



<a name="235263173"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235263173" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235263173">(Apr 19 2021 at 23:05)</a>:</h4>
<p>Ok, I think I understand a little better.  For the first unconstrained dual (the bad one if you will), the goal would be to find examples of <code>f</code>, <code>h</code>, and <code>g</code> satisfying those various constraints.  The generated functions would be used to generate an example of a problem solvable by that specific trick (the primary tactic).  I can see how that would be an ML problem to find such <code>f</code>, <code>g</code> and <code>h</code>.  It is much more targeted, of course, than what any of us had in mind since it I think it requires building generator duals for all or many of the nondeterministic tactics in some framework of non-deterministic tactics (which of course is still a work in progress on your part <span aria-label="smile" class="emoji emoji-1f642" role="img" title="smile">:smile:</span>).</p>



<a name="235263264"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235263264" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235263264">(Apr 19 2021 at 23:06)</a>:</h4>
<p>I have to admit I'm very interested in how well your ideas are going to play out <span class="user-mention" data-user-id="230999">@Daniel Selsam</span>.  They seem very foreign to me.  On one hand, it seems like hand-coding heuristics for solving problems which seems very slow and arduous.  On the other hand, you do hint at a great level of generality which I don't fully grasp yet.  If, for example, your approach both solves a large set of interesting problems (e.g. IMO grand challenge) and it can quickly be adapted to other problems, then I will definitely take note.</p>



<a name="235263406"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235263406" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235263406">(Apr 19 2021 at 23:08)</a>:</h4>
<p>(If it solves the IMO grand challenge, but doesn't easily generalize to other problems, then it seems like a lot of work to hand code all of the tricks used in the IMO grand challenge, like Mathematica hand coded all sorts of integration tricks to solve integrals.)</p>



<a name="235264144"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235264144" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235264144">(Apr 19 2021 at 23:16)</a>:</h4>
<p><span class="user-mention silent" data-user-id="112680">Johan Commelin</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/235238172">said</a>:</p>
<blockquote>
<p>If the generator at the same time tries to minimize the pretty-printed length of the theorem, that might be one constraint pushing it towards "interesting" theorems.</p>
</blockquote>
<p><span class="user-mention" data-user-id="112680">@Johan Commelin</span>  I agree, minimizing length seems like a useful constraint. </p>
<p><span class="user-mention silent" data-user-id="112680">Johan Commelin</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/235238172">said</a>:</p>
<blockquote>
<p>Otherwise I fear you'll quickly end up with 5-kilobyte long trivialities about intersections and unions of subsets.</p>
</blockquote>
<p>My hope is that if the generator repeatedly produced trivial variations on the same problems then the prover might achieve sufficient generalization within that problem class to push the branch factor of search low enough that the generator would be forced to try something new (since in that case its reward within that problem class would be declining). Admittedly this requires that the prover be very sample efficient and is probably asking too much.</p>



<a name="235265999"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235265999" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235265999">(Apr 19 2021 at 23:35)</a>:</h4>
<p><span class="user-mention" data-user-id="230999">@Daniel Selsam</span> Thanks for sharing your thought, also thanks <span class="user-mention" data-user-id="115715">@Jason Rute</span> for your questions (I had the same ones) and the breakdown above. I suppose one could attempt to find f, h, and g via program synthesis guided by learned models.</p>



<a name="235266557"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235266557" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235266557">(Apr 19 2021 at 23:42)</a>:</h4>
<p>Admittedly I'm new to Lean and still learning the basics. Are there many nondeterministic tactics currently in Lean? I assume any tactic that requires search to execute can be considered nondeterministic, e.g. tidy or gptf.</p>



<a name="235270814"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235270814" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235270814">(Apr 20 2021 at 00:33)</a>:</h4>
<p>Nondeterministic in this setting doesn’t mean stochastic (or dependent on the machine, etc.).  (I think it comes from the fact that the list monad is called the nondeterministic monad since you can think of a list as the result of multiple paths being computed at the same time.). Anyway, no tactics in current lean are like this.  This is a theoretical framework that Daniel is implementing as part of his project to solve the IMO grand challenge.  They are like tactics but have choice points in them.  You can find out more with some of the resources I put above.</p>



<a name="235270869"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235270869" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235270869">(Apr 20 2021 at 00:33)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/235263173">said</a>:</p>
<blockquote>
<p>I think it requires building generator duals for all or many of the nondeterministic tactics in some framework of non-deterministic tactics</p>
</blockquote>
<p>The "grail" of this direction would be to automatically synthesize more general nondeterministic tactics from regular tactic proofs, to automatically synthesize diverse duals from nondeterministic tactics, and then to use ML/RL to learn to execute nondeterministic tactics effectively. But I think many of the most critical tactics---especially the ones that must be explicitly taught to humans---will be relatively easy to implement and yet basically impossible to synthesize, so we should be pragmatic and write them.</p>



<a name="235271795"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235271795" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235271795">(Apr 20 2021 at 00:45)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/235263264">said</a>:</p>
<blockquote>
<p>I have to admit I'm very interested in how well your ideas are going to play out <span class="user-mention silent" data-user-id="230999">Daniel Selsam</span>.  They seem very foreign to me.  On one hand, it seems like hand-coding heuristics for solving problems which seems very slow and arduous.  On the other hand, you do hint at a great level of generality which I don't fully grasp yet.  If, for example, your approach both solves a large set of interesting problems (e.g. IMO grand challenge) and it can quickly be adapted to other problems, then I will definitely take note.</p>
</blockquote>
<p>One way to phrase my goal regarding generality is that I hope the domain-specific part need not be globally coherent. It is hard to write complex software, but it can be easy to write down a bunch of tricks. Ideally different people can add tricks monotonically without much coordination, and the general-purpose "meta-AI" can "denoise" this web of tricks.</p>
<p>Aside: I thought of calling this "strategy programming" at somepoint, as in a higher-order version of <a href="https://arxiv.org/abs/1605.07723">Data Programming</a>, but the name seemed a little too vague.</p>



<a name="235291103"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235291103" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235291103">(Apr 20 2021 at 05:52)</a>:</h4>
<p><span class="user-mention silent" data-user-id="337523">Joe Palermo</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/235264144">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="112680">Johan Commelin</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Reinforcement.20learning.20for.20ATP/near/235238172">said</a>:</p>
<blockquote>
<p>Otherwise I fear you'll quickly end up with 5-kilobyte long trivialities about intersections and unions of subsets.</p>
</blockquote>
<p>My hope is that if the generator repeatedly produced trivial variations on the same problems then the prover might achieve sufficient generalization within that problem class to push the branch factor of search low enough that the generator would be forced to try something new (since in that case its reward within that problem class would be declining). Admittedly this requires that the prover be very sample efficient and is probably asking too much.</p>
</blockquote>
<p>But I don't see why a lazy generator can't just decide to return a theorem of length <span class="tex-error">$$e^{e^{e^x}}}$$</span> (add more exponentials if needed) to guarantee that even a reasonably smart prover will have a branch factor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">&gt; x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span> on such a theorem. But I should also note that I have no idea what I'm talking about. I've never done anything in ML beyond an assignment in a class 10 years ago.</p>



<a name="235295340"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235295340" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Scott Morrison <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235295340">(Apr 20 2021 at 06:46)</a>:</h4>
<p>A basic problem for an interesting generator is how to steer it away from "produce a preimage for this one-way function" style problems, which I fear are rather dense in the space of all propositions.</p>



<a name="235361480"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235361480" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235361480">(Apr 20 2021 at 15:16)</a>:</h4>
<p><span class="user-mention" data-user-id="112680">@Johan Commelin</span> That's a valid point. I think it would be important to cap proof length. Hopefully within a given max proof length, a prover can generalize enough to cover trivial variations on some proof types. I wouldn't be surprised to find that some problem classes break this paradigm though</p>



<a name="235391281"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235391281" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235391281">(Apr 20 2021 at 18:25)</a>:</h4>
<p><span class="user-mention" data-user-id="337523">@Joe Palermo</span> I don't know about capping proof length. I was suggesting capping statement length.</p>



<a name="235391328"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235391328" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235391328">(Apr 20 2021 at 18:25)</a>:</h4>
<p>Fermat's Last Theorem has a short statement and a loooooooooong proof.</p>



<a name="235397375"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235397375" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Oliver Nash <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235397375">(Apr 20 2021 at 19:03)</a>:</h4>
<p>Too long to fit in a margin anyway</p>



<a name="235404515"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/235404515" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#235404515">(Apr 20 2021 at 19:51)</a>:</h4>
<blockquote>
<p>But I don't see why a lazy generator can't just decide to return a theorem of length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><msup><mi>e</mi><msup><mi>e</mi><mi>x</mi></msup></msup></msup></mrow><annotation encoding="application/x-tex">e^{e^{e^x}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.08198em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.08198em;"><span style="top:-3.08198em;margin-right:0.05em;"><span class="pstrut" style="height:2.71898em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0271142857142856em;"><span style="top:-3.0271142857142856em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5961142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.83456em;"><span style="top:-2.904em;margin-right:0.1em;"><span class="pstrut" style="height:2.5em;"></span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> (add more exponentials if needed)</p>
</blockquote>
<p>If you put too many exponentials in, won't the generator have trouble returning it? Just writing that statement will take so long that the human operator will just have to step in and kill the generator</p>



<a name="237601212"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/237601212" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Shariq Hashme <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#237601212">(May 06 2021 at 03:48)</a>:</h4>
<p>Hey all, love all the discussions about theorem proving RL. ~4 years ago I worked at OpenAI and was very interested in a similar approach which I shortly discussed with Christian Szegedy at the time. I don't have the time to pursue it myself, but I wrote down the critical parts of the approach which I believe has a chance of achieving "theorem proving escape velocity": a theorem proving system which came up with its own interesting theorems and proved them (at scale, perhaps enough to prove any human math as well). If anyone finds it relatively novel and uses these ideas I'd really appreciate the acknowledgment, since I spent probably around 2 years (maybe more) thinking about this <span aria-label="stuck out tongue closed eyes" class="emoji emoji-1f61d" role="img" title="stuck out tongue closed eyes">:stuck_out_tongue_closed_eyes:</span> I have a number of thoughts I didn't include, e.g, around how it could be implemented, so if you're interested please email me. </p>
<p>Here's the write-up: <a href="https://docs.google.com/document/d/1bqJcWdYfl460Y_TU48f1esOkyHgNkyvbA7leVO-YJ3g/edit?usp=drivesdk">https://docs.google.com/document/d/1bqJcWdYfl460Y_TU48f1esOkyHgNkyvbA7leVO-YJ3g/edit?usp=drivesdk</a></p>



<a name="237659401"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/237659401" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#237659401">(May 06 2021 at 13:41)</a>:</h4>
<p><span class="user-mention" data-user-id="410209">@Shariq Hashme</span> Thanks for the write up!  I think there are a lot of great ideas floating around now about treating math as a multiagent game.  I've also heard Szegedy talk about having a population of provers (I don't know if he gave any specific details about how this would work).  I know David McCallister has also been thinking a lot about the "game of mathematics" and capturing the idea of "interesting" mathematics.  I also saw a paper by one of the GamePAD authors with a proposal a number of years ago.  (I also have some of my own ideas which I need to write down.)  It would probably be helpful for us to find and organize these various proposals.</p>



<a name="237659431"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/237659431" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#237659431">(May 06 2021 at 13:41)</a>:</h4>
<p>I also hope researchers start working on this.  I imagine smaller logical systems could be a good testing ground, but at the same time it might be easier to just start with a large amount of built-in mathematics to get the ball rolling.  Of course, I think it goes without saying that the credit should probably go mostly to those who can get something like this working.  I imagine there are a number of important design choices that need to be made to get such a system to learn and avoid collapse.</p>



<a name="237740814"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Reinforcement%20learning%20for%20ATP/near/237740814" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Reinforcement.20learning.20for.20ATP.html#237740814">(May 06 2021 at 22:32)</a>:</h4>
<p>I think your compression idea <span class="user-mention" data-user-id="410209">@Shariq Hashme</span> is very similar to ideas in the dream coder paper.  I want to spend some time fleshing that out and how it relates to theorem proving.  For example, I think something like descriptive complexity (calculated as the negative log of the probability your model assigns to the proof) is probably a better notion of proof size.</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>