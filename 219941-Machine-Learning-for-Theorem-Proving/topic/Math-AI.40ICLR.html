---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html">Math-AI@ICLR</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="237742950"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237742950" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237742950">(May 06 2021 at 22:56)</a>:</h4>
<p>There is a <a href="https://mathai-iclr.github.io/schedule/">workshop tomorrow on Math and AI</a>.  I won't personally be attending (I have work), but I look forward to <a href="https://mathai-iclr.github.io/papers/">reading the papers and posters</a> and <a href="https://mathai-iclr.github.io/schedule/">watching the talks</a> when they appear on YouTube in about a month.  There are some big names in deep learning there (Bengio, Szegedy), as well as mathematics (Gowers).  Also, a lot of familiar faces giving talks and papers, like Josef Urban, Markus Rabe, <span class="user-mention" data-user-id="230999">@Daniel Selsam</span>, <span class="user-mention" data-user-id="249373">@Stanislas Polu</span>, <span class="user-mention" data-user-id="240875">@Yuhuai Tony Wu</span>, <span class="user-mention" data-user-id="110187">@Minchao Wu</span>, and <span class="user-mention" data-user-id="116045">@Jesse Michael Han</span> (talking about Lean GPT-f or course <span aria-label="smile" class="emoji emoji-1f642" role="img" title="smile">:smile:</span>).</p>



<a name="237745848"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237745848" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuhuai Tony Wu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237745848">(May 06 2021 at 23:27)</a>:</h4>
<p>Thanks Jason for helping to advertise the workshop! Yes -- to attend the workshop you need to register at <a href="https://iclr.cc/Register/view-registration">https://iclr.cc/Register/view-registration</a>. You can then follow the program at <a href="https://iclr.cc/virtual/2021/workshop/2124">https://iclr.cc/virtual/2021/workshop/2124</a>.</p>
<p>I would like to specifically mention the panel discussion tomorrow at <strong>10am PT/ 1pm ET</strong>. It'll be a discussion on general reasoning, the role of math in general intelligence, and the challenges ahead. We will leave 10-20 min in the end for questions from the audience. You can join zoom or leave questions on the rocket chat.</p>
<p>The panel features Fields Medalist Tim Gowers, Turing Award winner Yoshua Bengio, neural network founding father Jay McClelland, Cambridge AI Professor Mateja Jamnik, neural network guru Christian Szegedy, formal reasoning expert Josef Urban, and math philosopher Alison Pease.</p>
<p>Hope to see you there!</p>



<a name="237746498"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237746498" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237746498">(May 06 2021 at 23:35)</a>:</h4>
<p>Some things which look interesting to me in the <a href="https://mathai-iclr.github.io/papers/">papers and posters</a>:</p>
<ul>
<li>Proof Artifact Co-training for Theorem Proving With Language Models  of course!  (Although I would recommend looking at the <a href="https://arxiv.org/abs/2102.06203">arXiv version</a> or the <a href="https://www.youtube.com/watch?v=EXpmbAfBNnw">Harvard YouTube talk</a>.  Or Jesse's talk when it becomes available.)<div class="youtube-video message_inline_image"><a data-id="EXpmbAfBNnw" href="https://www.youtube.com/watch?v=EXpmbAfBNnw"><img src="https://uploads.zulipusercontent.net/bd2a9c6a819c867efb19ea439f4a113b891f5dae/68747470733a2f2f692e7974696d672e636f6d2f76692f4558706d624166424e6e772f64656661756c742e6a7067"></a></div></li>
<li><span class="user-mention" data-user-id="230999">@Daniel Selsam</span> has a paper and poster about his non-deterministic tactics.  I look forward to learning more about how those actually work (especially now that the examples seem to be written in Lean code).</li>
<li>I'm particularly interested in the REFACTOR paper where they extract "Modular and Reusable" theorems from MetaMath proofs.</li>
<li>A lot of papers on transformers and mathematical tasks.</li>
<li>"Pretrained Transformers as Universal Computation Engines " is a really interesting paper.  The main idea is that a pertrained Transformer model (an advanced type of language model) are good for all sorts of things having nothing to do with the original task it was trained on.  Also like pretrained image models, one can get away with just fine tuning on a much smaller subset of parameters.  (Here is a <a href="https://www.youtube.com/watch?v=Elxn8rS88bI">video</a> on the paper.) <div class="youtube-video message_inline_image"><a data-id="Elxn8rS88bI" href="https://www.youtube.com/watch?v=Elxn8rS88bI"><img src="https://uploads.zulipusercontent.net/546a0e25f52bce758736f9f431828dc1b84dc82d/68747470733a2f2f692e7974696d672e636f6d2f76692f456c786e387253383862492f64656661756c742e6a7067"></a></div></li>
<li>Also, in a similar idea is the LIME paper, which is a particular mathematical method of pertaining transformers on a synthetic dataset.  (It reminds me of another paper where they pretrain image models on <a href="https://arxiv.org/abs/2103.13023">synthetic fractals</a>.  Maybe we can get away with having pretrained language models which don't contain data, just synthetically created computational primitives.)</li>
</ul>
<p>As for the invited talks, I'm always excited to hear what Google (<span class="user-mention" data-user-id="217806">@Markus Rabe</span>) and OpenAI (<span class="user-mention" data-user-id="249373">@Stanislas Polu</span>) have been up to.  The panel discussion also looks pretty interesting.</p>
<p>Actually, I have to admit all the papers and invited talks look really interesting.  (I'm starting to really regret not taking off work to attend this virtual workshop. <span aria-label="frown" class="emoji emoji-1f641" role="img" title="frown">:frown:</span>.)</p>



<a name="237748438"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237748438" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237748438">(May 07 2021 at 00:00)</a>:</h4>
<p>OpenAI has an interesting one on "grokking". The picture says it all: <a href="/user_uploads/3121/xHkFCKKOHYPiLFkUw4LQDHcb/grokking.png">grokking.png</a> Wild.</p>
<div class="message_inline_image"><a href="/user_uploads/3121/xHkFCKKOHYPiLFkUw4LQDHcb/grokking.png" title="grokking.png"><img src="/user_uploads/3121/xHkFCKKOHYPiLFkUw4LQDHcb/grokking.png"></a></div>



<a name="237760741"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237760741" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuhuai Tony Wu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237760741">(May 07 2021 at 02:53)</a>:</h4>
<p>Something I'd like to mention about the REFACTOR work <span class="user-mention" data-user-id="115715">@Jason Rute</span>  mentioned. One of the inspiration of the work also comes from dreamcoder, where we found that the algorithm for extracting reusable components is very hard to scale to realistic problems (it's using something called fragment grammer -- check their supplementary materials for details). We instead went with a data-driven way, by creating synthetic data points to train a GNN agent for extracting reusable components. Hence it might be interesting for people who were discussing the dreamcoder paper on another thread of this stream.</p>



<a name="237761489"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237761489" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237761489">(May 07 2021 at 03:04)</a>:</h4>
<p><span class="user-mention silent" data-user-id="240875">Yuhuai Tony Wu</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237760741">said</a>:</p>
<blockquote>
<p>One of the inspiration of the work also comes from dreamcoder, where we found that the algorithm for extracting reusable components is very hard to scale to realistic problems</p>
</blockquote>
<p>Which variables did you find it scaled poorly in? The beam-search abstraction algorithm of S4.5.3 is at least nominally linear in the number of programs/proofs.</p>



<a name="237763131"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237763131" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuhuai Tony Wu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237763131">(May 07 2021 at 03:30)</a>:</h4>
<p>We did some experiments with his repo: <a href="https://github.com/ellisk42/ec/tree/master/dreamcoder">https://github.com/ellisk42/ec/tree/master/dreamcoder</a>, and I remember the algorithm couldn't cope with large programs/graphs, and also generated poor fragments after all. Also to me the experiments presented in the paper is quite toy-ish, I would guess this is their bottleneck.</p>



<a name="237766763"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237766763" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237766763">(May 07 2021 at 04:25)</a>:</h4>
<p>I also had trouble with that step in their repo.  I was running out of memory if I had more than 20 examples.  Honestly I didn’t troubleshoot it much and could have went to a higher resource machine, so I wasn’t sure if it was just user error on my part.</p>



<a name="237767184"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237767184" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237767184">(May 07 2021 at 04:32)</a>:</h4>
<p><span class="user-mention" data-user-id="230999">@Daniel Selsam</span> I wonder how that Grokking paper is related to the <a href="https://openai.com/blog/deep-double-descent/">double descent phenomenon</a></p>



<a name="237818123"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237818123" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237818123">(May 07 2021 at 13:00)</a>:</h4>
<p><span class="user-mention silent" data-user-id="240875">Yuhuai Tony Wu</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237763131">said</a>:</p>
<blockquote>
<p>We did some experiments with his repo: <a href="https://github.com/ellisk42/ec/tree/master/dreamcoder">https://github.com/ellisk42/ec/tree/master/dreamcoder</a>, and I remember the algorithm couldn't cope with large programs/graphs, and also generated poor fragments after all. Also to me the experiments presented in the paper is quite toy-ish, I would guess this is their bottleneck.</p>
</blockquote>
<p>Do you think your REFACTOR approach could fit into a program synthesis engine (similar in purpose to DreamCoder), but one that scales beyond "toyish" examples?  I have been collecting problems that I think would be a good test of a DreamCoder-like-system.</p>



<a name="237831001"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237831001" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Yuhuai Tony Wu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237831001">(May 07 2021 at 14:23)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237818123">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="240875">Yuhuai Tony Wu</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237763131">said</a>:</p>
<blockquote>
<p>We did some experiments with his repo: <a href="https://github.com/ellisk42/ec/tree/master/dreamcoder">https://github.com/ellisk42/ec/tree/master/dreamcoder</a>, and I remember the algorithm couldn't cope with large programs/graphs, and also generated poor fragments after all. Also to me the experiments presented in the paper is quite toy-ish, I would guess this is their bottleneck.</p>
</blockquote>
<p>Do you think your REFACTOR approach could fit into a program synthesis engine (similar in purpose to DreamCoder), but one that scales beyond "toyish" examples?  I have been collecting problems that I think would be a good test of a DreamCoder-like-system.</p>
</blockquote>
<p>Yes definitely, we started doing some early experiments with <span class="user-mention" data-user-id="258175">@Albert Jiang</span>, but we didn't finish the effort -- happy to chat if someone wants to continue our effort.</p>



<a name="237899936"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/237899936" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#237899936">(May 07 2021 at 22:33)</a>:</h4>
<p>How did the <del>conference</del> workshop go?  Any highlights?</p>



<a name="240594554"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/240594554" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Oliver Nash <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#240594554">(May 28 2021 at 12:05)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237746498">said</a>:</p>
<blockquote>
<p>Although I would recommend looking at the <a href="https://arxiv.org/abs/2102.06203">arXiv version</a> or the <a href="https://www.youtube.com/watch?v=EXpmbAfBNnw">Harvard YouTube talk</a>.  Or Jesse's talk when it becomes available.)</p>
</blockquote>
<p>I just watched this talk. Very nice. I laughed out loud when I saw that you used my "Human-written proof" as the example (c.45:00) of where GPTf crushed it!</p>



<a name="242471359"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Math-AI%40ICLR/near/242471359" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Math-AI.40ICLR.html#242471359">(Jun 12 2021 at 20:38)</a>:</h4>
<p>The talks for Math-AI are out now.  I haven't had a time to watch them yet.  <a href="https://mobile.twitter.com/Yuhu_ai_/status/1402832506177609729">https://mobile.twitter.com/Yuhu_ai_/status/1402832506177609729</a></p>
<div class="inline-preview-twitter"><div class="twitter-tweet"><a href="https://mobile.twitter.com/Yuhu_ai_/status/1402832506177609729"><img class="twitter-avatar" src="https://uploads.zulipusercontent.net/2b2aea46b32aac6ac3e0b8b147d04b990bfe53bc/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313238313332303237383336333131393631372f64394e67673035545f6e6f726d616c2e6a7067"></a><p>All talks and the panel discussion is now released at <a href="https://t.co/0guE7zh4zR">https://iclr.cc/virtual/2021/workshop/2124</a>.

The panel discussion starts at 4:33:47. 

Enjoy! <a href="https://t.co/nOmN6D5un0">https://twitter.com/Yuhu_ai_/status/1390649168868245504</a></p><span>- Yuhuai (Tony) Wu (@Yuhu_ai_)</span></div></div>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>