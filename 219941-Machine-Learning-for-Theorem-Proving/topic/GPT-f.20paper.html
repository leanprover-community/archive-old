---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html">GPT-f paper</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="209486101"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209486101" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209486101">(Sep 09 2020 at 08:51)</a>:</h4>
<p>Hi! We (OpenAI) just released a paper describing GPT-f, a Transformer-based automated theorem prover. It covers a lot of work we've been doing with Metamath <img alt=":metamath:" class="emoji" src="https://zulip-avatars.s3.amazonaws.com/3121/emoji/images/17589.gif" title="metamath"> and that could be applied to Lean. </p>
<p>Full abstract:</p>
<blockquote>
<p>We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans – the generation of original mathematical terms – might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep learning based system has contributed proofs that were adopted by a formal mathematics community.</p>
</blockquote>
<p>arXiv link: <a href="https://arxiv.org/abs/2009.03393">https://arxiv.org/abs/2009.03393</a></p>



<a name="209486170"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209486170" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209486170">(Sep 09 2020 at 08:52)</a>:</h4>
<p>Any feedback/suggestions or questions are obviously welcome. I also hope to meet some of you at AITP next week to discuss it!</p>



<a name="209486444"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209486444" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209486444">(Sep 09 2020 at 08:55)</a>:</h4>
<p>Nice! I'll try to read it soon</p>



<a name="209486453"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209486453" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209486453">(Sep 09 2020 at 08:55)</a>:</h4>
<p>Abstract looks promising!</p>



<a name="209524897"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209524897" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209524897">(Sep 09 2020 at 15:12)</a>:</h4>
<p>I also look forward to reading it.  (I’m on honeymoon this week, so don’t expect much of a response soon.)</p>



<a name="209524991"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209524991" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209524991">(Sep 09 2020 at 15:13)</a>:</h4>
<p>However, one quick question: Are you working on applying it to Lean, or is that an exercise for the reader?</p>



<a name="209527773"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209527773" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Patrick Massot <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209527773">(Sep 09 2020 at 15:31)</a>:</h4>
<p>Oh great, I remember you had to postpone your wedding because of Covid, but it looks like it happened in the end, congratulations!</p>



<a name="209536955"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209536955" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209536955">(Sep 09 2020 at 16:47)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> I haven't read through everything in detail, because I don't know enough about ML. But I'm very impressed by the fact that GPT-f found several shorter proofs than those that were in <a href="http://set.mm">set.mm</a> at the time.</p>



<a name="209548439"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209548439" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209548439">(Sep 09 2020 at 18:18)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> we're still at an exploratory stage, but short answer is yes, definitely!</p>



<a name="209548716"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209548716" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209548716">(Sep 09 2020 at 18:21)</a>:</h4>
<p><span class="user-mention" data-user-id="112680">@Johan Commelin</span> Thank you! (I have to admit we were ourselves super excited by this result as well :))</p>



<a name="209560443"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209560443" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jesse Michael Han <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209560443">(Sep 09 2020 at 20:02)</a>:</h4>
<p>during proof search, is the model conditioned on the proof tree / previously expanded goals?</p>



<a name="209601565"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209601565" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209601565">(Sep 10 2020 at 06:54)</a>:</h4>
<p>No it's not due to the size of the expressions involved being already quite large. That being said, we experimented with conditioning on the top goal and were not able to demonstrate a huge lift. But this was very exploratory so I wouldn't bet my money on it.</p>



<a name="209725970"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209725970" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Wojciech Nawrocki <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209725970">(Sep 11 2020 at 00:26)</a>:</h4>
<p>This is impressive! And it makes me quite excited for what the future holds. Two small corrections if you care about that sort of thing:<br>
Pg. 8, the <code>ring</code> footnote points to the <a href="https://leanprover-community.github.io/mathlib_docs/algebra/ring/basic.html#ring">class</a> rather than the <a href="https://leanprover-community.github.io/mathlib_docs/tactic/ring.html">tactic</a><br>
Table 3: Matmath -&gt; Metamath?</p>



<a name="209759873"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209759873" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209759873">(Sep 11 2020 at 09:43)</a>:</h4>
<p>Thanks a lot <span class="user-mention" data-user-id="128280">@Wojciech Nawrocki</span> for the kind words as well as the feedback. Duly noted and factored in the next version of the paper <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span> Thanks!</p>



<a name="209950827"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209950827" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209950827">(Sep 13 2020 at 21:43)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> What is considered a valid proof step?  GPT-f will return both a theorem and substitutions, which then must unify with the goal.  If the substitutions don't unify, then I'm sure it is marked invalid.  However, what if the theorem isn't in the list of previously proved theorems?  What does GPT-f do?</p>
<ol>
<li>Try to prove that theorem,</li>
<li>consider this an invalid proof step, or</li>
<li>restrict the output to only known theorems?<br>
(I assume it is the first option, but I want to check.)</li>
</ol>



<a name="209974134"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/209974134" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#209974134">(Sep 14 2020 at 07:52)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> Good questions!</p>
<ul>
<li>If the unification fails, the kernel rejects the proof step and it is not even considered in the proof tree search (not added to the tree or queue, nor valued by the value function).</li>
<li>If the theorem statement generated is not in the theorem database, currently and in the experiments reported in the paper, the kernel rejects it as well. That being said we're experimenting with letting the model prove such conjectures if they are considered interesting by the value function. In that case we simply add the theorem itself as a subgoal (with a special tag to make sure we re-check the distinct variables once a proof is found (DVs are a metamath technicality that is ok to abstract in your thinking and revisit later if you don't know how they work)) and the subgoal is valued and added to the queue accordingly.</li>
</ul>



<a name="210004095"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210004095" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210004095">(Sep 14 2020 at 13:25)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> Another follow up question: When validating or testing the model, do you have any kind of dependency restriction on theorems?  For example, in most of these papers, to prove 0 = 0 (if it is in the test set), one must use theorems which came before 0 = 0.  I believe the Holophrasm and MetaGen papers do this.  The MetaGen paper calls these "background theorems".  (This is not perfect however for tactic based provers, since 0 = 0 might be provable with a tactic which came along after the proof of 0=0.  I have more thoughts on better ways to do this, but that is for another time.)  Does your paper do this?   I ask, because without a restriction like this, you could have a much higher percentage of theorems solved.</p>



<a name="210004253"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210004253" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210004253">(Sep 14 2020 at 13:26)</a>:</h4>
<p>Yep this is mentioned in the paper (I believe?). We enforce theorem ordering as we evaluate on <a href="http://set.mm">set.mm</a> <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>



<a name="210005785"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210005785" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210005785">(Sep 14 2020 at 13:37)</a>:</h4>
<p>Sorry.  I had trouble finding it.  I see it now, on the middle of page 4.  Thanks!</p>



<a name="210007974"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210007974" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210007974">(Sep 14 2020 at 13:55)</a>:</h4>
<p>(FWIW the lift you generally get from waiving that constraint is 2-3% success rate)</p>



<a name="210087014"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087014" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087014">(Sep 15 2020 at 03:21)</a>:</h4>
<p>I should mention that there are a few threads talking about GPT-f on the Internet besides this one:</p>
<ul>
<li>Twitter: <a href="https://twitter.com/spolu/status/1303578985276887042">https://twitter.com/spolu/status/1303578985276887042</a></li>
<li>Reddit: <a href="https://www.reddit.com/r/MachineLearning/comments/ipdu7m/r_gptf_a_new_sota_for_automated_mathematical/">GPT-f: a new SOTA for automated mathematical proofs : MachineLearning</a></li>
<li>MetaMath Google Group: <a href="https://groups.google.com/g/metamath/c/gZPfD-DlQBI">GPT-f paper</a></li>
</ul>
<div class="inline-preview-twitter"><div class="twitter-tweet"><a href="https://twitter.com/spolu/status/1303578985276887042"><img class="twitter-avatar" src="https://pbs.twimg.com/profile_images/2008060134/423231_3486704046246_1230777314_33453905_1333098466_n_normal.jpeg"></a><p>Posted my first paper on arXiv<span aria-label="boom" class="emoji emoji-1f4a5" role="img" title="boom">:boom:</span><span aria-label="raised hands" class="emoji emoji-1f64c" role="img" title="raised hands">:raised_hands:</span>

GPT-f is a Transformer-based automated theorem prover. We show that Transformer + Search is suitable to formal reasoning and continuous self-improvement 🦾

<a href="https://t.co/VllDcCV3Kc">https://arxiv.org/abs/2009.03393</a> <a href="https://t.co/5ttVX0MNBC">https://twitter.com/spolu/status/1303578985276887042/photo/1</a></p><span>- Stanislas Polu (@spolu)</span><div class="twitter-image"><a href="https://t.co/5ttVX0MNBC"><img src="https://pbs.twimg.com/media/Ehc-LRsXYAEObi-.jpg:medium"></a></div><div class="twitter-image"><a href="https://t.co/5ttVX0MNBC"><img src="https://pbs.twimg.com/media/Ehc-LRrWkAA4PDt.jpg:thumb"></a></div><div class="twitter-image"><a href="https://t.co/5ttVX0MNBC"><img src="https://pbs.twimg.com/media/Ehc-LRtXcAEdoMM.jpg:thumb"></a></div><div class="twitter-image"><a href="https://t.co/5ttVX0MNBC"><img src="https://pbs.twimg.com/media/Ehc-LRjWoAEn-NG.jpg:thumb"></a></div></div></div>



<a name="210087032"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087032" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087032">(Sep 15 2020 at 03:21)</a>:</h4>
<p>As usual, I’m going to try to summarize the paper for an audience who knows more about theorem provers than neural networks.</p>



<a name="210087086"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087086" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087086">(Sep 15 2020 at 03:22)</a>:</h4>
<p>In many ways GPT-f is similar to other theorem provers which have come before, HOList/DeepMath, CoqGym/ASTTactic, TacticToe, etc.  What all of these have in common is that they treat theorem proving as a tree search.  What has been known for a long time is that one can avoid combinatorial explosion in tree (and graph) search by adopting smart heuristics.  What AlphaGo and its successors has taught us is that these heuristics can be entirely learned either from examples or from bootstrapping and reinforcement learning.  GPT-f is no different in this regard.  (I’m not going to say much more about the specific tree search algorithm used by GPT-f, since I don’t think their approach is especially optimized more than any other similar paper.)</p>



<a name="210087095"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087095" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087095">(Sep 15 2020 at 03:22)</a>:</h4>
<p>Currently GPT-f is a system for MetaMath, but that is just a design choice.  The MetaMath system is embarrassingly simple, but at the same time user-friendly enough to have developed a large library.  That makes it a good candidate for a first experiment.  Also, as we will see, GPT-f has no problem handling the most difficult part about MetaMath automation.</p>



<a name="210087108"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087108" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087108">(Sep 15 2020 at 03:22)</a>:</h4>
<p>From a certain point of view, one can think of MetaMath as a tactic-based prover with only one tactic.  If one reads a MetaMath proof backward, at every point there is one or more goals, say <code>( 3 + 2 ) = 5</code>.  One can then apply a previously proved theorem, for example transitivity: <code>{ A = B, B = C } |- A = C</code> and specify how to substitute the free variables.  After substitution, the conclusion must equal the goal.  Therefore, in this example the substitutions for <code>A</code> and <code>C</code> must be <code>A = ( 3 + 2 )</code>  and <code>C = 5</code>, while <code>B</code> could be anything.  The trick, of course, is to substitute something useful for <code>B</code>.  If you choose <code>B = 4 + 1</code> , then after applying this theorem (backwards), one gets a new goal for each hypothesis: <code>( 3 + 2 ) = ( 4 + 1 )</code> and <code>( 4 + 1 ) = 5</code>.  The latter happens to be a theorem (the definition of <code>5</code> in MetaMath), which would close that particular goal.</p>



<a name="210087115"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087115" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087115">(Sep 15 2020 at 03:22)</a>:</h4>
<p>In most of the work applying deep learning to (tactic-based) theorem proving, there are four main tasks:</p>
<ul>
<li><em>tactic selection</em>: Given a goal, find the best tactic.  The nice thing about tactic selection is that there is a fixed list of tactics.  Choosing the best thing from a fixed list is easy for deep learning systems.  For MetaMath, it is trivial, since there is only one tactic.</li>
<li><em>premise/lemma selection</em>: Given a goal (and a tactic), find the best theorem to apply (assuming the tactic takes one or more theorems as parameters, e.g. a rewrite tactic).  There are multiple ways to do this.  Many systems assign each theorem a vector, a key, and assign the goal another vector, a query, and try to find the theorem(s) whose keys most closely match the query.    Other systems, try to find the goal most similar in the training data and use whatever tactic and premises that goal used.  GPT-f takes a unique approach here as we will see.</li>
<li><em>parameter selection</em>: Besides any theorems, there are often other parameters that need to be selected for a tactic as well.  HOList handles this by avoiding tactics with such parameters (or filling in the parameters with constant values).  CoqGym has a fixed, limited grammar from which those parameters can be taken.  Other provers allow using local variables and subterms.  Still others find a similar example in the training data and use those parameters. For MetaMath, the choice of parameters is especially important and not at all trivial.  Previous Metamath AIs (Holophrasm and MetaGen) both use recurrent neural networks to guess at the substitution.  This is where GPT-f will shine, since transformers are especially good at “making stuff up”.</li>
<li><em>value estimation</em>: Finally, to make the tree search more efficient, usually a score is applied to each goal to say how “provable” it is.</li>
</ul>



<a name="210087124"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087124" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087124">(Sep 15 2020 at 03:23)</a>:</h4>
<p>One comment on the above four steps is that the first three are called a <em>policy</em> and can be done together or separately.  Also, in, say, Lean, it is not uncommon to see tactics like <code>apply (lem p)</code>  which don’t fit cleanly into the above paradigm.  The “premise” is technically <code>lem p</code> but morally the premise is <code>lem</code> and one modifies <code>lem</code> by instantiating the universal quantifier with the term <code>p</code>.  GPT-f (if applied to Lean) would show a lot of promise for handling situations like this as well.</p>



<a name="210087130"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087130" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087130">(Sep 15 2020 at 03:23)</a>:</h4>
<p>GPT-f is based on a transformer architecture.  (See <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Papers.20on.20Neural.20Conjecturing">my notes on transformers here</a>.)  Without getting into the details, it basically is a (really good!) text generator.  You give it a prompt and it completes the prompt.  In this case, the system was trained to take prompts like this:</p>
<div class="codehilite"><pre><span></span><code>GOAL [[]] |- (3 + 2) = 5 PROOFSTEP
</code></pre></div>


<p>and then complete that prompt with something like the following (except without the new lines which I added for readability):</p>
<div class="codehilite"><pre><span></span><code>GOAL [[]] |- ( 3 + 2 ) = 5
PROOFSTEP
[[ |- A = B |- C = B ]] |- A = C
{{ A : ( 3 + 2 ) }}
{{ B : ( 4 + 1 ) }}
{{ C : 5 }}
&lt;|endoftext|&gt;
</code></pre></div>



<a name="210087134"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087134" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087134">(Sep 15 2020 at 03:23)</a>:</h4>
<p>The way the text is generated allows for generating multiple stochastic completions to the prompt.  Each completion is scored and checked by MetaMath to see if it is a valid proof step.  If so, it is plugged into the tree search.  (The value function is generated similarly, but trained via reinforcement learning similar to HOList/DeepMath.  See the GPT-f paper for details.)</p>



<a name="210087136"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087136" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087136">(Sep 15 2020 at 03:23)</a>:</h4>
<p>What separates this paper from other similar works is a few small but important details.  Whereas other models might design a custom machine learning architecture for the exact logic at hand, transformers take the viewpoint: “As long as its text, I can handle it.”</p>



<a name="210087181"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087181" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087181">(Sep 15 2020 at 03:24)</a>:</h4>
<p>There have been a number of papers recently showing that transformers can mimic logical reasoning.  (The most famous is the paper showing that transformers can solve integrals.)  I hope it is very clear to any remaining skeptics that if we are willing to pay for the computer power (more on that in a second…), then we basically have a general purpose tool which can begin to mimic advanced human reasoning on novel tasks.   I’m not saying it can come up with a proof of a Millennium Problem, but it can solve straightforward proofs in whatever ITP you want.  There is nothing special here about tactic-mode proofs vs term mode vs Isar-style vs first-order logic.  In the end, they can all be implemented by a tree search guided by a (transformer) neural network.  The only thing stopping us is (1) engineering and (2) computer power.</p>



<a name="210087193"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087193" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087193">(Sep 15 2020 at 03:24)</a>:</h4>
<p>Getting back to the “it’s just text” theme, probably the most surprising thing about this paper is the way the model was pre-trained.  GPT-style transformer models are most famously known for generating fake realistic text, be it a screen play or question answering.  It is well known that to achieve the best results one must pre-train the model on a large corpus of text, usually public domain books and websites.  GPT-f is no different.  The model improved by 8 percentage points when pre-trained on such information. It even did a few points better when trained on more mathematical text, including arXiv, GitHub, and Math StackExchange.  (I have so many questions about what the model is getting out of these datasets.  It is just about recognizing math terminology and parentheses matching, or is it somehow memorizing common proofs?)</p>



<a name="210087201"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087201" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087201">(Sep 15 2020 at 03:24)</a>:</h4>
<p>Another thing which is fascinating about GPT-style transformers is that they don’t use a fixed vocabulary.  This can really be a problem for other models.  What if a user uses a new definition, or just picks a unique variable name?  GPT uses something called <em>byte pair encoding</em> which scans the training data for common groupings of symbols.  Those then become the tokens.  A common word may be its own token, but an uncommon word may be made of multiple tokens, possibly just a token for each letter.  This allows any new words at test time.</p>



<a name="210087212"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087212" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087212">(Sep 15 2020 at 03:25)</a>:</h4>
<p>Now, as for MetaMath, the transformer architecture provides a number of interesting possibilities.  First, notice that when the transformer returned the theorem to apply, it didn’t call it by name or look it up from a list of theorems.  It called it by the statement of the theorem.  To be clear, this is a design choice, but it is a really interesting one.  There are two related ways to look at this: (1) the transformer has memorized the theorems in the dataset.  (2) the transformer says “hey, what I really need here is a theorem of the form …”.  Of course, it is probably a little of both cases.  However, the second case leads to the possibility of conjecturing.  If the “theorem” isn’t in the dataset, then one could set out to prove it anyway.  (As <span class="user-mention" data-user-id="249373">@Stanislas Polu</span>  said above, they don’t do this yet.)</p>



<a name="210087221"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087221" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087221">(Sep 15 2020 at 03:25)</a>:</h4>
<p>The second interesting thing is that unlike a lot of similar systems, the transformer has little or no problem filling in the variable substitutions.  It can just “guess” a substitution.  Of course it may not be useful or even type check, but MetaMath can check that the proof step is valid and the tree search will test it for usefulness. <span class="user-mention" data-user-id="239426">@Christian Szegedy</span>  has also said he thinks that GPT-style text generation is also a promising best path forward when a theorem prover needs to come up with new ideas.</p>



<a name="210087267"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087267" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087267">(Sep 15 2020 at 03:26)</a>:</h4>
<p>Before I get into the negatives, I want to really commend <span class="user-mention" data-user-id="249373">@Stanislas Polu</span>  and team on making this not only into a paper, but into <a href="https://groups.google.com/g/metamath/c/D09W2QVR-_I">a usable tool</a> that the MetaMath community can use.  I think this back-and-forth interaction with the community is what is going to drive AI for ITPs forward.</p>



<a name="210087277"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087277" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087277">(Sep 15 2020 at 03:26)</a>:</h4>
<p>Ok, now for the negatives.  Basically, this is a great experiment and I’m glad OpenAI is fitting the bill for this system, but to be clear this is a quite expensive project.  It shows what is possible, but it probably isn’t scalable to the average MetaMath user level right now.  I doubt any of us could build a comparable system without the backing of a large research lab like Google, Facebook, OpenAI, or DeepMind.</p>



<a name="210087280"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087280" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087280">(Sep 15 2020 at 03:26)</a>:</h4>
<p>It’s well known that transformers are computationally expensive.  They require O(n^2) computations to compute a sequence of length n (including the prompt).  They also have more parameters that many other neural network types.</p>



<a name="210087285"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087285" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087285">(Sep 15 2020 at 03:27)</a>:</h4>
<p>To run the larger model once over the training data required 20,000 GPU-hours on a V100.  Contrast this with the first HOList/DeepHOL paper.  While the DeepHOL model took a lot of iterations to train via reinforcement learning (eight V100 GPUs running for an unspecified amount of time), the trained version is something I could run on my Macbook Air.  When doing the “reinforcement learning”, the GPT-f model is only iterated twice since it is so expensive to run, compared to the thousands of iterations used by HOList/DeepHOL.</p>



<a name="210087290"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087290" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087290">(Sep 15 2020 at 03:27)</a>:</h4>
<p>To put it in dollars, a V100 GPU-hour costs on the order of $1 per GPU-hour, so this is $20,000 to run an already trained model once across the training data.  I’m very curious what their MetaMath proof assistant web-tool is costing OpenAI.</p>



<a name="210087293"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087293" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087293">(Sep 15 2020 at 03:27)</a>:</h4>
<p>Nonetheless, there is a lot of room for optimization.  I think I’ve seen five or so papers in the last few months suggesting how to make transformers behave closer to O(n).</p>



<a name="210087335"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087335" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087335">(Sep 15 2020 at 03:28)</a>:</h4>
<p>Also, I’m still of the opinion that since formulas have so much built-in structure, that using some of that structure as an inductive bias is still valuable.  <a href="https://papers.nips.cc/paper/9376-novel-positional-encodings-to-enable-tree-based-transformers.pdf">It’s been shown</a> (<a href="https://arxiv.org/pdf/2003.04218.pdf">more than once</a>) that transformers trained with tree-based position encodings do much better at symbolic reasoning tasks.  However, I also realize that such encodings would limit the pre-training options.  I recall N2Formal (<span class="user-mention" data-user-id="239426">@Christian Szegedy</span>) suggesting ideas for pre-training which may be helpful here.  Also, it might be useful to try to gather a large dataset of formula-like data from the web parsed into a tree or graph structure.</p>



<a name="210087337"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087337" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087337">(Sep 15 2020 at 03:28)</a>:</h4>
<p>I can also think of a number of other possible optimizations.  While having a transformer which guesses everything is a nice experiment, it might still be more efficient to fill in the constrained substitutions using a Prolog like system instead.   It also might still be faster to use the theorem database more directly for lookup of theorems.  For example, <a href="https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html">Google recently showed the possibility of using transformers to lookup data from a database</a>.</p>



<a name="210087339"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087339" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087339">(Sep 15 2020 at 03:28)</a>:</h4>
<p>Finally, the holy grail is code generation.  Why have an expensive black box when you can have an AI that generates code (custom tactics in this case)?  This code would be reusable, fast, and interpretable.  Of course, transformers are being used for code generation too. :)</p>



<a name="210087346"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087346" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087346">(Sep 15 2020 at 03:29)</a>:</h4>
<p>One last thought.  It is so difficult to compare results since we don’t have standard datasets, but they report a success rate of 56% for their best model, which is much better than the previous SOTA of 21%.  I’d love it if they try this out on the HOList dataset so that they can directly compare with Google’s state of the art.  Even then however, I feel that the best judge is to put this in the hands of ITP users and to ask them what it does well on and doesn’t do well on.  Again, I’m really glad for their engagement with the MetaMath community.</p>



<a name="210087347"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087347" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087347">(Sep 15 2020 at 03:29)</a>:</h4>
<p>Overall, I am really grateful for this paper.  It is well-written (if you know a bit about transformers at least), and I think it shows that we have a lot of the tools at least to start building high powered experimental tools.  Hopefully, we can then turn to making these systems useful to the average ITP user.  I’m really looking forward to what the future brings.</p>



<a name="210087683"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210087683" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210087683">(Sep 15 2020 at 03:39)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> can, of course, correct all my misconceptions. :)</p>



<a name="210089700"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210089700" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Christian Szegedy <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210089700">(Sep 15 2020 at 04:32)</a>:</h4>
<ul>
<li>
<p>For comparison, on the HOl-Light corpus, we can reach 70% proof success rate (67% without any imitation on exsisting proofs). I'm not sure how it compares, but my guess would be that HOL-Light is a bit harder the metamath.</p>
</li>
<li>
<p>In the beginning of the year, we have also tried the approach of using transformer in autoregressive manner to generate the whole tactic invocation together with all the substitutions, theorem labels, etc. and while the results seemed somewhat comparable, it was so much more expensive to run computationally, that it just did not seem to make a lot of sense to us.</p>
</li>
</ul>
<p>That's why we started to look for tasks where generative transformers would shine: conjecturing, filling in assumptions, etc.</p>
<p>I don't say that GPT-f does not make sense, we have a very similar system for more than half a year, but it was simply not justifiable from a practicabality point of view at this point in time, especially that HOLst was also criticized for being slow, while we use a few minutes per proof attempt on CPU.</p>



<a name="210097727"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210097727" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210097727">(Sep 15 2020 at 07:23)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> thank you for the thoughtful comments. This is a great summary and glad to see our work put in perspective this way.</p>
<p>Commenting quickly on the 20k GPU.hours. They are required for the data generation/augmentation process (running proof searches on the train set) which is in turn used to train the value function you refer to in your post (same was true for alphago/zero, the data generation, aka exploration, is where you pay the price). So, the number is definitely accurate but just wanted to point out that the training of the model itself is less expensive (more like 500-1k GPU.hours).</p>
<p>As for the user experience of the ITP, I'll be demonstrating it today at AITP'20. I'll gladly make another video for folks here if interested. As you'll see the model is fast enough for it to be a somewhat pleasing experience (once you've climbed the Metamath learning curve that is :p)</p>
<p>(Also, I'm pretty confident OpenAI will be happy to foot the bill, for the foreseeable future, for usage of these systems once we manage to port them to Lean, as we do today with Metamath. The main challenge/problem I believe and as you point out is to make a useful system and share it effectively with the community <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span>)</p>



<a name="210098050"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098050" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098050">(Sep 15 2020 at 07:27)</a>:</h4>
<p>To attend the talk same as what Daniel said <a href="#narrow/stream/208328-IMO-grand-challenge/topic/AITP.20invited.20talk/near/210070867">here</a>. Ping me if interested, it's at ~15h CET (see <a href="http://aitp-conference.org/2020/">AITP Program</a>)</p>



<a name="210098147"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098147" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098147">(Sep 15 2020 at 07:28)</a>:</h4>
<p>But do I understand correctly that for the time being we will depend on external computing power to be able to run GPT-f? You can't extract a trained artifact that I can run on a 16 GB RAM + modern desktop CPU/GPU, or can you? (And expect it to spit back results withing seconds instead of days.)</p>



<a name="210098353"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098353" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098353">(Sep 15 2020 at 07:31)</a>:</h4>
<p>We can extract a trained artifact that could run correctly on one modern GPU for inference. It's just that this trained artifact, today, is served through the OpenAI API.</p>



<a name="210098429"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098429" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098429">(Sep 15 2020 at 07:32)</a>:</h4>
<p>Ok, cool</p>



<a name="210098451"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098451" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098451">(Sep 15 2020 at 07:32)</a>:</h4>
<p>I was expecting that simply executing the thing would already require &gt; 20GB RAM</p>



<a name="210098460"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098460" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098460">(Sep 15 2020 at 07:32)</a>:</h4>
<p>Not yet :p</p>



<a name="210098463"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210098463" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Johan Commelin <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210098463">(Sep 15 2020 at 07:33)</a>:</h4>
<p>I mean, just fitting the parameters into memory is already a mild feat (-;</p>



<a name="210257680"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/210257680" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#210257680">(Sep 16 2020 at 13:03)</a>:</h4>
<p><span class="user-mention silent" data-user-id="239426">Christian Szegedy</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/GPT-f.20paper/near/210089700">said</a>:</p>
<blockquote>
<ul>
<li>For comparison, on the HOl-Light corpus, we can reach 70% proof success rate (67% without any imitation on exsisting proofs).</li>
</ul>
</blockquote>
<p>The <a href="https://sites.google.com/view/holist/home">HOList website</a> currently lists 60% as the best success rate.  Was there anything big you did to get it to 70%/67%, or is it just combining the two techniques from your last two HOList papers (GNNs and better reinforcement learning)?  Is there another HOList paper coming?</p>



<a name="212244013"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212244013" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212244013">(Oct 04 2020 at 22:15)</a>:</h4>
<p>Hi <span class="user-mention" data-user-id="249373">@Stanislas Polu</span> - I’m wondering how you converted proof step substitutions generated by your model back into MetaMath (i.e. a sequence of labels). Of course in actual MetaMath proofs one needs to construct the terms that get substituted in. Constructing a complex term could require many steps in a MetaMath proof. Which specific steps (labels) are required is represented only implicitly in the substitutions. It’s not obvious to me how to write a verifier for proof steps written in your substitution format. Can you offer any pointers? Many thanks!</p>



<a name="212247313"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212247313" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212247313">(Oct 04 2020 at 23:47)</a>:</h4>
<p><span class="user-mention" data-user-id="337523">@Joe Palermo (S2'17)</span> I can't speak for <span class="user-mention silent" data-user-id="249373">Stanislas Polu</span> 's implementation, but it is fairly common for metamath proof assistants to work directly with intermediate statements and infer the substitutions by unification. <a href="https://en.wikipedia.org/wiki/Unification_(computer_science)#Syntactic_unification_of_first-order_terms">First order unification</a> is not a particularly hard problem, it is decidable with a fast algorithm (in contrast to higher order unification, which lean has to deal with sometimes and is undecidable in general).</p>



<a name="212265055"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212265055" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212265055">(Oct 05 2020 at 07:09)</a>:</h4>
<p>Hi <span class="user-mention" data-user-id="337523">@Joe Palermo (S2'17)</span> the language model generates the substitutions, then we operate at the term level in our kernel. To ensure correctness, we indeed have to prove that expressions are properly typed, as we work on proof we do by checking that the term we operate on comply to the Metamath grammar (which is encoded by the wff, class, setvar axioms). Only when we want to check the proof with another kernel, we dump the proof in native Metamath format, using our parse trees to generate the full proofs. Hope that answers your question?</p>



<a name="212265128"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212265128" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Gabriel Ebner <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212265128">(Oct 05 2020 at 07:10)</a>:</h4>
<p>If I read the paper correctly, the model doesn't generate the lemma names.  Do you just try all lemmas in <a href="http://set.mm">set.mm</a> and see which one fits the statements produced by the model?</p>



<a name="212265224"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212265224" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212265224">(Oct 05 2020 at 07:13)</a>:</h4>
<p><span class="user-mention" data-user-id="110043">@Gabriel Ebner</span> indeed we generate the theorem terms and check that they exist in <a href="http://set.mm">set.mm</a>. We've observed that it helps the machine learning a lot (which kind of makes sense as it makes the distribution of theorems and the distribution of term substitutions more alike and therefore easier to fit together)</p>



<a name="212301627"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212301627" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212301627">(Oct 05 2020 at 13:53)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span> Yes, thank you!</p>



<a name="212622697"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212622697" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212622697">(Oct 07 2020 at 21:39)</a>:</h4>
<p><span class="user-mention" data-user-id="249373">@Stanislas Polu</span>  Would I be correct in thinking that the axioms required to define this grammar are all the axioms of the form:</p>
<p>&lt;label&gt; $a wff &lt;expression&gt; $.<br>
&lt;label&gt; $a class &lt;expression&gt; $.<br>
&lt;label&gt; $a setvar &lt;expression&gt; $. (seems that this particular pattern doesn't occur in the <a href="http://set.mm">set.mm</a> database)</p>



<a name="212625692"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212625692" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212625692">(Oct 07 2020 at 22:12)</a>:</h4>
<p>Since we are asking questions, as for <span class="user-mention" data-user-id="110043">@Gabriel Ebner</span>’s question, can you look up the lemma directly from the output of GPT-f?  In other words, can you plug the output into a hash map and get the lemma (maybe after standardizing variable names)?  Or do you need to do the O(n) operation where you try to unify every earlier occurring lemma against the output of GPT-f.</p>



<a name="212626254"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212626254" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212626254">(Oct 07 2020 at 22:19)</a>:</h4>
<p><span class="user-mention" data-user-id="337523">@Joe Palermo (S2'17)</span> Yes, metamath terms are given by a CFG where each $a with a typecode other than |-  contributes one production (and the nonterminals are wff, class, setvar). The $f variable declaration commands also contribute productions, so the setvar nonterminal is not empty, it only contains variables (as terminals).</p>



<a name="212633137"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212633137" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212633137">(Oct 07 2020 at 23:42)</a>:</h4>
<p><span class="user-mention" data-user-id="110049">@Mario Carneiro</span> Thank you!</p>



<a name="212633187"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212633187" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212633187">(Oct 07 2020 at 23:43)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> I'm wondering the same thing...</p>



<a name="212638461"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212638461" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212638461">(Oct 08 2020 at 01:14)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> This problem seems pretty similar to the problem of <code>simp</code>: Given a term and a collection of lemmas, find one that unifies. You can do it pretty efficiently with a discrimination tree, and in fact this process is fast enough that the mmj2 metamath proof assistant has a feature where every open goal is automatically unified against everything in the library any time you do anything, as if <code>simp</code> was constantly running in the background. It applies every lemma that makes the goal "smaller" in some sense, except for some blacklisted lemmas, and it's quite convenient and effective.</p>



<a name="212651517"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212651517" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212651517">(Oct 08 2020 at 06:34)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> we're just looking up in a hash table constructed from <a href="http://set.mm">set.mm</a> (enforcing ordering here) <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>



<a name="212651586"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212651586" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212651586">(Oct 08 2020 at 06:35)</a>:</h4>
<p>The theorem statement generated by GPT-f is pre-unification so it's a simple lookup. GPT-f also generates substitutions that are then checked against the grammar, applied, and the fact that they unify is verified.</p>



<a name="212651699"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/212651699" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#212651699">(Oct 08 2020 at 06:37)</a>:</h4>
<p><span class="user-mention" data-user-id="337523">@Joe Palermo (S2'17)</span> As explained by <span class="user-mention" data-user-id="110049">@Mario Carneiro</span>, yes <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>



<a name="213313046"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213313046" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213313046">(Oct 14 2020 at 16:06)</a>:</h4>
<p>Hi <span class="user-mention" data-user-id="249373">@Stanislas Polu</span>. I’m trying to replicate something similar to the MetaMath environment you developed for GPT-f. <span class="user-mention" data-user-id="110049">@Mario Carneiro</span>  mentioned to me that he described to you a “KLR(0)” parsing algorithm for MetaMath expressions. Was this the one you ended up implementing? In the paper you refer to a “modified LR(0) parser”.</p>



<a name="213318189"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213318189" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213318189">(Oct 14 2020 at 16:42)</a>:</h4>
<p>Yes we implemented an LR(0) parser with basic backtracking as the amount of backtracking necessary to parse the Metamath grammar is well behaved and limited in practice. It's somewhat different than what is implemented in mmj2, in case you looked into it, where the "backtracking" is done at parser construction time.</p>



<a name="213586586"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213586586" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213586586">(Oct 16 2020 at 17:01)</a>:</h4>
<p><span class="user-mention" data-user-id="110049">@Mario Carneiro</span> would you mind sharing some of the documentation on that KLR(0) parser here?</p>



<a name="213588717"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213588717" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213588717">(Oct 16 2020 at 17:18)</a>:</h4>
<p>Sure, data dump coming right up. The following example walks through the parsing of <code>{ &lt;. x , y &gt;. }</code> vs <code>{ &lt;. x , y &gt;. | ph }</code> in the <a href="http://set.mm">set.mm</a> grammar, which yields a shift reduce conflict when parsed with LR(0). (This description assumes you know a bit about how LR(0) parse table generation works; see the <a href="https://en.wikipedia.org/wiki/LR_parser">wikipedia page</a> for a primer.)</p>



<a name="213588733"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213588733" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213588733">(Oct 16 2020 at 17:18)</a>:</h4>
<p><a href="https://github.com/digama0/mmj2/blob/master/src/mmj/verify/LRParser.java#L151-L208">This</a> is the new part of the code that distinguishes the KLR parser from LR(0). A "conflict" is a place where an LR(0) parser would fail outright.</p>



<a name="213588767"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213588767" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213588767">(Oct 16 2020 at 17:18)</a>:</h4>
<p>During parse table generation each state is associated with a bunch of partially read productions that agree on a common prefix, and in this case you are stuck at the state:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">}</span>
</code></pre></div>

<p>This is known as a shift-reduce conflict, and usually shifting is the right answer, so that's built in as a heuristic, which is why lark takes the first option over the second. But neither choice is "correct", because this changes the grammar - you are now rejecting a string that should be valid to the grammar (<code>{ &lt;. x , y &gt;. }</code> in this case) - so essentially you are finding a "closest LALR(1) approximation" to the grammar when you use lark with this heuristic.</p>
<p>To resolve this, the question is what to do from that state if you read a <code>&lt;.</code>. We haven't actually hit the conflict yet. In the first production it's clear that we should step to <code>-&gt; { &lt;. o setvar , setvar &gt;. } | ph }</code>, but the second production requires us to look at the <code>class</code> nonterminals that start with <code>&lt;.</code>. (In fact, in the first state we also have all productions for the class nonterminal, like <code>-&gt; o 1</code> and <code>-&gt; o ( class u. class )</code> and many others. These are represented in a special way in LRParser.java to save space.) Let's step through the states that the example takes. The <code>shift &lt;.</code> step takes us to:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
</code></pre></div>

<p>and <code>shift x</code> takes us to:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="n">x</span> <span class="n">o</span>
</code></pre></div>

<p>Since we are now at the end of a production, we can reduce with <code>setvar -&gt; x</code> at this point, and there are no competing productions so this is safe. This <code>reduce x</code> edge pops the stack and acts like a <code>shift setvar</code> edge from the previous step, leading to:</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="n">o</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="n">setvar</span> <span class="n">o</span>
</code></pre></div>

<p>The <code>-&gt; setvar o</code> comes from the <code>class -&gt; setvar</code> production. Now we are stuck, because we can both reduce with this production (which gives us a <code>cv</code> node) and shift a comma to continue with the first production. This is a shift-reduce conflict, and lark at this point will throw away the reduce option and shift here, leading to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">o</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
</code></pre></div>

<p>which is not correct, as we have lost the ability to parse <code>{ &lt;. x , y &gt;. }</code>.</p>



<a name="213588787"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213588787" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213588787">(Oct 16 2020 at 17:18)</a>:</h4>
<p>What I do instead to resolve this is "pre-compositing" the rules. We first got in trouble at</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="n">o</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="n">setvar</span> <span class="n">o</span>
</code></pre></div>

<p>which is a "bad state" because of the shift-reduce conflict. We want to remove the reduce node, and we do so by backing up to see how we got here. We obtained this state by <code>shift setvar</code> applied to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
</code></pre></div>

<p>and we want to repair this state so that we don't hit the train wreck one step from here. So we delete the offending rule <code>-&gt; o setvar</code> and add the composition of <code>class -&gt; setvar</code> with <code>class -&gt; &lt;. class , class &gt;.</code> as a new "composite rule" which looks like a production <code>class -&gt; &lt;. setvar , class &gt;.</code>, so that the "before" state instead looks like</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="n">setvar</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span> <span class="n">except</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">setvar</span>
</code></pre></div>

<p>and we <code>shift setvar</code> from here instead, getting to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="n">o</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="n">o</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
</code></pre></div>

<p>and we have safely cleared the conflict. (The modified "before" state is not a real state, it is only used to calculate this new state. This state is replacing the original shift-reduce bad state as the result of <code>shift setvar</code> applied to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
</code></pre></div>

<p>.)</p>



<a name="213588808"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213588808" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213588808">(Oct 16 2020 at 17:19)</a>:</h4>
<p>To finish the example off, let's make it to the end. The next step is <code>shift ,</code> which takes us to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">o</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">o</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
</code></pre></div>

<p>(and <code>shift y</code> takes us to the simple <code>-&gt; y o</code> state, so we plan to reduce there and come back here with <code>shift setvar</code>), and <code>shift setvar</code> from here takes us to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="n">o</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="n">setvar</span> <span class="n">o</span>
</code></pre></div>

<p>which is another shift reduce conflict. Again, we analyze the conflict to find out what to composite. We want to apply the <code>class -&gt; setvar</code> *production here, which was considered one step ago because closure over <code>-&gt; &lt;. setvar , o class &gt;.</code> required us to add the <code>-&gt; o setvar</code> production to the state. So we composite <code>class -&gt; &lt;. setvar , class &gt;.</code> and <code>class -&gt; setvar</code> to get a new <code>class -&gt; &lt;. setvar , setvar &gt;.</code> production, create a temporary modified version of the previous state</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">o</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">o</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">o</span> <span class="n">setvar</span> <span class="bp">&gt;.</span>
<span class="n">all</span> <span class="n">setvar</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span> <span class="n">except</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">setvar</span>
</code></pre></div>

<p>and use it to calculate the new result of <code>shift setvar</code>, which is</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="n">o</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="n">o</span> <span class="bp">&gt;.</span>
</code></pre></div>

<p>and we have cleared another shift reduce conflict. There is still one more to go, though, since the next step is <code>shift &gt;.</code> which takes us to</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="n">o</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="n">o</span>
</code></pre></div>

<p>which is again a shift reduce conflict. Now we must backtrack a lot, because we have to go back 5 steps (the length of the current reduce candidate) to find out which production required us to put <code>-&gt; &lt;. setvar , setvar &gt;.</code> into the mix. In fact, five steps ago this production was not even <code>-&gt; &lt;. setvar , setvar &gt;.</code> at all but rather <code>-&gt; &lt;. class , class &gt;.</code> but this makes no difference, we need two productions to do the compositing. This is the first state I posted, which looks like</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">}</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span>
</code></pre></div>

<p>where among the <code>class</code> rules is <code>-&gt; o &lt;. class , class &gt;.</code>. The reason the class rules are there is because of the <code>-&gt; { o class }</code> rule, so we composite <code>class -&gt; { class }</code> with <code>class -&gt; &lt;. setvar , setvar &gt;.</code>  to get the temporary state</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="kd">class</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="o">{</span> <span class="n">o</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="o">}</span>
<span class="n">all</span> <span class="kd">class</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="n">rules</span> <span class="n">except</span> <span class="bp">-&gt;</span> <span class="n">o</span> <span class="bp">&lt;.</span> <span class="kd">class</span> <span class="o">,</span> <span class="kd">class</span> <span class="bp">&gt;.</span>
</code></pre></div>

<p>and now shift 5 steps forward along <code>&lt;. setvar , setvar &gt;.</code> to get the repaired state</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="n">o</span> <span class="bp">|</span> <span class="n">wff</span> <span class="o">}</span>
<span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="n">o</span> <span class="o">}</span>
</code></pre></div>

<p>Now we have finally cleared the last hurdle, as we can clearly now either <code>shift |</code> or <code>shift }</code> depending on what we see next to parse both <code>{ &lt;. x , y &gt;. | ph }</code> and <code>{ &lt;. x , y &gt;. }</code>. For the purpose of the example let's say we shift <code>}</code> so we get to state</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">-&gt;</span> <span class="o">{</span> <span class="bp">&lt;.</span> <span class="n">setvar</span> <span class="o">,</span> <span class="n">setvar</span> <span class="bp">&gt;.</span> <span class="o">}</span> <span class="n">o</span>
</code></pre></div>

<p>and a reduce is unambiguous.</p>



<a name="213588821"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213588821" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213588821">(Oct 16 2020 at 17:19)</a>:</h4>
<p>But what are we reducing anyway? I've been talking about compositing rules, and what I haven't been showing here is that each production is associated to a partial expression tree. You can imagine them as lambda expressions. The original rules from the grammar will have a result like <code>\e1 e2. (cpr e1 e2)</code> for the production <code>class -&gt; &lt;. class , class &gt;.</code>, which is to say that we take the two class expressions in the brackets and put them into the two arguments of a <code>cpr</code> node. The arguments aren't always in parse order, for example I think <code>wal</code> takes its arguments in the order <code>wal ph x</code> (because the <code>$f</code> variable declaration of <code>vx</code> comes after <code>wph</code>), so the production <code>wff -&gt; A. setvar wff</code> has result <code>\e1 e2. (wal e2 e1)</code>.</p>
<p>Now compositing rules has the effect of a composition of these two expressions. In the first part we composited <code>class -&gt; &lt;. class , class &gt;.</code> with <code>class -&gt; setvar</code>, with associated results <code>\e1 e2. (cpr e1 e2)</code> and <code>\e1. (cv e1)</code>, so we insert the <code>cv</code> expression in for <code>e1</code> of the <code>cpr</code> expression to get a new result <code>\e1 e2. (cpr (cv e1) e2)</code> for the new production <code>class -&gt; &lt;. setvar , class &gt;.</code>. Similarly, the production <code>class -&gt; &lt;. setvar , setvar &gt;.</code> is formed by inserting <code>cv</code> in for <code>e2</code> in this new production, resulting in <code>\e1 e2. (cpr (cv e1) (cv e2))</code>. And finally, we composited this expression with the <code>class -&gt; { class }</code> production with result <code>\e1. (csn e1)</code>, and this composition yields, for the composite rule <code>class -&gt; { &lt;. setvar , setvar &gt;. }</code>, the result <code>\e1 e2. (csn (cpr (cv e1) (cv e2)))</code>. This is what we reduce with.</p>
<p>So for the example <code>{ &lt;. x , y &gt;. }</code>, we first reduce using <code>setvar -&gt; x := vx</code>, then <code>setvar -&gt; y := vy</code>, then <code>class -&gt; { &lt;. setvar , setvar &gt;. } := \e1 e2. (csn (cpr (cv e1) (cv e2)))</code> to get the final parse tree <code>(csn (cpr (cv vx) (cv vy)))</code>.</p>



<a name="213780249"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213780249" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213780249">(Oct 19 2020 at 12:55)</a>:</h4>
<p><span class="user-mention" data-user-id="110049">@Mario Carneiro</span> Thanks very much! Might take me some time to wrap my head around this since I don't know much about parsers. I'll get back to you if I have questions.</p>



<a name="213780344"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/213780344" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Mario Carneiro <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#213780344">(Oct 19 2020 at 12:55)</a>:</h4>
<p>It is a repost, so the context might not be exactly right for the venue. Feel free to ask if something is out of context</p>



<a name="240042826"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/240042826" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Brando Miranda <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#240042826">(May 24 2021 at 12:16)</a>:</h4>
<p>I'm curious <span class="user-mention" data-user-id="249373">@Stanislas Polu</span>  how different/similar is GPT-f vs the GPT used for coding (<a href="https://analyticsindiamag.com/open-ai-gpt-3-code-generator-app-building/">https://analyticsindiamag.com/open-ai-gpt-3-code-generator-app-building/</a>)?</p>



<a name="240051467"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/240051467" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#240051467">(May 24 2021 at 13:33)</a>:</h4>
<p><span class="user-mention" data-user-id="246156">@Brando Miranda</span> the architecture is the same (smaller model similar in size to GPT-2) but the training objective is quite different (see section 4.2) <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>



<a name="240062907"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/240062907" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Brando Miranda <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#240062907">(May 24 2021 at 15:01)</a>:</h4>
<p><span class="user-mention silent" data-user-id="249373">Stanislas Polu</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/GPT-f.20paper/near/240051467">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="246156">Brando Miranda</span> the architecture is the same (smaller model similar in size to GPT-2) but the training objective is quite different (see section 4.2) <span aria-label="+1" class="emoji emoji-1f44d" role="img" title="+1">:+1:</span></p>
</blockquote>
<p>Thanks Stanslas! I appreciate the message. I will check that out.</p>
<p>Out of curiosity, is there a reason you prefered a transformer model e.g. GPT than an enumerator with a neural recognition model (e.g. as in Dreamcoder, Deepcoder, etc. related work in that path)</p>



<a name="240075113"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/240075113" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Stanislas Polu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#240075113">(May 24 2021 at 16:27)</a>:</h4>
<p>Wellll... Dreamcoder and GPT-f objective are not too dissimilar.</p>
<p>You could view proof search as the wake phase, the model conjecturing capabilities (generating theorem statements during proof search that are not part of the formal library) as the abstraction phase and attempting to prove them as the dream phase.</p>



<a name="240270766"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/GPT-f%20paper/near/240270766" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper.html#240270766">(May 26 2021 at 01:45)</a>:</h4>
<p><span class="user-mention silent" data-user-id="246156">Brando Miranda</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/GPT-f.20paper/near/240042826">said</a>:</p>
<blockquote>
<p>I'm curious <span class="user-mention silent" data-user-id="249373">Stanislas Polu</span>  how different/similar is GPT-f vs the GPT used for coding (<a href="https://analyticsindiamag.com/open-ai-gpt-3-code-generator-app-building/">https://analyticsindiamag.com/open-ai-gpt-3-code-generator-app-building/</a>)?</p>
</blockquote>
<p>You probably know this, but my understanding of GPT-3 applications, is one doesn't retrain the model.  Instead they come up with a prompt to extract information out of the model.  For example to find the capital of England, you would give GPT-3 a prompt like `France =&gt; Paris, China =&gt; Beijing, England =&gt; " and the model will complete the string with "London".  It's crazy, but with good prompt engineering, you can take this really far.  In this way, GPT-3 can code since there was a lot of code (using standard programming languages) in the training data, and all one has to do if find a good way to engineer prompts.</p>
<p>GPT-f (as Stan said) uses a smaller model than GPT-3 (more like GPT-2) and therefore can be fine-tuned as is standard with GPT-2 (and BERT) applications.  GPT-f was pretrained on web text and web math, and we fine tune it with tasks like <code>GOAL P Q : Prop ⊢ ((P → Q) → P) → P PROOFSTEP apply or.elim (em P)</code>, following the pattern <code>GOAL &lt;TacticState&gt; PROOFSTEP &lt;Tactic&gt;</code>.  When using it to predict tactics, you give it the prompt <code>GOAL &lt;TacticState&gt; PROOFSTEP </code> and it fills in the tactic.  (The main insight in our paper is that we get a big boost by also co-training it with a number of other tasks.  These tasks are not used for prediction, but nontheless, really help our model.  The details are in the paper.)</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>