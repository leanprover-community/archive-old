---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Property.20Preserving.20Embedding.20of.20First-order.20Logic.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Property.20Preserving.20Embedding.20of.20First-order.20Logic.html">Paper: Property Preserving Embedding of First-order Logic</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="196136889"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Property%20Preserving%20Embedding%20of%20First-order%20Logic/near/196136889" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Property.20Preserving.20Embedding.20of.20First-order.20Logic.html#196136889">(May 04 2020 at 03:28)</a>:</h4>
<p><a href="https://easychair.org/publications/paper/Cwgq" title="https://easychair.org/publications/paper/Cwgq">Property Preserving Embedding of First-order Logic</a>, Julian Parsert, Stephanie Autherith and Cezary Kaliszyk</p>



<a name="196136916"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Property%20Preserving%20Embedding%20of%20First-order%20Logic/near/196136916" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Property.20Preserving.20Embedding.20of.20First-order.20Logic.html#196136916">(May 04 2020 at 03:29)</a>:</h4>
<p>I haven't read it yet, but they appeared to <a href="http://aitp-conference.org/2019/abstract/paper%2011.pdf" title="http://aitp-conference.org/2019/abstract/paper%2011.pdf">present on this in AITP 2019</a>.</p>



<a name="196137612"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/Paper%3A%20Property%20Preserving%20Embedding%20of%20First-order%20Logic/near/196137612" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Property.20Preserving.20Embedding.20of.20First-order.20Logic.html#196137612">(May 04 2020 at 03:49)</a>:</h4>
<p>Coming up with formula embeddings will be incredibly important for theorem proving.  I'm starting to fully come to grips with this.  I think to understand this, one can think of the analogy with text processing.  For example, predict how a user will rate a movie from their IMDB review.  Early last decade the state of the art was tools like TF-IDF which roughly counts the occurrences of words in the sentence (or document) and compares them to the occurrences in the dataset.  There are similar tools currently being used for formulas which count the symbols or some connected groups of symbols.  These don't take into account the whole formula structure.  Next, came tools like recurrent neural networks, convolutional neural networks, and LSTMs.  These take into account sentence structure, but again they don't take into account the meanings of the symbols except in so far as the task being trained on.  This is similar to current graph neural network approaches to formulas.  Finally, now the best language models are are pre-trained by training on larger datasets of sentences (e.g. wikipedia) in a self-supervised manner.  For example, the goal might be just to predict the next word in a sentence.  Then when one applies this to smaller datasets, like the IMBD datasets, they can "transfer" this knowledge to the smaller dataset.  We are starting to see similar approaches in theorem proving and I'm excited about this.  I could easily image two scenarios.  One, we can basically create algorithms which explore formulas of a certain type (FOL, HOL, DTT, etc) and learn to recognize patterns that can then be applied to specific problems (even on unseen formulas).  Second, we can take embeddings trained on say Coq and apply them to Lean (or vice versa).  I think the similarities in the logic should make the ability to transfer embeddings quite doable.</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>