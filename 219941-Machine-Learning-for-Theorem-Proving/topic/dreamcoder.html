---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html">dreamcoder</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="234245608"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234245608" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> mikey liu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234245608">(Apr 12 2021 at 23:28)</a>:</h4>
<p>hi i've been playing with ATPs for a bit and just saw this paper called dreamcoder. it learns concepts by itself and is able to perform certain nontrivial tasks with just a few examples (sorting, painting specific images, etc.) it is also able to grow on its own so to speak by a process called dreaming, which i think is kinda like a more sophisticated 'exploratory play' in reinforcement learning. i felt like this might work very well with lean and other ATPs in general to further automate theorem proving. here's the link to the paper: <a href="https://arxiv.org/abs/2006.08381">https://arxiv.org/abs/2006.08381</a> and here's its github link (if you wanna install it search for 'singularity 2.5' and install that also): <a href="https://github.com/ellisk42/ec">https://github.com/ellisk42/ec</a></p>



<a name="234259853"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234259853" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234259853">(Apr 13 2021 at 02:41)</a>:</h4>
<p>I’ve been meaning for a long time to write up something on this.  I agree this paper has a lot of potential (so much that I feel if I start talking about all my ideas related to it that I won’t be able to stop <span aria-label="smile" class="emoji emoji-1f642" role="img" title="smile">:smile:</span>).</p>



<a name="234259877"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234259877" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234259877">(Apr 13 2021 at 02:41)</a>:</h4>
<p>If I write something up it will be on the <a class="stream" data-stream-id="219941" href="/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving">#Machine Learning for Theorem Proving</a>  steam.</p>



<a name="234259987"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234259987" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234259987">(Apr 13 2021 at 02:43)</a>:</h4>
<p>Also this might be a coincidence but Yannic Kilcher just made a video on this paper yesterday: <a href="https://m.youtube.com/watch?v=qtu0aSTDE2I">https://m.youtube.com/watch?v=qtu0aSTDE2I</a></p>
<div class="youtube-video message_inline_image"><a data-id="qtu0aSTDE2I" href="https://m.youtube.com/watch?v=qtu0aSTDE2I"><img src="https://i.ytimg.com/vi/qtu0aSTDE2I/default.jpg"></a></div>



<a name="234260101"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234260101" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234260101">(Apr 13 2021 at 02:44)</a>:</h4>
<p>Also Kevin Ellis (the lead author) has a video: <a href="https://m.youtube.com/watch?v=NYIeP1hns6A&amp;feature=youtu.be">https://m.youtube.com/watch?v=NYIeP1hns6A&amp;feature=youtu.be</a></p>
<div class="youtube-video message_inline_image"><a data-id="NYIeP1hns6A" href="https://m.youtube.com/watch?v=NYIeP1hns6A&amp;feature=youtu.be"><img src="https://i.ytimg.com/vi/NYIeP1hns6A/default.jpg"></a></div>



<a name="234260974"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234260974" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234260974">(Apr 13 2021 at 02:59)</a>:</h4>
<p><span class="user-mention" data-user-id="405058">@mikey liu</span> have you been able to run it?  If so how long does it take to train?</p>



<a name="234264026"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234264026" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Notification Bot <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234264026">(Apr 13 2021 at 03:46)</a>:</h4>
<p>This topic was moved here from <a class="stream-topic" data-stream-id="113488" href="/#narrow/stream/113488-general/topic/dreamcoder">#general &gt; dreamcoder</a> by <span class="user-mention silent" data-user-id="110087">Scott Morrison</span></p>



<a name="234299839"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234299839" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> mikey liu <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234299839">(Apr 13 2021 at 10:13)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234259987">said</a>:</p>
<blockquote>
<p>Also this might be a coincidence but Yannic Kilcher just made a video on this paper yesterday: <a href="https://m.youtube.com/watch?v=qtu0aSTDE2I">https://m.youtube.com/watch?v=qtu0aSTDE2I</a></p>
</blockquote>
<p>not a coincidence. i sub to his channel. otherwise i would not be able to dig up something like this.</p>



<a name="234385573"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234385573" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234385573">(Apr 13 2021 at 19:22)</a>:</h4>
<p>This is a really cool paper! <span class="user-mention" data-user-id="115715">@Jason Rute</span> I would love to hear your thoughts on it if you decide to do a writeup. </p>
<p>I can imagine applying this to theorem proving in Lean. </p>
<p>The <strong>wake phase</strong> could be proof search with a model like gpt-f. </p>
<p>The <strong>sleep-abstraction phase</strong> could involve constructing new tactics by searching for frequently used combinations of tactics. Essentially, abstracting new tactics from parts of proofs found during the wake phase. </p>
<p>The <strong>sleep-dreaming phase</strong> appears to require randomly generating new theorems. <span class="user-mention" data-user-id="115715">@Jason Rute</span> , as you know I've been thinking a lot about how to do this in Lean (for others, see this thread: <a href="#narrow/stream/113489-new-members/topic/Inferring.20Theorem.20from.20a.20Proof.20Term.3F">https://leanprover.zulipchat.com/#narrow/stream/113489-new-members/topic/Inferring.20Theorem.20from.20a.20Proof.20Term.3F</a>). </p>
<p>I think being able to generate new theorems is critical to enable open-ended improvement in neural provers. I also think it may be possible to get a feedback loop working between a neural generator and a neural prover (basically a form of asymmetric self-play, unlike the symmetric forms of self-play seen in systems like AlphaZero/MuZero). I have some ideas for how to do this. One of the main problems is how you optimize the theorem generator. A purely adversarial objective (as in GANs) is probably not going to work. One idea is to optimize the generator to produce theorems that are near the limit of the prover's capability. Since the prover uses tree search (again let's assume a gpt-f like model), you can use the branch-factor of the prover's search tree as a metric for how hard the proof was. If the prover fails completely there's nothing you can say, except that it was too hard. But if it succeeds, then branch factor is a measure of how much search was required. An easy proof requires little search (branch factor could be as low as 1, if the search is linear). The prover's search times out after a certain number of steps, so one could optimize the generator (via RL) to produce theorems that when the prover tries to prove them result in a large branch factor but are still provable within the timeout limit. </p>
<p>Would love to know what people think of this idea, especially reasons it might not work or ways one could improve it. </p>
<p>Overall I think the wake-abstraction-dream cycle is a very interesting framework for advancing program synthesis / reasoning. I'm reminded of Francois Chollet's recent <a href="https://slideslive.com/38935790/abstraction-reasoning-in-ai-systems-modern-perspectives">talk</a> on program synthesis. I did a write up on it if you prefer <a href="https://joepalermo.github.io/2021/01/10/talk-review-francois-chollet-neurips-2020.html">reading</a>. Basically he argues that abstraction is the key to generalization, and strong generalization from little experience is the essence of intelligence. Furthermore he argues that while deep learning is great at what he calls "value-centric abstraction" it cannot perform "program-centric abstraction", the solution to which he claims is program synthesis.</p>



<a name="234422021"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234422021" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234422021">(Apr 14 2021 at 00:16)</a>:</h4>
<p>For anyone looking for more details on DreamCoder (like how does it actually work), here is the extended version of the paper: <a href="https://web.mit.edu/ellisk/www/documents/dreamcoder_with_supplement.pdf">https://web.mit.edu/ellisk/www/documents/dreamcoder_with_supplement.pdf</a></p>



<a name="234423012"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234423012" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234423012">(Apr 14 2021 at 00:28)</a>:</h4>
<p>Speaking of Chollet and his views, you may already know this, but there is a group (or two groups?) at MIT, working on applying DreamCoder to Chollet's ARC challenge.  Here is a <a href="https://openreview.net/forum?id=-gjy2V1ko6t">preliminary paper</a>, a <a href="https://iaidl.org/2021/02/24/an-intro-to-the-fast-paced-world-of-artificial-intelligence-mit-news/">news article</a> and an <a href="https://cbmm.mit.edu/video/modular-learning-and-reasoning-arc">informal talk</a>.</p>



<a name="234532622"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234532622" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234532622">(Apr 14 2021 at 16:40)</a>:</h4>
<p>Thanks for sharing that. I was aware some people at MIT were working on applying dream coder to ARC but hadn't seen that talk. It's a very interesting talk. It seems like what they're proposing has a shot at really working on ARC.</p>



<a name="234545426"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234545426" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234545426">(Apr 14 2021 at 17:57)</a>:</h4>
<p>FYI <span class="user-mention" data-user-id="257994">@Ryan Krueger</span> <span class="user-mention" data-user-id="116045">@Jesse Michael Han</span> and I played around with ARC last year. We built an extremely flexible higher-order tactic-based synthesis framework for it (<a href="https://github.com/dselsam/arc">https://github.com/dselsam/arc</a>). It is essentially a prototype of the framework we want to try on IMO. Our ARC solver didn't solve all of the training/eval problems, but I think it is just a few missing features/abstractions away from solving almost all of them.</p>



<a name="234589847"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234589847" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234589847">(Apr 14 2021 at 23:01)</a>:</h4>
<p><span class="user-mention" data-user-id="230999">@Daniel Selsam</span> My understanding of your ARC work (based on Jesse's <a href="https://jesse-michael-han.github.io/blog/imo-gc-geo/">writeup</a>) is something along the lines of that you built a hand crafted DSL of components that had branching points.  Then you efficiently searched over the branching points finding programs which could solve the ARC problem.  When you say "a few missing features/abstractions" do you think that approach is scalable to solving ARC, or do you need machine learning to make the search space efficient (and possibly, like DreamCoder, expand your DSL)?</p>



<a name="234590039"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234590039" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234590039">(Apr 14 2021 at 23:02)</a>:</h4>
<p>And I should add that my understanding is that this DSL is in a new abstraction framework (<code>StateT</code>) which is supposed to simplify the search space for solving problems like this.</p>



<a name="234597691"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234597691" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234597691">(Apr 15 2021 at 00:25)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234589847">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="230999">Daniel Selsam</span> My understanding of your ARC work (based on Jesse's <a href="https://jesse-michael-han.github.io/blog/imo-gc-geo/">writeup</a>) is something along the lines of that you built a hand crafted DSL of components that had branching points. </p>
</blockquote>
<p>Our guiding principle was to <em>not</em> build a DSL. The tactics are arbitrary nondeterministic Haskell code that produce subproblems in arbitrary ways, and specify arbitrary ways of constructing guesses to the original problem given guesses to the subproblems. It is very similar to the Lean tactic framework, except the nondeterminism is reified rather than implicit in <code>&lt;|&gt;</code> calls.</p>
<blockquote>
<p>Then you efficiently searched over the branching points finding programs which could solve the ARC problem. </p>
</blockquote>
<p>We performed (un-optimized) search over the choice points reached by the nondeterministic tactics during execution.</p>
<blockquote>
<p>When you say "a few missing features/abstractions" do you think that approach is scalable to solving ARC, or do you need machine learning to make the search space efficient (and possibly, like DreamCoder, expand your DSL)?</p>
</blockquote>
<p>All three of us agreed to never even glimpse at the eval problems until near the very end. The system we built looking only at the training problems solved most of the training problems (I forget exactly but I think roughly ~70%). Then we ran it on the eval problems and it only got ~20%, but then in one weekend, without changing the architecture at all, we bumped this up to (I forget but roughly) ~60% by adding only (modular) features that we spotted in the eval set along with one or two new (modular) tactics. We didn't change the architecture at all. For both the training and eval sets, there are several other classes of problems that we had promising abstractions for but did not finish implementing, which we think would us get to "almost all". But there is still a pretty long tail of "one-offs" for which we didn't figure out how to write good abstractions for. Even after reaching "almost all", if Chollet studied our solver, he could probably construct an entire test set that humans could solve that our solver would fail on.</p>
<p>I think most of the abstractions need to be built-in, but to some extent, the better the heuristic search, the less precise you need to make the abstractions. So, great heuristics would help and could simplify parts of the codebase. I don't see how DL in particular would help much though. There isn't much data, and generatively modeling the problem distribution well enough to provide good synthetic data seems as hard as building a solver in the first place.</p>



<a name="234598512"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234598512" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234598512">(Apr 15 2021 at 00:36)</a>:</h4>
<blockquote>
<blockquote>
<p>Then you efficiently searched over the branching points finding programs which could solve the ARC problem.</p>
</blockquote>
</blockquote>
<blockquote>
<p>We performed (un-optimized) search over the choice points reached by the nondeterministic tactics during execution.</p>
</blockquote>
<p>This may be a subtle point so let me clarify. There is no DSL and there are no programs. The tactics execute nondeterministically and directly produce <em>guesses</em>. It would be easy to refactor so that the tactics return <em>Haskell</em> functions that map grids to guesses, but it would be a nightmare to try our approach with an actual concrete DSL. Our solver is much more like one big lean tactic than a traditional synthesis engine.</p>



<a name="234662008"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234662008" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234662008">(Apr 15 2021 at 12:00)</a>:</h4>
<p><span class="user-mention" data-user-id="230999">@Daniel Selsam</span>   I clearly remembered it wrong, and this is my fault.  If I understand correctly now, you wrote a nondeterministic tactic that given the three grid pairs and the unpaired grid as a "goal", finds the solution to the last grid.  By "nondeterministic" you mean the tactic has branch points and making the execution of your tactic behave like a (DFS?) tree search, correct?  (I'm unclear what "abstractions" mean here, but I think it is something like sub tactics that your main tactic calls.)  Even with new tactic search framework, it is surprising that you claim you were able to almost solve ARC.  I find this surprising since ARC is considered a big deal in the ML world, but you say this very nonchalantly, and it doesn't sound like you plan to work on it further.  Do you consider this to be a missed opportunity since the official challenge is over, and you have already looked at the eval set? I think Chollet has a whole suit of secret problems he keeps adding to, but I admit I don't know when he is going to open it up for others to check against them, or what he thinks is the right way to work on ARC since data leakage is a real danger.</p>



<a name="234691135"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234691135" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234691135">(Apr 15 2021 at 14:58)</a>:</h4>
<p>For some context, the winner of the ARC Kaggle competition shared these stats on their eval performance:</p>
<p>"Depth 3, without adding flipped images. Faster, but probably worse than a full run.</p>
<p>Total: 419 (Some of the 400 tasks have multiple tests)<br>
Size : 409 (97.6% deduced the correct output size)<br>
Cands: 176 (42% had the correct answer among the generated candidates)<br>
Correct: 164 (39% gave the correct answer among top 3 answers)"</p>
<p>I think a "full run" for them implies searching to a max depth of 4, not sure how much that would improve the 39% eval accuracy they reported. Of course, their test accuracy fell to about 20%.</p>
<p><a href="https://www.kaggle.com/c/abstraction-and-reasoning-challenge/discussion/154597">https://www.kaggle.com/c/abstraction-and-reasoning-challenge/discussion/154597</a></p>



<a name="234705939"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234705939" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234705939">(Apr 15 2021 at 16:11)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234662008">said</a>:</p>
<blockquote>
<p>By "nondeterministic" you mean the tactic has branch points and making the execution of your tactic behave like a (DFS?) tree search, correct? </p>
</blockquote>
<p>Yes. For example, there are many different ways of parsing shapes. Sometimes you need to treat diagonals as connections and sometimes you need to ignore them. Sometimes you need to consider multi-color shapes and sometimes color boundaries denote shape boundaries. There are many more variations. But a tactic will just say <code>shapes &lt;- parse grids</code> where <code>parse</code> is nondeterministic and effectively tries all possibilities behind the scenes.</p>



<a name="234707736"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234707736" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234707736">(Apr 15 2021 at 16:21)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234662008">said</a>:</p>
<blockquote>
<p>(I'm unclear what "abstractions" mean here, but I think it is something like sub tactics that your main tactic calls.)  </p>
</blockquote>
<p>There are a few different types of abstractions. For example:</p>
<ul>
<li>types (shapes, objects, rectangles, axes, ...</li>
<li>features (has uniform frame, nearest unique shape, how many holes in shape, ...)</li>
<li>tactics (align shape, impute motion, remove background, fill blanks, impute permutation, ...)</li>
</ul>
<p>There is a general-purpose (not-ARC-specific) higher-order synthesis engine in <a href="https://github.com/dselsam/arc/tree/master/src/Synth">https://github.com/dselsam/arc/tree/master/src/Synth</a> that is called as a subroutine by most tactics, that makes use of features in a generic way. So adding features (which is extremely modular) improves all the tactics. And adding types opens up the door for adding more features.</p>



<a name="234708093"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234708093" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234708093">(Apr 15 2021 at 16:23)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234662008">said</a>:</p>
<blockquote>
<p>I find this surprising since ARC is considered a big deal in the ML world</p>
</blockquote>
<p>Really? I have never heard anybody talk about it. Why would ML people in particular be interested? No ML-first approach has a chance.</p>



<a name="234710316"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234710316" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234710316">(Apr 15 2021 at 16:36)</a>:</h4>
<p><span class="user-mention silent" data-user-id="115715">Jason Rute</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234662008">said</a>:</p>
<blockquote>
<p>it doesn't sound like you plan to work on it further.</p>
</blockquote>
<p>I do not plan to work on it further. It is an extremely interesting domain, and it was the perfect playground for us to develop our architecture for IMO. It was also absurdly fun. But I think it has a few significant limitations. Now that we know the architecture basically works, it essentially comes down to reverse engineering boring low-level abstractions that are builtin to humans. Second, it is fundamentally ill-posed. One person has a finite set of secrets, and the challenge basically reduces to guessing these secrets. In contrast, once our IMO architecture is in place, I think the tricks we need to write for IMO will be much closer to the tricks that are <em>taught</em> to humans as opposed to built-in. And regarding evaluation, we get <em>new</em> problems every year from all over the world.</p>



<a name="234713049"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234713049" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo (S2'17) <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234713049">(Apr 15 2021 at 16:52)</a>:</h4>
<p><span class="user-mention silent" data-user-id="230999">Daniel Selsam</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234708093">said</a>:</p>
<blockquote>
<p>Really? I have never heard anybody talk about it. Why would ML people in particular be interested? No ML-first approach has a chance.</p>
</blockquote>
<p>As an ML person interested in ARC, I'd say ARC is still pretty niche, but there seems to be a growing subset of researchers who find it very interesting. In my case I'm interested in it precisely because it highlights the limitations of an ML-first approach to AI/AGI.</p>



<a name="234732640"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234732640" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Daniel Selsam <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234732640">(Apr 15 2021 at 18:56)</a>:</h4>
<p>@_<strong>Joe Palermo (S2&#39;17)|337523</strong> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234713049">said</a>:</p>
<blockquote>
<p><span class="user-mention silent" data-user-id="230999">Daniel Selsam</span> <a href="#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/dreamcoder/near/234708093">said</a>:</p>
<blockquote>
<p>Really? I have never heard anybody talk about it. Why would ML people in particular be interested? No ML-first approach has a chance.</p>
</blockquote>
<p>As an ML person interested in ARC, I'd say ARC is still pretty niche, but there seems to be a growing subset of researchers who find it very interesting. In my case I'm interested in it precisely because it highlights the limitations of an ML-first approach to AI/AGI.</p>
</blockquote>
<p>There are some cool things one could try with ML besides just learning heuristics for search. As I mentioned above, new features lift all ships in our solver. One could easily extract additional features from NNs and throw them into the stew.</p>



<a name="234758518"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234758518" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234758518">(Apr 15 2021 at 21:38)</a>:</h4>
<p><span class="user-mention" data-user-id="230999">@Daniel Selsam</span> Thanks. This helps clarify a lot.  (Also you might be interesting in <a href="https://mobile.twitter.com/ChrSzegedy/status/1373420309248843778">making a bet</a> with Szegedy.)</p>
<div class="inline-preview-twitter"><div class="twitter-tweet"><a href="https://mobile.twitter.com/ChrSzegedy/status/1373420309248843778"><img class="twitter-avatar" src="https://pbs.twimg.com/profile_images/909267923687968768/g1HLJMp9_normal.jpg"></a><p>I would happy to bet that we will have a generic AI (simple, general purpose architecture, trained from generally available data) by 2025 that will be able to solve 95+% of the ARC problems.

(Not that I think that ARC is the most challenging benchmark one could come up with.) <a href="https://t.co/W3qip5JYtQ">https://twitter.com/fchollet/status/1373404016915357699</a></p><span>- Christian Szegedy (@ChrSzegedy)</span></div></div>



<a name="234765832"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234765832" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Vladislav Bezhentsev <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234765832">(Apr 15 2021 at 22:42)</a>:</h4>
<p>By the way, here is an interesting question. Is it possible to modify DreamCoder in such a way that it becomes capable of iteratively refining itself? I mean DreamCoder learns to generate programs that solve arbitrary given tasks. So, in theory, you can ask DreamCoder to generate programs "that learn to generate programs that solve arbitrary given tasks".</p>



<a name="234772245"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234772245" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234772245">(Apr 15 2021 at 23:42)</a>:</h4>
<p>I think you are giving DreamCoder way too much credit for what it is capable of.</p>



<a name="234890196"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/234890196" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#234890196">(Apr 16 2021 at 17:14)</a>:</h4>
<p><span class="user-mention" data-user-id="371437">@Vladislav Bezhentsev</span> Let me give a better answer.  I think in <em>theory</em> one could have an outer DreamCoder which has as one of its DSL primitives a function <code>dreamcoder</code> whose inputs are a DSL (given as a list of functions), a number of steps to run, and a list of example problems.  The output would be both a new, expanded DSL as well as solutions to the given problems.  The outer DreamCoder can call the inner DreamCoder with a curated DSL and problem list.  I do however see a lot of initial problems getting something like this off the ground.  DreamCoder (and other program synthesis engines) are designed around the programs it generates being fast since it needs to explore a large space of programs.  The inner <code>dreamcoder</code> function is not fast at all.  (This is for a similar reason that hyperparameter tuning usually doesn't use reinforcement learning with neural networks.  Training the (inner) model is time consuming so you only have a few datapoints to use for hyper parameter selection.  Instead one usually uses Bayesian optimization for the search which is really data efficient.)  Now we shouldn't rule out code which programs itself when the Singularity comes <span aria-label="smile" class="emoji emoji-1f642" role="img" title="smile">:smile:</span>, but I personally think DreamCoder isn't anywhere close to up to this yet.</p>



<a name="235592091"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/235592091" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#235592091">(Apr 21 2021 at 22:28)</a>:</h4>
<p>I'm very interested in trying out DreamCoder.  I have a number of toy tasks (and if those work, more interesting tasks) to try out on it.  Has anyone here (<span class="user-mention" data-user-id="405058">@mikey liu</span>, <span class="user-mention" data-user-id="337523">@Joe Palermo</span>, <span class="user-mention" data-user-id="321696">@Julian Berman</span>) tried to run it, either on the predesigned training sets or on custom ones?  I'm going to work through the documentation, but if anyone has any advice, I'd appreciate it.</p>



<a name="235605902"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/235605902" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Joe Palermo <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#235605902">(Apr 22 2021 at 01:12)</a>:</h4>
<p><span class="user-mention" data-user-id="115715">@Jason Rute</span> I haven't but I'm very interested in trying it also. </p>
<p>For others, here's the repo: <a href="https://github.com/ellisk42/ec">https://github.com/ellisk42/ec</a></p>



<a name="235753693"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/dreamcoder/near/235753693" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Julian Berman <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder.html#235753693">(Apr 22 2021 at 21:48)</a>:</h4>
<p>I got as far as installing it but didn't use it yet personally. I put it on my list too though, might give it a shot tomorrow</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>