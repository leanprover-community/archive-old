---
layout: archive
title: Zulip Chat Archive
permalink: /stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html
---

<h2>Stream: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/index.html">Machine Learning for Theorem Proving</a></h2>
<h3>Topic: <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html">gptf generating theorems</a></h3>

<hr>

<base href="https://leanprover.zulipchat.com/">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="236695144"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/gptf%20generating%20theorems/near/236695144" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Kevin Buzzard <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html#236695144">(Apr 29 2021 at 15:30)</a>:</h4>
<p>I was showing one of my sons gptf (in the lean-liquid repo) and he suggested that instead of training GPT to generate proofs, could one use it to generate theorem statements? I am not a computer scientist but this sounded like a reasonable enough question to me!</p>



<a name="236697437"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/gptf%20generating%20theorems/near/236697437" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jesse Michael Han <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html#236697437">(Apr 29 2021 at 15:45)</a>:</h4>
<p>yeah this would be a fun experiment! this has been studied elsewhere: the N2Formal team had some <a href="https://openreview.net/pdf?id=YmqAnY0CMEy">experiments</a> with unconditional generation in HOL Light</p>
<p>currently the model is trained on theorem naming: lean statement -&gt; <code>theorem_name_in_mathlib</code>, one could reverse the role of the names and statements and then try seeing what happens when you supply a reasonable-looking theorem name it's never seen during training</p>
<p>one way you can try playing with (goal-conditioned) conjecturing now is to use <code>gptf {prefix := "have :"}</code> to force it to synthesize a have-statement</p>



<a name="236697715"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/gptf%20generating%20theorems/near/236697715" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Alex J. Best <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html#236697715">(Apr 29 2021 at 15:46)</a>:</h4>
<p>If you visit <a href="https://beta.openai.com/playground?model=formal-lean-wm-to-tt-m1-m2-v4-c4">https://beta.openai.com/playground?model=formal-lean-wm-to-tt-m1-m2-v4-c4</a> and sign in (assuming you signed up for the beta they announced some time back) you can play around with just clicking submit and  having the model generate statements and then try and prove them. Basically given any prefix gptf will try and complete it, so you can start with something like </p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="bp">&lt;|</span><span class="n">endoftext</span><span class="bp">|&gt;</span><span class="n">LN</span><span class="o">]</span> <span class="n">GOAL</span> <span class="n">α</span> <span class="o">:</span> <span class="kt">Type</span> <span class="n">u_1</span><span class="o">,</span> <span class="n">_inst_1</span> <span class="o">:</span> <span class="n">group</span> <span class="n">α</span><span class="o">,</span>  <span class="n">g</span> <span class="n">h</span> <span class="o">:</span> <span class="n">G</span> <span class="bp">⊢</span> <span class="n">g</span> <span class="bp">*</span> <span class="n">h</span>
</code></pre></div>
<p>and click submit to see what gptf thinks this term should be, I just tried this a couple of times and got <code>g * h = g * h</code> by <code>rfl</code>, but more interestingly also the theorem</p>
<div class="codehilite" data-code-language="Lean"><pre><span></span><code><span class="n">g</span> <span class="bp">*</span> <span class="n">h</span> <span class="bp">*</span> <span class="n">h</span><span class="bp">⁻¹</span> <span class="bp">=</span> <span class="n">g</span> <span class="n">PROOFSTEP</span> <span class="n">rw</span> <span class="o">[</span><span class="n">mul_assoc</span><span class="o">,</span> <span class="bp">←</span> <span class="n">gpow_add</span><span class="o">,</span> <span class="n">add_right_cancel_iff</span><span class="o">,</span> <span class="n">one_mul</span><span class="o">]</span>
</code></pre></div>
<p>not sure that the proof works in this case <span aria-label="grinning" class="emoji emoji-1f600" role="img" title="grinning">:grinning:</span></p>



<a name="236698387"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/gptf%20generating%20theorems/near/236698387" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jesse Michael Han <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html#236698387">(Apr 29 2021 at 15:51)</a>:</h4>
<p>neat, thanks Alex! yeah the playground at <code>beta.openai.com</code> might be your best bet for playing with the model while avoiding all the <code>gptf</code> theorem proving machinery</p>



<a name="236699434"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/gptf%20generating%20theorems/near/236699434" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html#236699434">(Apr 29 2021 at 15:58)</a>:</h4>
<p>And if you want top level theorems with no assumptions, use the prefix <code>GOAL    ⊢</code> or <code>TYPE</code> (and whatever boilerplate you need at the beginning).  The <code>TYPE</code> keyword might be better even since that was just trained on Lean top level theorems (and definitions, maybe).</p>



<a name="236700001"></a>
<h4><a href="https://leanprover.zulipchat.com/#narrow/stream/219941-Machine%20Learning%20for%20Theorem%20Proving/topic/gptf%20generating%20theorems/near/236700001" class="zl"><img src="https://leanprover-community.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Jason Rute <a href="https://leanprover-community.github.io/archive/stream/219941-Machine-Learning-for-Theorem-Proving/topic/gptf.20generating.20theorems.html#236700001">(Apr 29 2021 at 16:01)</a>:</h4>
<p>See Figure 4 in <a href="https://arxiv.org/pdf/2102.06203.pdf">https://arxiv.org/pdf/2102.06203.pdf</a> for all the available keywords.</p>



{% endraw %}

<hr><p>Last updated: Jan 25 2023 at 00:06 UTC</p>