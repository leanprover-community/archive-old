[
    {
        "content": "<p>This isn't a question about lean per se, but given that lean is based on CiC, I thought people in this community might be able to answer a question I've had for a while: What is the importance of the curry-howard isomorphism / propositions as types representation mechanism for formal proof verification? </p>\n<p>There are plenty of proof assistants that don't follow this paradigm so that makes me ask why follow this unusual formalism (for most mathematicians) at all? I mean, it must have some very compelling advantages and I wonder what those are.  </p>\n<p>The term that immediately comes to mind to explain its advantages is \"computational content\". Formalisms based on the lambda calculus have computational content because terms of the lambda calculus are programs and \"run\" through beta reduction. I don't see how this benefits formalization at all. How can a computer use this to verify a proof is correct? It is easy to conceive of some program that can run through the steps of a proof and verify it is all correct. I can't conceive of proofs themselves being programs as somehow helping this process. The only thing remotely relevant in my mind would be how a lambda term denoting <br>\nan arithmetic expression, e.g. (2+2)*3, can be evaluated to an integer by running the program it is encoded as. </p>\n<p>Now evidently I don't entirely understand what this term means as people will say, dependent types for instance have a computational interpretation, and a big problems apparently is how adding axioms to these systems, for instance function extensionality or the univalence axiom, may not have a computational interpretation. My understanding at the  present goes that each term describes an algorithm for constructing a term of a given type.  The function type operator is a primitive constructor or algorithm for constructing a terms of one type out of a term of another type.  But what is to stop you from asserting other axioms have a computational interpretation?</p>\n<p>For example function extensionality would be a term of the type <br>\n(A)(B)(f:A\\toB)(g:A\\toB)((x:A)(f(x)=g(x))\\to(f=g)).<br>\n(apologies for the awful notation) What is to stop us from regarding this as a primitive construction? In terms of classical logic this is like adding an extra axiom and I would ask \"is this consistent\", but I'm not sure what the right question to ask here would be. It must be the case that the formal system has certain properties or not whereby we deem such an axiom to have computational content or not. Such formal properties may suggest what advantages \"computational content\" confers upon a proof assistant but I don't even know what these properties would be at this  point. (canonicity property?... and how is it even possible to have a closed term of type int that doesn't evaluate to a numeral?)</p>\n<p>Along these lines is the decidability of type checking. This is important because type checking corresponds to proof checking. As I understand it type checking is decidable if we have an algorithm to determine if a term in a given context is of a given type, but isn't this redundant in a proof assistant where the entire point is to interactively construct a term of a given type. You wouldn't need to check a term is of a given type because your construction is already an algorithm to verify this. </p>\n<p>The second possible line of advantages in my mind is that type theory just more closely reflects how most mathematicians think. Cue remarks on synthetic euclidean geometry and nonsense terms you can form in set theory such as the intersection of + with the number 9. I could regard this as a compelling reason for CiC, just not in isolation. So really I'm more curious on the computational angle here.</p>",
        "id": 216026951,
        "sender_full_name": "Robin Allison",
        "timestamp": 1604872032
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"238830\" href=\"/#narrow/stream/238830-Lean-for-the-curious-mathematician-2020/topic/Why.20use.20propositions.20as.20types.3F\">#Lean for the curious mathematician 2020 &gt; Why use propositions as types?</a> by <span class=\"user-mention silent\" data-user-id=\"123965\">Bryan Gin-ge Chen</span></p>",
        "id": 216030880,
        "sender_full_name": "Notification Bot",
        "timestamp": 1604877737
    },
    {
        "content": "<p>My personal take on this as a mathematician is that if you're going to formalise, you have to choose your foundations, and there currently seem to be three viable choices: set theory (e.g. metamath), HOL (e.g. Isabelle/HOL) or dependent type theory (e.g. Coq, Lean). Metamath has no tactics, and Isabelle has no dependent types which makes it very good for 19th century mathematics but problematic for a bunch of 20th century mathematics. So if you want to do 20th and 21st century mathematics you're forced to use dependent types, and now this whole Curry-Howard thing is just part of the furniture. One big difference between Coq and Lean is that in Coq they place a great emphasis on constructive logic; in Lean we seem to have firmly rejected this point of view, even to the extent that polynomials are not computable. I think it's no coincidence that this aggressively non-computational viewpoint has enabled us to prove a ton of theorems which aren't in any other prover, but has given us a maths library which is very bad at doing computations, or perhaps \"not optimised for computations\" is a better way of putting it. We do not usually prove things using \"heavy refl\"s, preferring to reason with rewrites. In short then, propositions as types is part of the furniture, but often we don't care about the computational consequences of this because we are using the axiom of choice and things like functional extensionality and LEM all over the place in the maths library, thus disabling the ability to compute anyway. I rant about this more <a href=\"https://xenaproject.wordpress.com/2019/06/15/proofs-are-not-programs/\">here</a>. Hopefully someone else will come along and give another point of view though -- this is just my personal take on things.</p>",
        "id": 216031165,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1604878158
    },
    {
        "content": "<p>I agree with Kevin for the most part. I would say that the mathlib community is not at all representative of \"real type theory\", and we tend to use lean <em>despite</em> DTT rather than <em>because</em> of it. Coq is a much better place to find DTT true believers.</p>\n<p>Nevertheless, you have already identified some of the small advantages of having a computational interpretation, or more specifically a nontrivial \"definitional equality\" judgment: it allows you to silently unfold things when applying other operations, for example if some definition unfolds to an <code>and</code> then you can directly pattern match on it to construct or destruct it. Overuse of this feature is considered a code smell, but it is used in myriad small ways in mathlib.</p>\n<p>As for \"heavy refl\"s, which is to say computations that perform many reduction steps in the kernel, this is very rarely taken advantage of in mathlib, and the few places we do use it are failed experiments (<code>ring2</code>) or current bottlenecks (<code>omega</code>), largely because lean's kernel is not really built to handle heavy computation efficiently. Instead, computation tactics produce proofs on the fly, which is a mechanism that works in any theorem prover, whether or not it has a computational interpretation.</p>\n<p>Finally, there is the question of \"propositions as types\" itself. Since lean has definitional axiom K (proof irrelevance), while propositions are types they are really trivial types, basically just <code>true = {trivial}</code> and <code>false = {}</code>. This does allow for some unification of theories between propositions and types, via the <code>Sort u</code> universe, which is parametric over <code>Prop</code> (propositions) and <code>Type u</code> (\"proper\" types). However, this also leads to some messy universe level arithmetic, and furthermore a large fraction of mathlib is restricted to <code>Type u</code> because small elimination makes propositions the \"odd man out\" in a number of ways, so it's not clear to me whether this amounts to a net gain.</p>",
        "id": 216035030,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1604883933
    },
    {
        "content": "<blockquote>\n<p>and how is it even possible to have a closed term of type int that doesn't evaluate to a numeral?</p>\n</blockquote>\n<p>here's one:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"k\">#reduce</span> <span class=\"bp\">@</span><span class=\"n\">eq.rec_on</span> <span class=\"kt\">Prop</span> <span class=\"o\">(</span><span class=\"n\">true</span> <span class=\"bp\">=</span> <span class=\"n\">true</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">a</span><span class=\"o\">,</span> <span class=\"n\">a</span> <span class=\"bp\">→</span> <span class=\"n\">ℤ</span><span class=\"o\">)</span> <span class=\"n\">true</span>\n  <span class=\"o\">(</span><span class=\"n\">eq_true_intro</span> <span class=\"o\">(</span><span class=\"n\">eq.refl</span> <span class=\"n\">true</span><span class=\"o\">))</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">_</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span> <span class=\"n\">trivial</span>\n</code></pre></div>\n<p>The evaluation of the numeral gets stuck at the <code>eq_true_intro</code> which unfolds to <code>propext</code> which is an axiom</p>",
        "id": 216035405,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1604884455
    },
    {
        "content": "<p>it's provably equal to <code>2</code> but not definitionally equal to <code>2</code></p>",
        "id": 216035412,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1604884475
    },
    {
        "content": "<p>It is true that constructivists often find mystical significance in the conflation of propositions and data, but from my point of view, propositions as types is just a convenient way of representing mathematics, even for classical mathematicians. It is easy to interpret propositions as types in conventional mathematical terms, so there is nothing metaphysically dubious going on. (See the explanation in TPiL.) But it is often an advantage to use the same notation, parsing, type-checking, unification, and elaboration procedures for objects, data types, propositions, and proofs, i.e. nice to have a uniform language of defining all those things. And it is often convenient to put data and axiomatic assumptions in the same definition of a structure.</p>",
        "id": 216241087,
        "sender_full_name": "Jeremy Avigad",
        "timestamp": 1605027735
    },
    {
        "content": "<p>Some more comments about canonicity/computational interpretations.</p>\n<ul>\n<li>\n<p>There are plenty of closed terms of type <code>nat</code> which we don't know how to reduce to a literal: for example, <code>n : nat</code> defined as the maximum rank of an elliptic curve over the rationals, or 0 if there are curves of arbitrary large rank. In some sense, math is entirely about terms like this one which we can't simply evaluate, and not about terms like <code>(2+2)*3</code>. (I've seen computer scientists and even philosophers of mathematics write things along the lines of \"one of the fundamental principles of mathematics is that each closed term denoting a natural number can be reduced to a constant\", but it's completely at odds with how classical mathematics operates. One place where we do have common ground with computer scientists is that we agree that the process of <em>proof checking</em> is, in principle, completely mechanical, but this tends to be regarded as \"merely\" logic or foundations or outside of mathematics entirely.)</p>\n</li>\n<li>\n<p>I'm not sure whether the notion of \"computational interpretation\" has an exact meaning, but I think one manifestation of it is having a specific algorithm which can reduce any closed term of type <code>nat</code> to a literal. If you are a computer scientist and think of a term of type <code>nat</code> as a program which produces a natural number, then you would certainly expect to have such an algorithm! One of the design goals of type theories like CiC is that we do in fact have such an algorithm which works for the base system without any added axioms. However, if this algorithm encounters an application of an axiom, such as the univalence axiom, it gets stuck. The problem then is to find a better algorithm that works even in the presence of the univalence axiom. It doesn't do any good to merely \"assert\" the existence of a computational interpretation of univalence, because it would change the meaning of \"algorithm\".</p>\n</li>\n<li>\n<p>As a practical matter, the canonicity property relies on the ability to unfold definitions and so it appears to be incompatible with abstraction and implementation hiding. In mathematics, we frequently prove that there exists a unique object with some property, and then regard that as defining the object rather than working directly with its construction. For example, the reals are the unique complete ordered field and it would be weird for a mathematician to have an opinion about whether the reals are \"really\" defined using Cauchy sequences or Dedekind cuts. But it seems that a system with the canonicity property would have to expose this choice.</p>\n</li>\n</ul>",
        "id": 216267004,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605038987
    },
    {
        "content": "<blockquote>\n<p>But it seems that a system with the canonicity property would have to expose this choice.</p>\n</blockquote>\n<p>That depends on how \"exposed\" the computational interpretation is to the logic. If it is like a regular programming language where you don't reason about it at all, you just run programs at the end of the day, then there is no need for the particular implementation to be accessible, so you can still have a well defined API with \"implementation details\" that change the way something evaluates but not the theorems that can be proven about the definition.</p>",
        "id": 216268463,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605039677
    },
    {
        "content": "<p>An easier example than Reid's -- every sufficiently large natural number is known to be the sum of 7 natural cubes, and there are infinitely many natural numbers which are not the sum of three natural cubes. So <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo stretchy=\"false\">(</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G(3)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord\">3</span><span class=\"mclose\">)</span></span></span></span>, the smallest value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> for which every sufficiently large natural number is the sum of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> natural cubes, is known to be between 4 and 7, but we don't know what it is and as far as I know there is no known algorithm which will figure it out in a finite time. <a href=\"https://en.wikipedia.org/wiki/Waring%27s_problem\">Known results</a> about this function are tabulated on Wikipedia's page on Warings problem. Hilbert (himself a proponent of non-constructivism) proved that the function existed, but we still have no formula for it, although there is an MSC code for the problem, indicating that it is still an active area of research.</p>",
        "id": 216268527,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605039717
    },
    {
        "content": "<p>One topic I didn't realize until watching one of Kevin's talks is that there is a fair bit of overlap between how working mathematicians view these matters and how system programmers (as opposed to the more language-oriented computer scientists).  Even if there is a decision procedure or algorithm it may be too slow for actual usage.<br>\nThe ability to unfold definitions for reasoning purposes is often really important in proving properties about systems, because they are often sort of defined to meet some particular features and often do not have explicit nice invariants to reason about.  That's true for mathematics to, but it seems like a lot more thought is put into what is being defined.</p>",
        "id": 216269636,
        "sender_full_name": "Joe Hendrix",
        "timestamp": 1605040178
    },
    {
        "content": "<p>Yeah, I didn't state the third point very well. Maybe it would be better to just say that the premise behind canonicity is that things reducing is good and stuck terms are bad, but in practice, too much reduction or the wrong kind of reduction can also be bad.</p>",
        "id": 216270345,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605040475
    },
    {
        "content": "<p>There was some noise relatively recently about some conjecture in type theory being disproved and people were saying \"so, this puts Lean in a bad position, right?\" and I was just thinking \"I have no idea what this conjecture says but I don't think it has anything to do with mathematics\"</p>",
        "id": 216276165,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605043237
    },
    {
        "content": "<p><a href=\"https://artagnon.com/articles/leancoq\">https://artagnon.com/articles/leancoq</a> says \"There are three axioms in Lean: propositional extensionality, quotient soundness, and choice; however, these don't block computation, since computation is done in a VM. They do, however, break good type theoretic properties like strong normalization, subject reduction, and canonicity — this was a conscious design choice. \" but I am a Lean user doing mathematics in Lean without a clue about what strong normalization, subject reduction and canonicity even mean.</p>",
        "id": 216276639,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605043445
    },
    {
        "content": "<p>Perhaps I should add that I use the three axioms all the time in my work, and indeed some of the theorems I prove would not be provable without them. They are all regarded as completely uncontroversial by most working mathematicians.</p>",
        "id": 216276714,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605043476
    },
    {
        "content": "<blockquote>\n<p>They do, however, break good type theoretic properties like strong normalization, subject reduction, and canonicity — this was a conscious design choice. \" but I am a Lean user doing mathematics in Lean without a clue about what strong normalization, subject reduction and canonicity even mean.</p>\n</blockquote>\n<p>You haven't heard about them because they don't hold for lean so you wouldn't be able to leverage them anyway. It's just a thing a type system can have that lean doesn't</p>",
        "id": 216277202,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605043694
    },
    {
        "content": "<p>I like the Waring's problem example better. These examples are really just versions of \"if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">P</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span></span></span></span> holds then 1, else 0\", but dressed up in such a way that it would never occur to most mathematicians that there could be some doubt about whether it actually denotes a natural number.</p>",
        "id": 216279028,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605044691
    },
    {
        "content": "<blockquote>\n<p>a maths library which is very bad at doing computations, or perhaps \"not optimised for computations\"</p>\n</blockquote>\n<p>Oh yeah I noticed this, I did wonder whether one could build some kind of support or plugin for that</p>",
        "id": 216907463,
        "sender_full_name": "Alena Gusakov",
        "timestamp": 1605549404
    },
    {
        "content": "<p>As someone who's really inexperienced I welcome corrections or criticisms but I wonder if it would be difficult to set up some sort of system within Lean where you can disable axioms you don't want to use or something like that</p>",
        "id": 216907725,
        "sender_full_name": "Alena Gusakov",
        "timestamp": 1605549528
    },
    {
        "content": "<p>You can use <code>#print axioms</code> to see if you have avoided an axiom</p>",
        "id": 216907829,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605549576
    },
    {
        "content": "<p>However, lean has very few axioms which makes it difficult to carve out lots of logical subsystems. For example you can't have just the implicational fragment of intuitionistic logic without <code>or</code>, because <code>or</code> axiomatization counts as <code>no axioms</code></p>",
        "id": 216908042,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605549644
    },
    {
        "content": "<p>There are really only three subsystems of interest: <code>no axioms</code>, <code>propext</code> + <code>quot.sound</code>, and <code>propext</code> + <code>quot.sound</code> + <code>classical.choice</code>. However avoiding choice in lean also means avoiding the law of excluded middle, so it's not like ZF, and use of <code>classical.choice</code> is not directly related to whether or not the term is <code>computable</code> in the sense of generating bytecode, so it's all a bit useless</p>",
        "id": 216908674,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605549895
    },
    {
        "content": "<p>The <code>no axioms</code> fragment satisfies canonicity (AFAIK), but the others do not, and none of them are strongly normalizing</p>",
        "id": 216908939,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605550012
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"303889\">Alena Gusakov</span> <a href=\"#narrow/stream/236446-Type-theory/topic/Why.20use.20propositions.20as.20types.3F/near/216907463\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>a maths library which is very bad at doing computations, or perhaps \"not optimised for computations\"</p>\n</blockquote>\n<p>Oh yeah I noticed this, I did wonder whether one could build some kind of support or plugin for that</p>\n</blockquote>\n<p>The state of the art for this in Lean is <code>norm_num</code>, which can \"compute\" in the real numbers (for example) even though the ring structure of the reals is not <code>computable</code>.</p>",
        "id": 216910200,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605550565
    },
    {
        "content": "<p>There's \"computation\" in the type theorists' sense of terms reducing by definitional equalities to other terms, and then there's \"computation\" in the computer algebra sense which is more generally about algorithms for calculating or proving equalities which might be only propositional, and is closer to what mathematicians mean by \"computation\" (which I won't attempt to define!)</p>",
        "id": 216910458,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605550700
    },
    {
        "content": "<p>and I guess technically Lean's <code>computable</code> is not the same as any of these, but it's closest to the type theory one in that it involves a built-in evaluation procedure</p>",
        "id": 216910805,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605550855
    },
    {
        "content": "<p>I am pretty sure I could convince most mathematicians that<code>ring</code> was \"doing computation\".</p>",
        "id": 216911299,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605551058
    },
    {
        "content": "<p>well, be careful they don't agree with that statement too easily - computers compute</p>",
        "id": 216911379,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605551097
    },
    {
        "content": "<p>\"Lean is not a computer\" -- K. Lau, 2017.</p>",
        "id": 216911722,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605551261
    },
    {
        "content": "<p>Kenny would occasionally say these things to me when he was a first year, and they would increase my understanding of what Lean was. The other one I'm fond of quoting was \"Lean does not do magic\" (when I claimed that <code>simp</code> solving a goal was magic)</p>",
        "id": 216911920,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1605551343
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/236446-Type-theory/topic/Why.20use.20propositions.20as.20types.3F/near/216911299\">said</a>:</p>\n<blockquote>\n<p>I am pretty sure I could convince most mathematicians that<code>ring</code> was \"doing computation\".</p>\n</blockquote>\n<p>Right but even worse is that a mathematician will \"compute\" some homotopy group or whatever using the Serre spectral sequence, even though there's no algorithm involved because we might have to use some arbitrary other information to decide whether some differential exists</p>",
        "id": 216911922,
        "sender_full_name": "Reid Barton",
        "timestamp": 1605551343
    },
    {
        "content": "<p>When you say \"computation\" in the type theorists' sense, is it the same thing as decision problems? Or is that the second one (I guess it is about algorithms so I'm guessing the second)</p>",
        "id": 216911936,
        "sender_full_name": "Alena Gusakov",
        "timestamp": 1605551348
    },
    {
        "content": "<p>Decision procedures can be expressed in lean as computations in either sense</p>",
        "id": 216912195,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605551468
    },
    {
        "content": "<p><code>norm_num</code> is a decision procedure for less than on natural numbers in the second sense, <code>dec_trivial</code> is a decision procedure for less than on natural numbers in the first sense</p>",
        "id": 216912289,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1605551514
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/236446-Type-theory/topic/Why.20use.20propositions.20as.20types.3F/near/216908939\">said</a>:</p>\n<blockquote>\n<p>The <code>no axioms</code> fragment satisfies canonicity (AFAIK), but the others do not, and none of them are strongly normalizing</p>\n</blockquote>\n<p>How is the <code>no axioms</code> fragment not strongly normalizing?</p>",
        "id": 216948361,
        "sender_full_name": "Pedro Minicz",
        "timestamp": 1605569902
    },
    {
        "content": "<p>See <a href=\"#narrow/stream/113488-general/topic/Normalization.20fails.20in.20lean\">this thread</a>.</p>",
        "id": 216948836,
        "sender_full_name": "Bryan Gin-ge Chen",
        "timestamp": 1605570248
    },
    {
        "content": "<p>Is it something intrinsic that makes Lean (4) incompatible with HoTT or is it just the conventions of mathlib? <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>  said above that Lean has definitional axiom K.<br>\nBut if we chose to have propositions in <code>Type</code> instead of <code>Prop</code>would that be sufficient to develop HoTT? Would higher inductive types be definable? Could we define and use the univalence axiom?</p>",
        "id": 239836700,
        "sender_full_name": "Nasos Evangelou-Oost",
        "timestamp": 1621650668
    },
    {
        "content": "<p>The equation compiler is intrinsically un-HoTT, apparently, and you can't really \"turn it off\". This is why the <code>hott3</code> repository linked above uses an attribute, which via some metaprogramming inspects each declaration you make to see if it uses incompatible features of the language.</p>",
        "id": 239843096,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1621657763
    },
    {
        "content": "<p>You could do it synthetically, but you would get no help from the built-in commands and tactics. At which point - why bother?</p>",
        "id": 239849635,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1621664988
    },
    {
        "content": "<p>This is another rephrasing of Scott's point, but the existence of <code>Eq</code> in <code>Prop</code> (which exists in the core library, and even if you replaced core it can still be written as an inductive without using any \"axioms\") implies that if you define <code>Eq'</code> in <code>Type</code> then <code>Eq'</code> also satisfies axiom K (not definitionally, but with either <code>Eq</code> or <code>Eq'</code> equality). So you have to watch out for a whole collection of constants that lean thinks are \"no-axioms\" fine, which is what the <code>hott3</code> mode does.</p>",
        "id": 239857776,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621673560
    },
    {
        "content": "<blockquote>\n<p>Would higher inductive types be definable? </p>\n</blockquote>\n<p>Not really. Lean has built in quotients, which turn out to be strong enough to do a wide variety of HITs (<span class=\"user-mention\" data-user-id=\"111080\">@Floris van Doorn</span> has a paper on this IIRC), but there is no <code>inductive</code> support for HITs, and you don't get any HITs that can't be built from regular inductives and quotients .</p>",
        "id": 239857888,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621673736
    },
    {
        "content": "<p>Yeah, you get most set level (\"axiom K-compatible\") higher inductive types just from inductive types + quotients. There are some recursive set-level HITs that cannot easily defined this way, they are usually called \"quotient inductive types\". For example, you can constructively define initial algebras of infinitary algebraic theories. In this case, classically you can construct these also using the axiom of choice, but there might also be quotient inductive types that cannot be defined in Lean.</p>",
        "id": 239888579,
        "sender_full_name": "Floris van Doorn",
        "timestamp": 1621705483
    }
]