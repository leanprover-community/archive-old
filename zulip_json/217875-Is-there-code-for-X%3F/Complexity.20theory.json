[
    {
        "content": "<p>Hello! Has anything from Complexity theory already been formalized in Lean? I have just found Computability (in the mathlib library).</p>\n<p>I'd like to do a Ph.D. on formalizing Complexity theory in Lean — probably from scratch; however, if there was something already done, I might want to build on top of that instead.</p>",
        "id": 245792264,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1626164659
    },
    {
        "content": "<p>I'm not aware of any complexity theory in Lean. You might be interested in the recent papers on complexity theory in Coq via the λ calculus: <a href=\"https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=13915\">https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=13915</a> <a href=\"https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=13914\">https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=13914</a></p>",
        "id": 245793486,
        "sender_full_name": "Anne Baanen",
        "timestamp": 1626165399
    },
    {
        "content": "<p>The existing material on computability was written with an eye for complexity theory (and it is mentioned as future work and motivation in the paper), but not much in that direction has landed yet. I think <span class=\"user-mention\" data-user-id=\"319356\">@Pim Spelier</span> has some unmerged complexity theory work on P and NP</p>",
        "id": 245894468,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1626213180
    },
    {
        "content": "<p>see <a href=\"#narrow/stream/113489-new-members/topic/Computability.2C.20P.20.28and.20NP.29\">https://leanprover.zulipchat.com/#narrow/stream/113489-new-members/topic/Computability.2C.20P.20.28and.20NP.29</a></p>",
        "id": 245894605,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1626213257
    },
    {
        "content": "<p>Very exciting! I'll have a look at it after I learn the basics.</p>",
        "id": 245932488,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1626251415
    },
    {
        "content": "<p>Has there been some thought on proving time complexity bound for algorithms? <br>\nFor example, say my task is sorting. And we have different algorithms, insertion sort, quick sort, ... <br>\nWe care about 2 things: </p>\n<ol>\n<li>the algorithm solves the task </li>\n<li>the algorithm runs in O(T).</li>\n</ol>\n<p>Is there a framework / examples to describe these ideas?</p>\n<p>(extension) We may care about different computation model (e.g. turing machine, random access, ...) in the future.</p>",
        "id": 248462154,
        "sender_full_name": "david",
        "timestamp": 1628161032
    },
    {
        "content": "<p><a href=\"https://leanprover-community.github.io/mathlib_docs/find/list.sorted_merge_sort\">docs#list.sorted_merge_sort</a> is an example of \"1.\"</p>",
        "id": 248465690,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1628163767
    },
    {
        "content": "<p>Note that since <a href=\"https://leanprover-community.github.io/mathlib_docs/find/funext\">docs#funext</a> declares functions equal if their values are equal, you cannot make proofs about \"complexity\" of functions by any metric. Instead, you can define a copy of your <code>sort</code> function to return something like <code>(sorted_list, num_accesses)</code> (or write a metaprogram to generate a function that does that!) and translate statements about complexity to statements about <code>num_accesses</code>.</p>",
        "id": 248467701,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1628165113
    },
    {
        "content": "<p>Thank you! That sounds like a great idea, I will think about it. I think these tools will be useful and necessary when lean expands into the theoretical computer science world!</p>",
        "id": 248473083,
        "sender_full_name": "david",
        "timestamp": 1628168361
    },
    {
        "content": "<p>Did you start working on this? Is anyone working on formalizing complexity theory?</p>",
        "id": 307701628,
        "sender_full_name": "Aporías",
        "timestamp": 1667473298
    },
    {
        "content": "<p>There are some related open PRs, you can search on GitHub. See also <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory\">this topic</a>.</p>",
        "id": 307962586,
        "sender_full_name": "Yuyang Zhao",
        "timestamp": 1667570086
    },
    {
        "content": "<p>I put together some code following <span class=\"user-mention\" data-user-id=\"310045\">@Eric Wieser</span>  suggestion for multiplication (I ended up with a monad instead),<br>\nI was originally going to code through Karatsuba but stopped at traditional multiply instead (too much algebra - I probably need to get more experience with tactics - simp/ring/linarith don't seem to be cutting it),<br>\n<a href=\"https://github.com/calcu16/lean_complexity/tree/main/src\">https://github.com/calcu16/lean_complexity/tree/main/src</a>.</p>\n<p>I think my main concern is that ideally we'd just have to read the proposition of the correct value and cost of any function (in my case traditional_multiply_value and traditional_multiply_cost) to know that the function does what its supposed to do and that it has a certain complexity. But in this case you have to read the entire definition of traditional_multiply to make sure no funny business happens (this is in contrast to using turing machines or partrec definitions where I think we can be confident of the algorithm if we assume the machine definition is sound).</p>",
        "id": 312537998,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1669600410
    },
    {
        "content": "<p>Is there an encoding of asymptotic analysis in lean?</p>",
        "id": 318135596,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672163272
    },
    {
        "content": "<p>I suspect that this would be </p>\n<ol>\n<li>A prerequisite </li>\n<li>Achievable</li>\n</ol>",
        "id": 318135880,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672163397
    },
    {
        "content": "<p><a href=\"https://leanprover-community.github.io/mathlib_docs/find/asymptotics.is_O\">docs#asymptotics.is_O</a></p>",
        "id": 318135961,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672163418
    },
    {
        "content": "<p>I am not sure about the \"prerequisite\" part. I think we should first implement a model that counts steps exactly, to have a solid definition.</p>",
        "id": 318136330,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672163562
    },
    {
        "content": "<p>I am writing as I think here, so please bear with me: <br>\nWith my CS theory hat on, usually we don't seem to \"count\" per se. We have some algorithms with their respective complexities and then we compose them to get other algorithms of other complexities. But both this, and the counting you suggest have similar requirements. You have some operations, with certain input parameters and computational complexities in terms of those input parameters. There are certain ways of combining them (conditionals, loops, recursion etc). The complexity of the resultant operation is determined by the type of composition.</p>",
        "id": 318149476,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672169792
    },
    {
        "content": "<p>Importantly the cost of various operations is specific to the model.</p>",
        "id": 318149592,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672169848
    },
    {
        "content": "<p>There are a gazillion models out there</p>",
        "id": 318149737,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672169904
    },
    {
        "content": "<p>And depending on what sort of complexity theory you do, the model matters to differing degrees.</p>",
        "id": 318149862,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672169960
    },
    {
        "content": "<p>For example, fine-grained complexity people won't be happy with polynomial factors that, say, the NP-Completeness might not care about. Distributed algorithms people might care deeply about polylog factors.</p>",
        "id": 318150054,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672170058
    },
    {
        "content": "<p>I wonder if building a DSL for pseudocode, tagged with input parameter sizes and complexity expressions is the way to go? It might be the most intuitive for the algorithms and complexity people, plus we might avoid having to actually execute the computations</p>",
        "id": 318152054,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672170992
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318149476\">said</a>:</p>\n<blockquote>\n<p>(...) There are certain ways of combining them (conditionals, loops, recursion etc). The complexity of the resultant operation is determined by the type of composition.</p>\n</blockquote>\n<p>It will be crucial to design a model of computation that allows \"combining them\". Usually, in computer science, we employ a lot of arguments like \"and then we run algorithm A on the segment between # and $ and store the result on the ends of the tape\" which are amazing for handwaving but extremely tedious to formalize.</p>",
        "id": 318152423,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672171183
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318149592\">said</a>:</p>\n<blockquote>\n<p>Importantly the cost of various operations is specific to the model.</p>\n</blockquote>\n<p>Yes. And it is important to have a concrete model.</p>",
        "id": 318152643,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672171275
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318149737\">said</a>:</p>\n<blockquote>\n<p>There are a gazillion models out there</p>\n</blockquote>\n<p>It will be really important to choose the right one. If we choose something that's cumbersome to work with, we will end up writing hundreds of thousands of lines extra.</p>",
        "id": 318152925,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672171396
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318150054\">said</a>:</p>\n<blockquote>\n<p>For example, fine-grained complexity people won't be happy with polynomial factors that, say, the NP-Completeness might not care about. Distributed algorithms people might care deeply about polylog factors.</p>\n</blockquote>\n<p>Yeah. That is another reason why we should start with exact counting of computation steps. Later we can decide what factors we ignore.</p>",
        "id": 318153528,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672171686
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318152054\">said</a>:</p>\n<blockquote>\n<p>I wonder if building a DSL for pseudocode, tagged with input parameter sizes and complexity expressions is the way to go? It might be the most intuitive for the algorithms and complexity people, plus we might avoid having to actually execute the computations</p>\n</blockquote>\n<p>Keep in mind that the most difficult thing is to <em>reason</em> about computation, not to implement stuff and execute programs. I cannot imagine pseudocode would help us.</p>",
        "id": 318154050,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672171988
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318136330\">said</a>:</p>\n<blockquote>\n<p>I am not sure about the \"prerequisite\" part. I think we should first implement a model that counts steps exactly, to have a solid definition.</p>\n</blockquote>\n<p>As a computer-science student, I truly appreciate complexity classes that are agnostic to the programming language / model of computation. However, we have to be really precise here. We should choose one model, define the time (and space) complexity exactly, and build a good API over it.</p>",
        "id": 318155567,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672172857
    },
    {
        "content": "<p>Later we can introduce other models and show in which sense they are equivalent.</p>",
        "id": 318155668,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672172915
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318152423\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318149476\">said</a>:</p>\n<blockquote>\n<p>(...) There are certain ways of combining them (conditionals, loops, recursion etc). The complexity of the resultant operation is determined by the type of composition.</p>\n</blockquote>\n<p>It will be crucial to design a model of computation that allows \"combining them\". Usually, in computer science, we employ a lot of arguments like \"and then we run algorithm A on the segment between # and $ and store the result on the ends of the tape\" which are amazing for handwaving but extremely tedious to formalize.</p>\n</blockquote>\n<p>I think a very basic DSL can do much of this. The basic operations are : creating new identifiers, assigning to identifiers, arithematic and logic, conditionals, and loops. Then for correctness we use the hoare triplets approach which can be implemented as refinement types (which is possible in lean).</p>",
        "id": 318157958,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672174341
    },
    {
        "content": "<p>Then comes procedure calls . Then finally, (mu?)-recursion</p>",
        "id": 318158007,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672174381
    },
    {
        "content": "<p>This should give us the RAM model at the very least. Proving equivalences can be non-trivial to impossible (think LOCAL model, which allows undecidable local computations at each node, with say, RAM).</p>",
        "id": 318158317,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672174575
    },
    {
        "content": "<p>I think it is more important to get a model that works without the sugar... the DSL part is not the hard part but it is a large investment so either it needs to work on a wide spectrum of models or we need to make one model which has all the properties we want</p>",
        "id": 318158506,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672174680
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318158506\">said</a>:</p>\n<blockquote>\n<p>I think it is more important to get a model that works without the sugar... the DSL part is not the hard part but it is a large investment so either it needs to work on a wide spectrum of models or we need to make one model which has all the properties we want</p>\n</blockquote>\n<p>I think that is a non-starter. There are just too many models, completely incompatible in many cases. If there is one model that can be considered conceptually basic, it would be the RAM model.</p>",
        "id": 318158647,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672174757
    },
    {
        "content": "<p>hence why I don't think the DSL should be the focus</p>",
        "id": 318158709,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672174797
    },
    {
        "content": "<p>RAM model for the more algorithmy bits of complexity and Turing machine for the foundational results.</p>",
        "id": 318158776,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672174809
    },
    {
        "content": "<p>Note that the <code>turing_machine.lean</code> file alone defines 3 different computational models, and there are a couple more in other files</p>",
        "id": 318158867,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672174869
    },
    {
        "content": "<p>the ratio of programs to models is way too low for a DSL to be a worthwhile investment</p>",
        "id": 318158917,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672174903
    },
    {
        "content": "<p>the only model which has enough written in it to be worth having more sugar in mathlib is <code>primrec</code> and/or <code>partrec</code> (even here, there is a split...)</p>",
        "id": 318159014,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672174960
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318158709\">said</a>:</p>\n<blockquote>\n<p>hence why I don't think the DSL should be the focus</p>\n</blockquote>\n<p>Beyond foundational computability + complexity (and some hierarchy) results, trying to do  TCS in Turing machines is probably not meaningful.</p>",
        "id": 318159100,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175016
    },
    {
        "content": "<p>Have you actually looked at the file</p>",
        "id": 318159119,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175030
    },
    {
        "content": "<p>But it's true that TM-based models are not suitable for complexities tighter than P, because there is too much translation overhead</p>",
        "id": 318159298,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175133
    },
    {
        "content": "<p>Do you have a reference which defines a complexity model rigorously? They seem to be very hard to come by</p>",
        "id": 318159447,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175211
    },
    {
        "content": "<p>Well, there is the Barak-Arora textbook, freely available online.  I tried looking up complexity zoo. It seems to be down.</p>",
        "id": 318159630,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175328
    },
    {
        "content": "<p>BTW, if we are going to write a DSL for writing programs, the most obvious choice of language is Lean itself</p>",
        "id": 318159644,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175340
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318159630\">said</a>:</p>\n<blockquote>\n<p>Well, there is the Barak-Arora textbook, freely available online.  I tried looking up complexity zoo. It seems to be down.</p>\n</blockquote>\n<p><a href=\"https://users.cs.duke.edu/~reif/courses/complectures/books/AB/ABbook.pdf\">https://users.cs.duke.edu/~reif/courses/complectures/books/AB/ABbook.pdf</a> I guess?</p>",
        "id": 318159825,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175433
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318159644\">said</a>:</p>\n<blockquote>\n<p>BTW, if we are going to write a DSL for writing programs, the most obvious choice of language is Lean itself</p>\n</blockquote>\n<p>This occurred to me at first. I see two challenges:</p>\n<ol>\n<li>The typical theory paper is written in imperative style pseudo code. That's what most theorists understand.</li>\n<li>I am trying to avoid performing the actual computation, and just build an expression tree with the relevant proofs and complexities.</li>\n</ol>",
        "id": 318159835,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175443
    },
    {
        "content": "<p>You can write imperative style code in lean too you know</p>",
        "id": 318159910,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175493
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318159910\">said</a>:</p>\n<blockquote>\n<p>You can write imperative style code in lean too you know</p>\n</blockquote>\n<p>Yes, but then it is actually executed.</p>",
        "id": 318159938,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175516
    },
    {
        "content": "<p>??</p>",
        "id": 318159993,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175528
    },
    {
        "content": "<p>Most theory people don't care about the fine-grained aspects of getting an algorithm to run</p>",
        "id": 318160011,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175537
    },
    {
        "content": "<p>This is for modeling purposes, you don't have to call the function if you don't want to</p>",
        "id": 318160020,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175541
    },
    {
        "content": "<p>but the fact that the lean function functions as its own denotational semantics is immensely useful in this context</p>",
        "id": 318160055,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175561
    },
    {
        "content": "<p>What I mean is that, for example a theorist might be happy to leave several implementation details unspecified.</p>",
        "id": 318160140,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175636
    },
    {
        "content": "<p>I don't think that the fact that lean programs are also compiled is a downside here... it is at most a neutral thing</p>",
        "id": 318160195,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175650
    },
    {
        "content": "<p>you can put <code>noncomputable</code> if you really don't want the compiler to even contemplate compiling it, but... why?</p>",
        "id": 318160232,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175673
    },
    {
        "content": "<p>it helps to test the function in <code>#eval</code> in any case</p>",
        "id": 318160245,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175695
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318160140\">said</a>:</p>\n<blockquote>\n<p>What I mean is that, for example a theorist might be happy to leave several implementation details unspecified.</p>\n</blockquote>\n<p>Are you serious? You want to formally prove properties of programs that are so informal that you could not even execute them?</p>",
        "id": 318160263,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672175706
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318160232\">said</a>:</p>\n<blockquote>\n<p>you can put <code>noncomputable</code> if you really don't want the compiler to even contemplate compiling it, but... why?</p>\n</blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318160263\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318160140\">said</a>:</p>\n<blockquote>\n<p>What I mean is that, for example a theorist might be happy to leave several implementation details unspecified.</p>\n</blockquote>\n<p>Are you serious? You want to formally prove properties of programs that are so informal that you could not even execute them?</p>\n</blockquote>\n<p>Welcome to theoretical CS <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span> <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 318160302,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672175729
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318160140\">said</a>:</p>\n<blockquote>\n<p>What I mean is that, for example a theorist might be happy to leave several implementation details unspecified.</p>\n</blockquote>\n<p>Also, this is a formal theorem prover we're talking about here. Specifying the unspecified implementation details is exactly what we do</p>",
        "id": 318160311,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175737
    },
    {
        "content": "<p>I fully expect a large part of the work to be figuring out all the gaps left by the informal mathematician</p>",
        "id": 318160417,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672175785
    },
    {
        "content": "<p>Producing an implementable version of a theoretically proposed algorithm can be a challenge in and of itself: for example consider this:<br>\n<a href=\"https://link.springer.com/content/pdf/10.1007/BF01940648.pdf\">https://link.springer.com/content/pdf/10.1007/BF01940648.pdf</a>:<br>\n\"We conclude that there is no published correct presentation of the embedding phase of the<br>\nHopcroft and Tarjan algorithm. \". This refers to the paper here (<a href=\"https://dl.acm.org/doi/pdf/10.1145/321850.321852\">https://dl.acm.org/doi/pdf/10.1145/321850.321852</a>) and Hopcroft's book</p>",
        "id": 318160839,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672176081
    },
    {
        "content": "<p>It seems to me that you <span class=\"user-mention\" data-user-id=\"466334\">@Shreyas Srinivas</span> are aiming for some really inconvenient trade-offs.</p>",
        "id": 318160989,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672176174
    },
    {
        "content": "<p>If you can't even implement the algorithm, forget about verifying it formally.</p>",
        "id": 318161057,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672176228
    },
    {
        "content": "<p>I am trying to point out that algorithm engineering is different from algorithm design</p>",
        "id": 318161471,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672176494
    },
    {
        "content": "<p>You can ship your informal pseudocode with an informal proof of its time complexity. You correctly observe that theorists do it all the time.</p>",
        "id": 318161563,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672176562
    },
    {
        "content": "<p>However, for a formal proof of time complexity, having a formal implementation of the program is a basic need.</p>",
        "id": 318161604,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672176592
    },
    {
        "content": "<p>Which is why counting minutiae might lead us astray. It is still possible to prove things at a sufficiently high level of abstraction. For example, an algorithm might say : explore the graph this way, construct such and such partition, and do something on the subgraphs. We might know the properties of the input/output of each of these steps.</p>",
        "id": 318161685,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672176618
    },
    {
        "content": "<p>Then we might be able to prove meaningful things about the properties of the algorithm as a whole, without going too deep into specifics. This is also why a DSL might be helpful</p>",
        "id": 318161731,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672176656
    },
    {
        "content": "<p>It might allow us to introduce adhoc routines of specific complexities and properties.</p>",
        "id": 318161760,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672176675
    },
    {
        "content": "<p>I agree with Martin that it is not reasonable to consider high level reasoning about complexity without the ability to give overly precise step counting results first</p>",
        "id": 318162230,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672176947
    },
    {
        "content": "<p>of course we'd like to be able to blur the results to something more asymptotic, but this is a research question all on its own because the standard techniques of <code>is_O</code> don't quite apply when it's not clear what is being held fixed and what is existentially quantified</p>",
        "id": 318162416,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177059
    },
    {
        "content": "<p>I agree with that in the sense that, fundamental complexity results do require a lot of intricate counting with turing machines. For example, if we prove the first four chapters of the Barak-Arora book, this would already be a significant step forward.</p>",
        "id": 318162512,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672177096
    },
    {
        "content": "<p>I'm saying that even for the high level results where you just want a basic estimate, you can't even do that unless you have a precise definition of what you are estimating</p>",
        "id": 318162598,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177146
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318159825\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://users.cs.duke.edu/~reif/courses/complectures/books/AB/ABbook.pdf\">https://users.cs.duke.edu/~reif/courses/complectures/books/AB/ABbook.pdf</a> I guess?</p>\n</blockquote>\n<p>It's just a book draft. The full book is longer and has many serious mistakes fixed. If anyone wants the full book in PDF, or any other paper or book regarding complexity theory, I'm happy to share.</p>",
        "id": 318162738,
        "sender_full_name": "Patrick Johnson",
        "timestamp": 1672177207
    },
    {
        "content": "<p>Hmm.. okay. that might be true, but then we would be in the world of an exploding number of models.</p>",
        "id": 318162740,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672177208
    },
    {
        "content": "<p>and moreover, an issue that comes up fairly frequently IME is that if you try to bundle results in an existential too early, you have to go back and fix the proofs later when it turns out that you need to know that this constant C is not just fixed over <code>x</code> but also over <code>n</code>, or over some other parameter <code>k</code> you didn't think was going to vary</p>",
        "id": 318162876,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177303
    },
    {
        "content": "<p>the only way that I know to avoid that issue is to just say it's less than 2|x|^2 + 100 and be done with it</p>",
        "id": 318163018,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177353
    },
    {
        "content": "<p>(that doesn't have to be a precise bound either, but it's formally easy to see that <code>2</code> doesn't depend on <code>n</code> or <code>k</code> or anything else)</p>",
        "id": 318163095,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177401
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318162598\">said</a>:</p>\n<blockquote>\n<p>I'm saying that even for the high level results where you just want a basic estimate, you can't even do that unless you have a precise definition of what you are estimating</p>\n</blockquote>\n<p>Question : How would this counting work: I imagine that I have procedure <code>A</code> which has time complexity <code>f(n)</code> and procedure <code>B</code> with complexity <code>g(n)</code>.  Then a <code>sequential_compose A B</code> might have complexity <code>f(|x|) + g(|A(x)|)</code>.  To me it seems like we don't need to run the algorithm and explicitly count the number of steps to achieve this. There are certain other kinds of compositions, but depending on the model, maybe not too many.</p>",
        "id": 318163608,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672177694
    },
    {
        "content": "<p>I'm not sure what you are talking about by \"explicitly running the algorithm\"</p>",
        "id": 318163662,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177730
    },
    {
        "content": "<p>that's just a basic theorem about the cost model</p>",
        "id": 318163692,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177751
    },
    {
        "content": "<p>Exactly. so, I just need to carry this proof forward when I compose procedures together. Not perform them, carrying out each step and carrying their data around. So the \"procedures\" need not be executable</p>",
        "id": 318163817,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672177824
    },
    {
        "content": "<p>In fact they could just be stubs.</p>",
        "id": 318163829,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672177848
    },
    {
        "content": "<p>One thing that is important though is that <code>A</code> has time complexity bounded by an actual function <code>f(n)</code> (like <code>14 * (n+1)^2</code>), not an existentially quantified function, in order to run that argument</p>",
        "id": 318163831,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177850
    },
    {
        "content": "<p>otherwise you have to add some existential unpacking steps in there and it can get tricky to automate that in such a way that the relevant parameters are generalized</p>",
        "id": 318163892,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177891
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318163817\">said</a>:</p>\n<blockquote>\n<p>Exactly. so, I just need to carry this proof forward when I compose procedures together. Not perform them, carrying out each step and carrying their data around. So the \"procedures\" need not be executable</p>\n</blockquote>\n<p>The procedures do not need to be executable, but it's not like that's a negative thing that drags on the formalization</p>",
        "id": 318164035,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177972
    },
    {
        "content": "<p>But they do need to be precisely defined</p>",
        "id": 318164061,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672177985
    },
    {
        "content": "<p>and they need to have a fixed denotation</p>",
        "id": 318164077,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178001
    },
    {
        "content": "<p>and it turns out that lean functions are one way to define the denotation of an algorithm</p>",
        "id": 318164095,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178022
    },
    {
        "content": "<p>so for example you might prove that <code>eval (sequential_compose A B) = eval B ∘ eval A</code> using lean's <code>∘</code> for sequential composition</p>",
        "id": 318164196,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178097
    },
    {
        "content": "<p>okay, maybe I misunderstood what you mean by counting each step: when I think of counting individual steps, I picture, say a merge sort implementation that, for each input, tells me how many swaps it did. Or some recursive procedure with the time complexity T(n), that explicitly computes the recursion until reaching the stopping condition.</p>",
        "id": 318164332,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672178179
    },
    {
        "content": "<p>the procedure itself isn't doing the counting, we are doing the counting by analyzing what the procedure does</p>",
        "id": 318164397,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178224
    },
    {
        "content": "<p>although I have seen some formalizations that bake the counting into the procedure itself by embedding it in a \"step-counting monad\"</p>",
        "id": 318164427,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178247
    },
    {
        "content": "<p>this feels like a bit of a cheat to me though, since you don't have to call the <code>tick</code> function if you don't want to</p>",
        "id": 318164432,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178256
    },
    {
        "content": "<p>I think what we want is some type <code>prog</code> of programs in a given computational model, and the cost model and execution semantics defined by a predicate <code>evals_in (p : prog) x n a</code> which says that <code>p</code> evaluates <code>x</code> to <code>a</code> in <code>n</code> steps. Then there are a bunch of things you can build on top to make that easier to use and compose</p>",
        "id": 318164818,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178481
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318164432\">said</a>:</p>\n<blockquote>\n<p>this feels like a bit of a cheat to me though, since you don't have to call the <code>tick</code> function if you don't want to</p>\n</blockquote>\n<p>I fully agree with this opinion. If we rely on the programmer to call <code>tick</code> with every operation, we can have a proof that \"algorithm A has a time complexity f(n) assuming that the implementation is done well\", but we can never prove that \"problem X has a time complexity f(n)\".</p>",
        "id": 318164988,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672178592
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318164818\">said</a>:</p>\n<blockquote>\n<p>I think what we want is some type <code>prog</code> of programs in a given computational model, and the cost model and execution semantics defined by a predicate <code>evals_in (p : prog) x n a</code> which says that <code>p</code> evaluates <code>x</code> to <code>a</code> in <code>n</code> steps. Then there are a bunch of things you can build on top to make that easier to use and compose</p>\n</blockquote>\n<p>With the important addition that there is usually more than one parameter.</p>",
        "id": 318165026,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672178616
    },
    {
        "content": "<p>BTW I looked a bit at the A-B book but I'm not sure if there is something you think is particularly good in there. I'm seeing a lot about P and NP stuff and turing machines suffice for that (and given the tone of the first part of the book I don't expect it to be very helpful about choosing a good computational model)</p>",
        "id": 318165108,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178656
    },
    {
        "content": "<p>there are no parameters in that example, I'm not sure what you mean?</p>",
        "id": 318165124,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178672
    },
    {
        "content": "<p>A statement on graphs : <code>depth-first-search</code> runs on a graph <code>G</code> in  <code>O(|G.V| + |G.E|)</code> steps</p>",
        "id": 318165204,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672178745
    },
    {
        "content": "<p>usually <code>|G.V|</code> and <code>|G.E|</code> are treated as independent parameters (denoted <code>n</code> and <code>m</code> for example)</p>",
        "id": 318165278,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672178786
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318165108\">said</a>:</p>\n<blockquote>\n<p>BTW I looked a bit at the A-B book but I'm not sure if there is something you think is particularly good in there. I'm seeing a lot about P and NP stuff and turing machines suffice for that (and given the tone of the first part of the book I don't expect it to be very helpful about choosing a good computational model)</p>\n</blockquote>\n<p>Its first few chapters deal with the TM model, but it also introduces circuits towards the end of part 1. Later it introduces interactive proofs, decision trees, communication complexity, algebraic complexity etc.</p>",
        "id": 318165481,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672178921
    },
    {
        "content": "<p><code>evals_in</code> is a very low level primitive, you wouldn't use it for that statement. But after desugaring, a theorem like that would take the form:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">\\</span><span class=\"n\">exists</span> <span class=\"n\">C</span><span class=\"o\">,</span>\n<span class=\"bp\">\\</span><span class=\"k\">forall</span> <span class=\"n\">x</span><span class=\"o\">,</span>\n<span class=\"n\">x</span> <span class=\"bp\">\\</span><span class=\"k\">in</span> <span class=\"n\">graph</span> <span class=\"n\">m</span> <span class=\"n\">n</span> <span class=\"bp\">-&gt;</span>\n<span class=\"n\">evals_in</span> <span class=\"n\">depth</span><span class=\"bp\">-</span><span class=\"n\">first</span><span class=\"bp\">-</span><span class=\"n\">search</span> <span class=\"n\">x</span> <span class=\"o\">(</span><span class=\"n\">c</span> <span class=\"bp\">*</span> <span class=\"o\">(</span><span class=\"n\">m</span> <span class=\"bp\">+</span> <span class=\"n\">n</span> <span class=\"bp\">+</span> <span class=\"mi\">1</span><span class=\"o\">))</span> <span class=\"o\">(</span><span class=\"n\">DFS_result</span> <span class=\"n\">x</span><span class=\"o\">)</span>\n</code></pre></div>",
        "id": 318165490,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178931
    },
    {
        "content": "<p>yeah, I saw the circuits. Circuits are \"fun\" since they are non-inductive</p>",
        "id": 318165527,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672178956
    },
    {
        "content": "<p>None of them really sounds like a general purpose computational model though, such as you might use to execute pseudocode style programs you see in the textbooks</p>",
        "id": 318165590,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672179002
    },
    {
        "content": "<p>That would be the RAM model. You would find it an algorithms book (Mehlhorn and Sanders for example). It is referenced in the chapter on NP completeness when they talk about graph problems and reductions if I remember correctly. The Barak-Arora book is basically the standard grad school textbook for complexity theory. But it tries its best to minimise contact with the algorithms side. </p>\n<p>It is certainly not covering every model there is. There is the whole world of distributed models, Local computation, query complexity, dynamic complexity etc, that have not been covered.</p>",
        "id": 318165897,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672179185
    },
    {
        "content": "<p>For some reason complexity zoo seems to be unavailable, but it is/was basically a wiki for everything to do with complexity theory (I mostly recall seeing complexity classes)</p>",
        "id": 318165943,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672179232
    },
    {
        "content": "<p>There are variations like the\"real RAM\" model that you would only encounter in places like computational geometry.</p>\n<p>Basically, finding a single source of comprehensive truth is hard in this subject</p>",
        "id": 318166058,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672179293
    },
    {
        "content": "<p>But the Barak-Arora book has the benefit of containing enough foundational material that using it as a guide might be a good idea.</p>",
        "id": 318166248,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672179403
    },
    {
        "content": "<p>The main thing we want to ensure here is that the model we actually use is suitably close (in the sense of having translations with low enough loss to not lose the distinctions of interest) to the ones that the textbooks use</p>",
        "id": 318166515,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672179602
    },
    {
        "content": "<p>As I mentioned, as long as a factor of O(n) loss is okay, TMs are fine. But that's not good enough for your DFS example</p>",
        "id": 318166608,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672179654
    },
    {
        "content": "<p>in fact, there is very commonly a O(log n) loss in a lot of RAM model style things in order to encode the pointer values</p>",
        "id": 318166655,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672179686
    },
    {
        "content": "<p>I think that's what word RAM is about</p>",
        "id": 318166668,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672179701
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318166515\">said</a>:</p>\n<blockquote>\n<p>The main thing we want to ensure here is that the model we actually use is suitably close (in the sense of having translations with low enough loss to not lose the distinctions of interest) to the ones that the textbooks use</p>\n</blockquote>\n<p>Well then Barak-Arora is the way to go. It is one of the standard textbooks. Michael Sipser's textbook is also used a lot for intro courses.</p>",
        "id": 318166754,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672179738
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318166655\">said</a>:</p>\n<blockquote>\n<p>in fact, there is very commonly a O(log n) loss in a lot of RAM model style things in order to encode the pointer values</p>\n</blockquote>\n<p>Some <code>log</code> might come up like that in pseudopolynomial algorithms ( for example see knapsack algorithms or the ellipsoid algorithm).</p>",
        "id": 318167120,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672179987
    },
    {
        "content": "<p>sure, but it's different if it's put there on purpose</p>",
        "id": 318167134,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180013
    },
    {
        "content": "<p>I mean, check this (recent, award winning) paper : <a href=\"https://arxiv.org/abs/2203.03456\">https://arxiv.org/abs/2203.03456</a></p>",
        "id": 318167183,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180044
    },
    {
        "content": "<p>I don't think the <code>log^8 n</code> has anything to do with pointer packing.</p>",
        "id": 318167209,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180072
    },
    {
        "content": "<p>Right, but if we tried to formalize that and got log^10 because of pointer packing issues, that wouldn't look very good, would it?</p>",
        "id": 318167289,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180103
    },
    {
        "content": "<p>I don't think it matters that much.</p>",
        "id": 318167321,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180140
    },
    {
        "content": "<p>I'm not sure we can make the judgment that those factors are never relevant in any proof that will be using the framework</p>",
        "id": 318167352,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180173
    },
    {
        "content": "<p>It seems to me that we have some misunderstanding going on. Do you <span class=\"user-mention\" data-user-id=\"466334\">@Shreyas Srinivas</span>  actually want to formally verify complexity theory? Or do you want to build a library of complexity-theoretic results àla Formal Abstracts project?</p>",
        "id": 318167360,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672180186
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167352\">said</a>:</p>\n<blockquote>\n<p>I'm not sure we can make the judgment that those factors are never relevant in any proof that will be using the framework</p>\n</blockquote>\n<p>In this case the authors are not bothered about those factors so they would probably the <code>\\tilde{O}</code> notation (which ignores log factors in addition to constants)</p>",
        "id": 318167491,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180273
    },
    {
        "content": "<p>In the latter case, you maybe don't care about correctness, but rather about having an expressive formal language to allow a computer to work with this database?</p>",
        "id": 318167501,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672180289
    },
    {
        "content": "<p>We proved that you can sort n numbers in <code>O(n log^2 n)</code> steps...</p>",
        "id": 318167580,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180328
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167360\">said</a>:</p>\n<blockquote>\n<p>It seems to me that we have some misunderstanding going on. Do you <span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span>  actually want to formally verify complexity theory? Or do you want to build a library of complexity-theoretic results àla Formal Abstracts project?</p>\n</blockquote>\n<p>I do care about correctness, but I am happy with correctness at the level of abstraction that the authors care about. Correctness is often tricky to prove as well.</p>",
        "id": 318167624,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180360
    },
    {
        "content": "<p>In the word RAM model where your numbers are guaranteed to fit into one word?<br>\n<span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167580\">said</a>:</p>\n<blockquote>\n<p>We proved that you can sort n numbers in <code>O(n log^2 n)</code> steps...</p>\n</blockquote>",
        "id": 318167760,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180454
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167624\">said</a>:</p>\n<blockquote>\n<p>I do care about correctness, but I am happy with correctness at the level of abstraction that the authors care about.</p>\n</blockquote>\n<p>Do you want your computer to enforce the correctness?</p>",
        "id": 318167787,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672180482
    },
    {
        "content": "<p>the sorting example is especially funny since those numbers are surely at least <code>O(log n)</code> long for the problem to be interesting, and hence at least <code>O(log n)</code> to compare, so most likely <code>O(n log^2 n)</code> is the correct bound</p>",
        "id": 318167793,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180490
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167760\">said</a>:</p>\n<blockquote>\n<p>In the word RAM model where your numbers are guaranteed to fit into one word?<br>\n<span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167580\">said</a>:</p>\n<blockquote>\n<p>We proved that you can sort n numbers in <code>O(n log^2 n)</code> steps...<br>\n</p>\n</blockquote>\n</blockquote>\n<p>This is my point. We're going to invest a bunch of time and effort into one model and then it turns out that for <em>this</em> result we've got to throw it away and get a better one</p>",
        "id": 318167876,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180551
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167787\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167624\">said</a>:</p>\n<blockquote>\n<p>I do care about correctness, but I am happy with correctness at the level of abstraction that the authors care about.</p>\n</blockquote>\n<p>Do you want your computer to enforce the correctness?</p>\n</blockquote>\n<p>I want to know if the papers I am checking have actually done their due diligence (which is a more non-trivial issue in theoretical CS than math, probably because of our conference culture).</p>",
        "id": 318167885,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180555
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167793\">said</a>:</p>\n<blockquote>\n<p>the sorting example is especially funny since those numbers are surely at least <code>O(log n)</code> long for the problem to be interesting, and hence at least <code>O(log n)</code> to compare, so most likely <code>O(n log^2 n)</code> is the correct bound</p>\n</blockquote>\n<p>If we find a way to avoid this issue, then we will be due some awards. Algorithms can quite often exploit very specific model properties to do things. So a result that you develop for one model might not work for another model by translation.</p>",
        "id": 318168070,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180656
    },
    {
        "content": "<p>well then <em>it sure would be nice</em> if papers were more precise about the definitions of their models</p>",
        "id": 318168166,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180686
    },
    {
        "content": "<p>Oh they are... it is just that everybody likes to write their own model from scratch</p>",
        "id": 318168220,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180719
    },
    {
        "content": "<p>if we can find some small set of equivalence classes of models into which we can bucket all the results that would be ideal...</p>",
        "id": 318168262,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180748
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167885\">said</a>:</p>\n<blockquote>\n<p>I want to know if the papers I am checking have actually done their due diligence (which is a more non-trivial issue in theoretical CS than math, probably because of our conference culture).</p>\n</blockquote>\n<p>I don't understand your sentence (which might be my mistake — my English isn't that good). To avoid further confusion, please, clarify:</p>\n<p>Do you want the correctness of the results to be enforced by computer?</p>",
        "id": 318168401,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672180834
    },
    {
        "content": "<p>Which brings us back to the word RAM model. It is ideal for several reasons:</p>\n<ol>\n<li>Most CS students instinctively know it. Most code for algorithms (not all) is written on something close to word RAM.</li>\n<li>Some of the work has already been done for us in the algorithms engineering world: <a href=\"https://www3.cs.stonybrook.edu/~algorith/implement/LEDA/implement.shtml\">https://www3.cs.stonybrook.edu/~algorith/implement/LEDA/implement.shtml</a><br>\nThey have nailed down the implementation of many algorithms in highly precise details.</li>\n</ol>",
        "id": 318168470,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672180884
    },
    {
        "content": "<p>the word RAM model is a bit weird in that it explicitly depends on <code>w</code> which is related to <code>n</code>, though... it's not entirely clear to me how to avoid the issues with nonuniform algorithms</p>",
        "id": 318168608,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672180970
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168401\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167885\">said</a>:</p>\n<blockquote>\n<p>I want to know if the papers I am checking have actually done their due diligence (which is a more non-trivial issue in theoretical CS than math, probably because of our conference culture).</p>\n</blockquote>\n<p>I don't understand your sentence (which might be my mistake — my English isn't that good). To avoid further confusion, please, clarify:</p>\n<p>Do you want the correctness of the results to be enforced by computer?</p>\n</blockquote>\n<p>Yes their proofs must be correct and this correctness must be machine checkable. I am willing to tolerate reasonable assumptions about existing algorithms. For example I am not really going to insist that the authors write a proof of correctness for Depth First Search. If they make known assumptions about it or use a library invocation then I am happy with that.</p>",
        "id": 318168900,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672181155
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168608\">said</a>:</p>\n<blockquote>\n<p>the word RAM model is a bit weird in that it explicitly depends on <code>w</code> which is related to <code>n</code>, though... it's not entirely clear to me how to avoid the issues with nonuniform algorithms</p>\n</blockquote>\n<p>nonuniform algorithms?</p>",
        "id": 318169052,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672181233
    },
    {
        "content": "<p>like non-uniform circuits? (one algorithm per input size)</p>",
        "id": 318169078,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672181256
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168900\">said</a>:</p>\n<blockquote>\n<p>Yes their proofs must be correct and this correctness must be machine checkable. I am willing to tolerate reasonable assumptions about existing algorithms. For example I am not really going to insist that the authors write a proof of correctness for Depth First Search. If they make known assumptions about it or use a library invocation then I am happy with that.</p>\n</blockquote>\n<p>I am completely happy with \"library invocation\" but not so much with \"known assumptions\". We are at a point when a library of complexity-theoretic results in Lean does not exist. We cannot invoke it.</p>\n<p>We need to prove first things directly from the definition. And here comes the question: What definition do we exactly want to work with?</p>",
        "id": 318169283,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672181397
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318169283\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168900\">said</a>:</p>\n<blockquote>\n<p>Yes their proofs must be correct and this correctness must be machine checkable. I am willing to tolerate reasonable assumptions about existing algorithms. For example I am not really going to insist that the authors write a proof of correctness for Depth First Search. If they make known assumptions about it or use a library invocation then I am happy with that.</p>\n</blockquote>\n<p>I am completely happy with \"library invocation\" but not so much with \"known assumptions\". We are at a point when a library of complexity-theoretic results in Lean does not exist. We cannot invoke it.</p>\n<p>We need to prove first things directly from the definition. And here comes the question: What definition do we exactly want to work with?</p>\n</blockquote>\n<p>There are formidable problems with asking for this. Take Depth First Search(hereon : DFS). There are at least 4-5 graph representations I can think of, off the top of my head.  Then there is the question of whether you explicitly create a stack or use recursions. Already that gives you 10 potentially different implementations (and possibly many more), with different representation specific complexities. Now let's say you are writing a paper where all you care about is that  DFS runs in O(|V| + |E|) time. Should you spend time on sorting through these implementations or building your own?</p>",
        "id": 318169573,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672181586
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318169573\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318169283\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168900\">said</a>:</p>\n<blockquote>\n<p>Yes their proofs must be correct and this correctness must be machine checkable. I am willing to tolerate reasonable assumptions about existing algorithms. For example I am not really going to insist that the authors write a proof of correctness for Depth First Search. If they make known assumptions about it or use a library invocation then I am happy with that.</p>\n</blockquote>\n<p>I am completely happy with \"library invocation\" but not so much with \"known assumptions\". We are at a point when a library of complexity-theoretic results in Lean does not exist. We cannot invoke it.</p>\n<p>We need to prove first things directly from the definition. And here comes the question: What definition do we exactly want to work with?</p>\n</blockquote>\n<p>There are formidable problems with asking for this. Take Depth First Search(hereon : DFS). There are at least 4-5 graph representations I can think of, off the top of my head.  Then there is the question of whether you explicitly create a stack or use recursions. Already that gives you 10 potentially different implementations (and possibly many more), with different representation specific complexities. Now let's say you are writing a paper where all you care about is that  DFS runs in O(|V| + |E|) time. Should you spend time on sorting through these implementations or building your own?</p>\n</blockquote>\n<p>It gets even worse. Suppose I have additional information that the graph has bounded degree. Now, do I rebuild a new version of these implementations from scratch?</p>",
        "id": 318169837,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672181743
    },
    {
        "content": "<p>I don't think you can formally verify stuff without being precise about it in the first place.</p>",
        "id": 318170317,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672181992
    },
    {
        "content": "<p>But you could verify that starting from known and simple assumptions, you can build a more complex algorithm</p>",
        "id": 318170415,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182025
    },
    {
        "content": "<p>These issues are exactly why the graph theory in lean took a while to get off the ground. What fixed it? Picking a definition and formalizing stuff about it</p>",
        "id": 318170513,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182103
    },
    {
        "content": "<p>We <em>know</em> there are many definitions. So we need to pick a good one</p>",
        "id": 318170578,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182130
    },
    {
        "content": "<p>And if you use too many different definitions then the results can't be used together and we end up not working well as a library of reusable results and more like an archive</p>",
        "id": 318170677,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182213
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318169052\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168608\">said</a>:</p>\n<blockquote>\n<p>the word RAM model is a bit weird in that it explicitly depends on <code>w</code> which is related to <code>n</code>, though... it's not entirely clear to me how to avoid the issues with nonuniform algorithms</p>\n</blockquote>\n<p>nonuniform algorithms?</p>\n<p>like non-uniform circuits? (one algorithm per input size)</p>\n</blockquote>\n<p>Yeah, the name escapes me at the moment. If you aren't careful here you can decide the undecidable</p>",
        "id": 318170979,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182416
    },
    {
        "content": "<p>The issue we will encounter here is that we will need all these definitions and show the relevant equivalences. Unfortunately, algorithmic upper and lower bounds don't always neatly translate between models. While I can recommend TMs and wordRAM as starting points,  even within this there will be 10 different DFS versions. It is unavoidable. We can definitely start somewhere and see how it goes.</p>",
        "id": 318171047,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182459
    },
    {
        "content": "<p>CS theory is messy in that way.</p>",
        "id": 318171057,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182469
    },
    {
        "content": "<p>Do you have a recommendable source for word RAM done the way you think it should be done?</p>",
        "id": 318171190,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182543
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318170979\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318169052\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318168608\">said</a>:</p>\n<blockquote>\n<p>the word RAM model is a bit weird in that it explicitly depends on <code>w</code> which is related to <code>n</code>, though... it's not entirely clear to me how to avoid the issues with nonuniform algorithms</p>\n</blockquote>\n<p>nonuniform algorithms?</p>\n<p>like non-uniform circuits? (one algorithm per input size)</p>\n</blockquote>\n<p>Yeah, the name escapes me at the moment. If you aren't careful here you can decide the undecidable</p>\n</blockquote>\n<p>So the result is that non uniform circuit families can accept undecidable languages. Take a simple binary alphabet {0,1}. Then for any subset <code>S</code> of <code>\\N</code>, you can build a family of circuits which accepts  <code>{1^x | x \\in S}</code></p>",
        "id": 318171204,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182557
    },
    {
        "content": "<p>right, I don't want word RAM accepting undecidable languages</p>",
        "id": 318171274,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182593
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318171274\">said</a>:</p>\n<blockquote>\n<p>right, I don't want word RAM accepting undecidable languages</p>\n</blockquote>\n<p>Word RAM does not have this issue.</p>",
        "id": 318171350,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182615
    },
    {
        "content": "<p>because?</p>",
        "id": 318171364,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182625
    },
    {
        "content": "<p>is <code>w</code> not observable to the program or something?</p>",
        "id": 318171423,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182668
    },
    {
        "content": "<p>More specifically you can write non-terminating programs in word RAM (any computer program). But is is equivalent to TMs upto polynomial time complexity factors. So it can do exactly what TMs can do</p>",
        "id": 318171432,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182672
    },
    {
        "content": "<p>A word RAM doesn't have infinite memory, does it?</p>",
        "id": 318171573,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182749
    },
    {
        "content": "<p>I don't see how they could be turing complete</p>",
        "id": 318171604,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182778
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318171364\">said</a>:</p>\n<blockquote>\n<p>because?</p>\n</blockquote>\n<p>If you know <code>n</code> you know <code>w</code></p>",
        "id": 318171621,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672182788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318171057\">said</a>:</p>\n<blockquote>\n<p>CS theory is messy in that way.</p>\n</blockquote>\n<p>That's why we should choose a suitable part of it and make it tidy.</p>",
        "id": 318171735,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672182853
    },
    {
        "content": "<p>If you have a sequence of programs running on larger machines then isn't this the non-uniform circuit issue all over again?</p>",
        "id": 318171752,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672182867
    },
    {
        "content": "<p>I think I found the kind of definition you were looking for <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>  : <a href=\"http://people.seas.harvard.edu/~cs125/fall14/lec6.pdf\">http://people.seas.harvard.edu/~cs125/fall14/lec6.pdf</a></p>",
        "id": 318172219,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183195
    },
    {
        "content": "<p>whoa, that model has a \"malloc\" that is literally \"buy another byte from the hardware store\"</p>",
        "id": 318172358,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183290
    },
    {
        "content": "<p>and maybe increase the word size so you can address it too</p>",
        "id": 318172431,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183329
    },
    {
        "content": "<p>I think they work around the limitation you speak of w</p>",
        "id": 318172450,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183349
    },
    {
        "content": "<p>by taking w = \\Omega(\\log n)</p>",
        "id": 318172464,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183360
    },
    {
        "content": "<p>it still looks like an incredibly painful model to work with</p>",
        "id": 318172482,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183386
    },
    {
        "content": "<p>Almost all computational models are</p>",
        "id": 318172581,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183464
    },
    {
        "content": "<p>it's like a turing machine, only worse because there are so many operations and you can't assume <code>w = 1</code></p>",
        "id": 318172637,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183513
    },
    {
        "content": "<p>Most people think of it as : Most arithmetic and boolean operations in constant time (not real numbers, that is real RAM)).</p>",
        "id": 318172664,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183552
    },
    {
        "content": "<p>no I mean it's really obvious that it is C inspired</p>",
        "id": 318172723,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183577
    },
    {
        "content": "<p>and I'm sure it's fine to program in</p>",
        "id": 318172740,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183592
    },
    {
        "content": "<p>but not to reason about</p>",
        "id": 318172747,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183596
    },
    {
        "content": "<p>composition looks very bad in this model</p>",
        "id": 318172785,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672183630
    },
    {
        "content": "<p>This is another reason you want to go through the DSL route. You can parameterize the choice of unit operations.</p>",
        "id": 318172814,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183651
    },
    {
        "content": "<p>And then build up from there.</p>",
        "id": 318172819,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183659
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318172637\">said</a>:</p>\n<blockquote>\n<p>it's like a turing machine, only worse because there are so many operations and you can't assume <code>w = 1</code></p>\n</blockquote>\n<p>TMs are different from the w=1 case. There is no addressing requirement. OTOH, the state register can store some bits of info.</p>",
        "id": 318172959,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672183772
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167885\">said</a>:</p>\n<blockquote>\n<p>I want to know if the papers I am checking have actually done their due diligence (which is a more non-trivial issue in theoretical CS than math, probably because of our conference culture).</p>\n</blockquote>\n<p>Let me just say that formalising a result in Lean is orders of magnitude harder than checking it informally, so I wouldn't take that as my main motivation if I were you.</p>",
        "id": 318173117,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1672183863
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318172581\">said</a>:</p>\n<blockquote>\n<p>Almost all computational models are</p>\n</blockquote>\n<p>People say \"choose your poison\". I add \"but don't choose too many poisons at the same time — it would become too hard to build immunity to them\".</p>",
        "id": 318173153,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672183892
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318172814\">said</a>:</p>\n<blockquote>\n<p>This is another reason you want to go through the DSL route. You can parameterize the choice of unit operations.</p>\n</blockquote>\n<p>I agree with this, although it doesn't have much to do with DSLs. A simple family of computational models which comes acceptably close to popular ones in the literature is also great</p>",
        "id": 318173345,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184038
    },
    {
        "content": "<p>certainly it's obvious how to insert a parameter to the word RAM to make the set of register operations configurable</p>",
        "id": 318173457,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184097
    },
    {
        "content": "<p>although IIRC the multiplication operation in particular is rather disproportionately more powerful than the others here</p>",
        "id": 318173482,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184121
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"387244\">Yaël Dillies</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318173117\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318167885\">said</a>:</p>\n<blockquote>\n<p>I want to know if the papers I am checking have actually done their due diligence (which is a more non-trivial issue in theoretical CS than math, probably because of our conference culture).</p>\n</blockquote>\n<p>Let me just say that formalising a result in Lean is orders of magnitude harder than checking it informally, so I wouldn't take as my main motivation if I were you.</p>\n</blockquote>\n<p>Realistically I expect this to remain infeasible for  at least the next decade for most of CS theory. At least until there is a reasonably large library.</p>",
        "id": 318173600,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184203
    },
    {
        "content": "<p>well that's what this phase is about, assessing the goals and picking what to go for and what to avoid</p>",
        "id": 318173637,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184250
    },
    {
        "content": "<p>what do you think would be in the \"reasonably large library\" that would make the second phase feasible?</p>",
        "id": 318173726,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184296
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318173637\">said</a>:</p>\n<blockquote>\n<p>well that's what this phase is about, assessing the goals and picking what to go for and what to avoid</p>\n</blockquote>\n<p>word RAM is not Turing complete. RAM is. Apologies.</p>",
        "id": 318173759,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184338
    },
    {
        "content": "<p>we want to go for the things that empower later work, not just endpoint theorems</p>",
        "id": 318173787,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184362
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318173637\">said</a>:</p>\n<blockquote>\n<p>well that's what this phase is about, assessing the goals and picking what to go for and what to avoid</p>\n</blockquote>\n<p>I think phase would be:</p>\n<ol>\n<li>Formalise basic complexity theory.</li>\n<li>Formalise linear programming. weak duality strong duality etc. Linear programming is fundamental to many areas inside CS.</li>\n<li>Formalise structural results from graph theory that are useful in CS, relating various invariants (for example :graph minors a la Robertson and Seymour)</li>\n<li>Formalise combinatorial results like : <a href=\"https://en.wikipedia.org/wiki/Isolation_lemma\">https://en.wikipedia.org/wiki/Isolation_lemma</a></li>\n</ol>",
        "id": 318174062,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184556
    },
    {
        "content": "<p>We can also probably go for a RAM model for algorithms.</p>",
        "id": 318174097,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184595
    },
    {
        "content": "<p>To make sure you can't claim arbitrary operations have arbitrary complexities, you need to restrict the ways in which valid algorithms are constructed (i.e. unit operations, function calls, loops, conditionals)</p>",
        "id": 318174220,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184670
    },
    {
        "content": "<p>why not allow arbitrary operations to have arbitrary complexities?</p>",
        "id": 318174235,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184694
    },
    {
        "content": "<p>In a reduction I would imagine it is convenient to allow weird cost functions on either the source or target to make things match up</p>",
        "id": 318174353,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184764
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318174235\">said</a>:</p>\n<blockquote>\n<p>why not allow arbitrary operations to have arbitrary complexities?</p>\n</blockquote>\n<p>If I have a theorem  which gives me a complexity result in a model, I can trust the result only if it does not use silly assumptions inside like polynomial time minimum vertex cover algorithms.</p>",
        "id": 318174366,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184772
    },
    {
        "content": "<p>sure, but it's not like that's a problem of the framework</p>",
        "id": 318174398,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672184807
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318174235\">said</a>:</p>\n<blockquote>\n<p>why not allow arbitrary operations to have arbitrary complexities?</p>\n</blockquote>\n<p>No, in reductions, you actually want to avoid this. Because you need to prove that the reduction has a certain time complexity for it to be valid. The reduction needs to be a valid algorithm with a valid time complexity.</p>",
        "id": 318174439,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672184846
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318173787\">said</a>:</p>\n<blockquote>\n<p>we want to go for the things that empower later work, not just endpoint theorems</p>\n</blockquote>\n<p>This is the approach which worked so well with mathematics -- for a long time at the beginning the mantra was just \"build as much as possible of a standard undergraduate maths degree in one place\", and the showcase theorems grew from this solid core.</p>",
        "id": 318174451,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1672184856
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318174439\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318174235\">said</a>:</p>\n<blockquote>\n<p>why not allow arbitrary operations to have arbitrary complexities?</p>\n</blockquote>\n<p>No, in reductions, you actually want to avoid this. Because you need to prove that the reduction has a certain time complexity for it to be valid. The reduction needs to be a valid algorithm with a valid time complexity.</p>\n</blockquote>\n<p>I think we are miscommunicating. The reduction is just a theorem about the relation between different models, with some assumptions on the source and/or target and some relation between the cost models on each side. There isn't any point at which there is a cutoff and it stops being a \"valid reduction\" or a \"valid time complexity\", that's something you can impute on the theorem after the fact. For example, I should be able to prove that sorting is <code>O(n!)</code> if I want, it's not a useful theorem perhaps but I shouldn't define big-O so that the theorem is <em>false</em>.</p>",
        "id": 318174766,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672185108
    },
    {
        "content": "<p>Oh, when I think of reductions, I think of reductions between problems in a single model.</p>",
        "id": 318174841,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185161
    },
    {
        "content": "<p>For example 3 CNF to Vertex Cover.</p>",
        "id": 318174844,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185174
    },
    {
        "content": "<p>In this case I mean reduction from one computational model to another, a translation</p>",
        "id": 318174867,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672185195
    },
    {
        "content": "<p>So such a translation usually means that you are simulating one model in the other. Like writing a program to simulate a universal Turing machine in C or vice versa. Then, a complexity of this translation is defined in the model in which you are doing the simulation. It makes sense that the simulation algorithm cannot do arbitrary operations in a single step, since it is in fact an algorithm.</p>",
        "id": 318174979,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185301
    },
    {
        "content": "<p>It would be absolutely false to claim that I can simulate RAM model in Turing machines with O(1) overhead.</p>",
        "id": 318175014,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185341
    },
    {
        "content": "<p>Having a wide space of parameters is useful since you can have \"algebraic\" operations between them, like maps and comaps and stuff like that. It is a really powerful technique for making things reusable and for shorter proofs too.</p>",
        "id": 318175020,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672185343
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318175020\">said</a>:</p>\n<blockquote>\n<p>Having a wide space of parameters is useful since you can have \"algebraic\" operations between them, like maps and comaps and stuff like that. It is a really powerful technique for making things reusable and for shorter proofs too.</p>\n</blockquote>\n<p>I agree with this. I am just saying that the parametrisation must be in the choice of unit operations.</p>",
        "id": 318175092,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185392
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318175014\">said</a>:</p>\n<blockquote>\n<p>It would be absolutely false to claim that I can simulate RAM model in Turing machines with O(1) overhead.</p>\n</blockquote>\n<p>I think it is fine to have an algebra that includes 0 cost for everything, for the same reason that <code>⊥</code> should be a filter</p>",
        "id": 318175131,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672185449
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318175131\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318175014\">said</a>:</p>\n<blockquote>\n<p>It would be absolutely false to claim that I can simulate RAM model in Turing machines with O(1) overhead.</p>\n</blockquote>\n<p>I think it is fine to have an algebra that includes 0 cost for everything, for the same reason that <code>⊥</code> should be a filter</p>\n</blockquote>\n<p>Yes, that would be fine. It would just be a different model then. Not a translation between say RAM and TM</p>",
        "id": 318175215,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185517
    },
    {
        "content": "<p>This is incidentally what Erik Demaine does for  describing a notion of energy complexity. All \"reversible\" operations have 0 energy cost. <a href=\"https://erikdemaine.org/papers/Energy_ITCS2016/paper.pdf\">https://erikdemaine.org/papers/Energy_ITCS2016/paper.pdf</a></p>",
        "id": 318175269,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185574
    },
    {
        "content": "<p>Once you fix the model in which you are writing an algorithm (including reductions), the cost function is predetermined for that algorithm.</p>",
        "id": 318175435,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185687
    },
    {
        "content": "<p>well, you can also consider a translation from TM with the usual cost function to RAM with a weird cost function, or the other way around</p>",
        "id": 318175574,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672185779
    },
    {
        "content": "<p>that is convenient because it is easy to prove how the same model with a different cost function relates</p>",
        "id": 318175608,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672185821
    },
    {
        "content": "<p>Okay I think I see what you mean. You are saying that there could be multiple reductions each with its own cost</p>",
        "id": 318175614,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185833
    },
    {
        "content": "<p>and what I am saying is that given a particular reduction, the cost is fixed by the model</p>",
        "id": 318175676,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672185876
    },
    {
        "content": "<p>I think we are in agreement if I read this correctly.</p>",
        "id": 318175998,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186135
    },
    {
        "content": "<p>The key thing is you don't want to allow a proof that says that the cost of solving some 3SAT instance along the way is O(1). A DSL is a good way to restrict the kind of programs you can build and the way the cost adds up.</p>",
        "id": 318176116,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186207
    },
    {
        "content": "<p>Nothing in lean itself might prevent someone from defining a function like that and tagging it with O(1) complexity.</p>",
        "id": 318176157,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186240
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318176116\">said</a>:</p>\n<blockquote>\n<p>The key thing is you don't want to allow a proof that says that the cost of solving some 3SAT instance along the way is O(1). A DSL is a good way to restrict the kind of programs you can build and the way the cost adds up.</p>\n</blockquote>\n<p>This will either be a theorem or not depending on the precise statement and the values of the unstated parameters like the underlying computational model and cost model. If it's not a theorem, then there is no way you can sneak it through in the proof regardless of any DSL</p>",
        "id": 318176442,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186480
    },
    {
        "content": "<p>you can't just tag functions with a complexity, the cost of all the primitives is part of the definition of the cost model which is a parameter to everything</p>",
        "id": 318176511,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186541
    },
    {
        "content": "<p>or it's a fixed quantity which is defined somewhere, presumably with reasonable values so that useful results can be formalized</p>",
        "id": 318176573,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186587
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318176511\">said</a>:</p>\n<blockquote>\n<p>you can't just tag functions with a complexity, the cost of all the primitives is part of the definition of the cost model which is a parameter to everything</p>\n</blockquote>\n<p>\"the cost of all the primitives is part of the definition of the cost model which is a parameter to everything\" =&gt; I am in complete agreement here</p>",
        "id": 318176590,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186606
    },
    {
        "content": "<p>but it's totally possible, even useful, to have a model which assigns O(1) cost to SAT problems. That's basically an oracle model</p>",
        "id": 318176744,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186705
    },
    {
        "content": "<p>Yeah that's also useful</p>",
        "id": 318176757,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186717
    },
    {
        "content": "<p>In the LOCAL and CONGEST model, we have 0 cost for local computations (even NP complete problems).</p>",
        "id": 318176775,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186741
    },
    {
        "content": "<p>But how would we define a cost model for every possible expression in lean?</p>",
        "id": 318176790,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186758
    },
    {
        "content": "<p>we don't need to?</p>",
        "id": 318176794,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186765
    },
    {
        "content": "<p>the cost model applies to operations definable in <code>prog</code></p>",
        "id": 318176812,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186784
    },
    {
        "content": "<p>so <code>prog</code> is a DSL (I am thinking like a haskell person here).</p>",
        "id": 318176887,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186814
    },
    {
        "content": "<p>it's an inductive type of programs</p>",
        "id": 318176899,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186825
    },
    {
        "content": "<p>it's the stuff you reason about - in the word RAM example this was a sequence of primitive operations on registers and gotos</p>",
        "id": 318176945,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186879
    },
    {
        "content": "<p>Ah I think we are using different terms for the same thing</p>",
        "id": 318176970,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186909
    },
    {
        "content": "<p>a DSL would be a syntax for constructing elements of <code>prog</code> conveniently (and maybe proving properties about it along the way)</p>",
        "id": 318177034,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186942
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318177034\">said</a>:</p>\n<blockquote>\n<p>a DSL would be a syntax for constructing elements of <code>prog</code> conveniently (and maybe proving properties about it along the way)</p>\n</blockquote>\n<p>I was thinking of an inductive type plus use of <code>notation</code>, which might yield a nice syntax</p>",
        "id": 318177053,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672186973
    },
    {
        "content": "<p>and Lean-as-DSL is the idea that tactics can just look at declarations in the environment and translate them to an element of <code>prog</code> plus a proof of correctness</p>",
        "id": 318177069,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672186985
    },
    {
        "content": "<p>if you try to use something too weird in the input lean program then the tactic will fail</p>",
        "id": 318177154,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672187044
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318176970\">said</a>:</p>\n<blockquote>\n<p>Ah I think we are using different terms for the same thing</p>\n</blockquote>\n<p>Now I am certain this is the case.</p>",
        "id": 318177184,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672187079
    },
    {
        "content": "<p>lean 4 also lets you write some syntax which directly macro expands to elements of <code>prog</code>, that's more explicitly a DSL</p>",
        "id": 318177213,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672187099
    },
    {
        "content": "<p>but <code>prog</code> itself is more of a deep embedding than a DSL</p>",
        "id": 318177234,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672187112
    },
    {
        "content": "<p>it's not necessarily a language at all, it might be a circuit or some other thing</p>",
        "id": 318177275,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672187155
    },
    {
        "content": "<p>With a bit of convenient notation and some macros, we should be able to make it much easier to write programs in a form familiar to theorists.<br>\n<span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318174062\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318173637\">said</a>:</p>\n<blockquote>\n<p>well that's what this phase is about, assessing the goals and picking what to go for and what to avoid</p>\n</blockquote>\n<p>I think phase (EDIT: one) would be:</p>\n<ol>\n<li>Formalise basic complexity theory.</li>\n<li>Formalise linear programming. weak duality strong duality etc. Linear programming is fundamental to many areas inside CS.</li>\n<li>Formalise structural results from graph theory that are useful in CS, relating various invariants (for example :graph minors a la Robertson and Seymour)</li>\n<li>Formalise combinatorial results used in CS(example : <a href=\"https://en.wikipedia.org/wiki/Isolation_lemma\">https://en.wikipedia.org/wiki/Isolation_lemma</a>)</li>\n</ol>\n</blockquote>",
        "id": 318177639,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672187393
    },
    {
        "content": "<p>Any thoughts on the above? I think these four points, along with <code>prog</code> would go a long way to make theory results easy to prove on theorem provers.</p>",
        "id": 318177655,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672187402
    },
    {
        "content": "<p>The trouble is most of the results are scattered across the literature, so starting with undergrad textbooks (or early graduate books) on graphs, complexity etc sounds like a good strategy.</p>",
        "id": 318178062,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672187647
    },
    {
        "content": "<p>I think it's a bit too long term to be immediately actionable</p>",
        "id": 318178464,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672187926
    },
    {
        "content": "<p>we're still talking about the setup to point 1 after all</p>",
        "id": 318178515,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672187973
    },
    {
        "content": "<p>so something more precise than \"basic complexity theory\" would be useful</p>",
        "id": 318178556,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672188001
    },
    {
        "content": "<p>Chapters 1 and 2 (<a href=\"https://theory.cs.princeton.edu/complexity/book.pdf\">https://theory.cs.princeton.edu/complexity/book.pdf</a>) should be a good starting point : Chapter 1 is 60% about computability,</p>",
        "id": 318179419,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672188523
    },
    {
        "content": "<p>Is this paper worth reading?<br>\n<a href=\"https://www.cs.ru.nl/~freek/pubs/sketches2.pdf\">https://www.cs.ru.nl/~freek/pubs/sketches2.pdf</a></p>",
        "id": 318376897,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672303603
    },
    {
        "content": "<p>[Quoting…]</p>\n<p>I gave it a quick skim. The problem the paper claims to solve is how proofs in ITPs are presented. I don't know if they idea of a proof sketch is the way to do, theorem prover experts have to chime in.</p>\n<p>But insofar as the authors claim that ITP proofs are practically unreadable, I think this is definitely true for lean. TPIL acknowledges this when it introduces tactic style proofs. I think the point was, the more the automation, the less readable the proof gets. #print on a theorem almost always produces a large messy term, produced by the tactics. </p>\n<p>I also agree that this is an important issue<br>\nWe discussed this issue in PM a few days ago. I pointed out that at least in theoretical CS, it is not only important that we know a claim is true, it is important to know the structure of the proof, what ideas come into play, and how they work together. So for example a proof that just calls \"simp\" would be unsatisfactory. #print of that proof might be an unreadable blob of code. </p>\n<p>My suspicion is that there is some solution to this issue that involves producing a clean and readable text from a proof term.</p>",
        "id": 318433573,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672325643
    },
    {
        "content": "<p>In complexity theory, we greatly care about the innards of our proofs to varying degrees. So making the proof output readable is very important to us.</p>",
        "id": 318433576,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672325643
    },
    {
        "content": "<p>For example the crux of a new result could be a new reduction gadget. In that case, being able to see how the gadget is used in the reduction matters</p>",
        "id": 318434381,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672325937
    },
    {
        "content": "<p>I'll just chime in with my usual message that in maths the things we're proving in mathlib are either completely mathematically trivial or are very well-documented in standard textbooks so readability is not really something I'm too bothered about. I find lean proofs very easy to read if I step through them in VS Code -- although <code>ring</code> or <code>linarith</code> might produce huge proof terms, size or complexity of proof terms is an extremely poor measure of proof complexity to a human mathematician. I know that <code>ring</code> is just expanding out the brackets and tidying up, the fact that it turns out that it takes 100 applications of the axioms of a ring to justify this is of absolutely no interest to me. I've noticed that this is a big difference in attitude between mathematicians and computer scientists, and because computer scientists have traditionally dominated this area I think readability has somehow been made a bigger deal than perhaps I would expect.</p>",
        "id": 318455994,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1672333959
    },
    {
        "content": "<p>I care a lot about readability of definitions and theorems. I care a bit about readability of lemmata. I don't care about readability of proofs.</p>\n<p>I like it when proofs are maintainable (for a developer who has invested some time to understanding the codebase) but I couldn't care less about how readable my proofs are without IDE.</p>",
        "id": 318467688,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672338833
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318455994\">said</a>:</p>\n<blockquote>\n<p>I'll just chime in with my usual message that in maths the things we're proving in mathlib are either completely mathematically trivial or are very well-documented in standard textbooks so readability is not really something I'm too bothered about. I find lean proofs very easy to read if I step through them in VS Code -- although <code>ring</code> or <code>linarith</code> might produce huge proof terms, size or complexity of proof terms is an extremely poor measure of proof complexity to a human mathematician. I know that <code>ring</code> is just expanding out the brackets and tidying up, the fact that it turns out that it takes 100 applications of the axioms of a ring to justify this is of absolutely no interest to me. I've noticed that this is a big difference in attitude between mathematicians and computer scientists, and because computer scientists have traditionally dominated this area I think readability has somehow been made a bigger deal than perhaps I would expect.</p>\n</blockquote>\n<p>There are at least three different reasons I can think of, which makes readable presentations of proofs important:</p>\n<ol>\n<li>Debugging: this is the reason people across CS, outside theory, care about it</li>\n<li>\n<p>Explanatory value: Often these days, the crux of theory papers are not results but algorithmic or proof techniques. It is debatable how much this can be rigorously formalized (or how readable even the statements of such theorems will be). Such techniques might then give you upper/lower bounds on a number of problems, when adapted to each specific problem. The technique would then be the key contribution of the paper, and seeing it action would be important. Typically what one can formalize might be specific uses of that technique.</p>\n</li>\n<li>\n<p>Pedagogy: Good proofs are often very revealing. They teach students how to think about a problem, and often also yield algorithms or ways to generate counter examples. After all students need to learn to prove claims, not just learn statements. With some tactic based proofs it is clear what we are doing at each. For example proofs that use linarith or rw with specific theorems, or simp only. Other tactics might just look magical</p>\n</li>\n</ol>",
        "id": 318469568,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672339662
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318469568\">said</a>:</p>\n<blockquote>\n<ol>\n<li>Debugging: this is the reason people across CS, outside theory, care about it</li>\n</ol>\n</blockquote>\n<p>Our stance on debugging will be very different from what software engineers think. When you write usual programs, you never know when another bug will be reported, forcing you to work with your code again. In theorem proving, we do our \"debugging\" only once. When our proof is done, we are guaranteed that there will never be a bug. If my code is incomprehensible for the future me, it isn't such a problem.</p>",
        "id": 318473295,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672341192
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318469568\">said</a>:</p>\n<blockquote>\n<ol start=\"2\">\n<li>Explanatory value: (...) The technique would then be the key contribution of the paper, and seeing it action would be important. (...)</li>\n</ol>\n</blockquote>\n<p>Reusability of our technique should be served in form of reusable lemmas. In formal proofs, we write much more of them than in usual theory papers. I'd focus on how the lemma is stated (and if the lemma cannot be reused, it will hopefully be at least possible to write an analogical lemma for the different settings). I don't want to dig into the proof of the lemma in order to extract some technique from within the proof to use it elsewhere. Another amazing thing is when you create a reusable tactic. With tactics, however, it is much harder to achieve high-level applicability.</p>",
        "id": 318474139,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672341564
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318473295\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318469568\">said</a>:</p>\n<blockquote>\n<ol>\n<li>Debugging: this is the reason people across CS, outside theory, care about it</li>\n</ol>\n</blockquote>\n<p>Our stance on debugging will be very different from what software engineers think. When you write usual programs, you never know when another bug will be reported, forcing you to work with your code again. In theorem proving, we do our \"debugging\" only once. When our proof is done, we are guaranteed that there will never be a bug. If my code is incomprehensible for the future me, it isn't such a problem.</p>\n</blockquote>\n<p>I disagree with this, maintaining a formal mathematics library has more similarities than differences to maintaining a software library. It's not true that proofs are written once and never returned to. Mathlib is refactored all the time, to add new typeclasses, reorder developments, etc. Proofs need to change when their dependencies change, or the tactics that they use change.</p>\n<p>It is also not true that a proof that compiles does not have a bug, it's just that the bugs are of a different nature than correctness. For example you can have an instance that loops, or simp lemma that causes non-confluence. The theorem itself is perfectly true, but it probably shouldn't have been annotated the way it was. Also it could be proving the wrong thing, either because the author didn't notice that the theorem was trivial or because a subsequent change to elaboration or something causes the statement / proof to be interpreted wrongly. These often lead to errors in the proof but not always - you can end up with a true proof of a garbage theorem.</p>",
        "id": 318475399,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672342139
    },
    {
        "content": "<p>I recall reading something like  \"formal verification is only as good as the specification and tools allow\"</p>",
        "id": 318480067,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672344469
    },
    {
        "content": "<p>When I came to lean in 2017 I told Mario that I could not possibly use his silly library with my students it couldn't even prove that 2 + 2 = 4 in the real numbers. He responded by writing <code>norm_num</code>. Chris Hughes then told him that proving that every positive integer was the sum of four squares was a nightmare and Mario wrote <code>ring</code>, following work of Assia and her group. Patrick complained about how he couldn't teach analysis because he couldn't do inequalities and Rob responded by writing <code>norm_num</code>. All of these tactics are writing completely unreadable code but which mathematician would want to read it? In all cases these tactics are doing things which humans find completely trivial. Type class inference is another example. I neither know nor care how it works. Coercions another, this one solved with <code>rorm_num</code>. I have absolutely no interest in what is going on under the hood here -- I am a mathematician!</p>",
        "id": 318497157,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1672354399
    },
    {
        "content": "<p>*<code>linarith</code> for the third, <code>norm_cast</code> for the fourth</p>",
        "id": 318499372,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1672355848
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318497157\">said</a>:</p>\n<blockquote>\n<p>When I came to lean in 2017 I told Mario that I could not possibly use his silly library with my students it couldn't even prove that 2 + 2 = 4 in the real numbers. He responded by writing <code>norm_num</code>. Chris Hughes then told him that proving that every positive integer was the sum of four squares was a nightmare and Mario wrote <code>ring</code>, following work of Assia and her group. Patrick complained about how he couldn't teach analysis because he couldn't do inequalities and Rob responded by writing <code>norm_num</code>. All of these tactics are writing completely unreadable code but which mathematician would want to read it? In all cases these tactics are doing things which humans find completely trivial. Type class inference is another example. I neither know nor care how it works. Coercions another, this one solved with <code>rorm_num</code>. I have absolutely no interest in what is going on under the hood here -- I am a mathematician!</p>\n</blockquote>\n<p>I wonder if the proof culture in different fields is different. I have encountered differences even within CS. Programming Languages and formal methods people are rigorous to a fault. They dot their i's and cross their t's. Cryptographic protocols people love machine checked proofs (see: tamarin prover, F*). Consensus folks have their way of writing proofs. Algorithms and Complexity theory people have this unrolling and verbose style of writing proofs (due to constraints of the conference system) which focuses on highlighting the key ideas  and their application, at many levels of abstraction. An algorithm's proof of correctness might be key to understanding why it works.  Based on my conversations with physicists, they too seem to care about the insights offered by a good proof.</p>",
        "id": 318499695,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672356086
    },
    {
        "content": "<p>What I am worried about are situations when one needs to illustrate, for example, a reduction by proving it with some new gadget or technique. Suppose I write a tactic based proof that doesn't make it clear how the technique was combined with other theorems (let's say a bunch of simps and library searches). If the output of the proof is not readable, then how do I know anything about the technique and its usage? Was it even used? How do I get the insights and ideas for new problems/algorithms that well structured and written proofs might provide me?</p>",
        "id": 318500073,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672356389
    },
    {
        "content": "<p>If you actually have an abstract idea in your proof I don't see why your abstract idea couldn't be expressed via a tactic or a lemma on its own. If I see a <code>simp</code> I know \"okay you chained a bunch of well known equalities + optional ones you defined in the parameter set\" if I see a ring I know \"okay this was solved by using ring axioms\" if I see linarith I know \"this was solved by some decision procedure for linear arithmetic\". If you have an abstract idea for a proof in complexy theory write a tactic for it and write that tactic in a way that is nice to understand. It doesn't even have to be a special meta program it can also just be some arrangement of tactics with a macro. If your proof idea has such interesting insight that it is useful in general you can also just put it as a sepqrate lemma for everyone to read like say the pumping lemma for regular languages.</p>\n<p>The closest i've seen to \"readable\" proofs would be Isabelle where they can mostly make leaps in logic <em>because</em> their automation with things like <code>sledgehammer</code>, <code>blast</code>, <code>auto</code> etc. are so good. So when they write their proofs in this Isar style they can just make a leap from one statement to another, put a <code>sledgehammer</code> under in and the system will do the rest (of course this doesn't work out always, but quite often).</p>",
        "id": 318500664,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1672356821
    },
    {
        "content": "<p>If you are ever actually in the kind of situation you're worried about, I think you should be very happy!</p>",
        "id": 318501500,
        "sender_full_name": "Reid Barton",
        "timestamp": 1672357475
    },
    {
        "content": "<p>Because I expect you will in practice have the opposite problem (need to spell out a huge amount of stuff explicitly).</p>",
        "id": 318501653,
        "sender_full_name": "Reid Barton",
        "timestamp": 1672357584
    },
    {
        "content": "<p>\"If you have an abstract idea for a proof in complexity theory write a tactic for it and write that tactic in a way that is nice to understand\" =&gt; I expect that this will be key for the complexity theory part of mathlib. I can imagine that reductions will be done with tactics that  take some gadgets and lemmas. Similarly, for deducing the existence of greedy algorithms for problem, tactics related to matroids might be handy</p>",
        "id": 318501683,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672357620
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110032\">Reid Barton</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318501653\">said</a>:</p>\n<blockquote>\n<p>Because I expect you will in practice have the opposite problem (need to spell out a huge amount of stuff explicitly).</p>\n</blockquote>\n<p>I expect this to happen as well.  Some tactics based proofs (especially ones that use limited tactics like <code>cases</code>, <code>linarith</code>, <code>rw</code>, <code>simp only</code> or <code>induction</code>) seem more readable compared to pen and paper versions of these proofs.</p>",
        "id": 318501815,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672357727
    },
    {
        "content": "<p>If there is a hypothetical tactic line that were to read <code>karp_reduction by [&lt;some_reduction_theorems&gt;, &lt;reduction_maps&gt;]</code> that is already quite readable.  Even more readable and informative is a proof that specifies the reduction step, then produces the goals about equivalence and complexity, which are in turn proved by some one line tactics.</p>",
        "id": 318502382,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672358199
    },
    {
        "content": "<p>While that might be an eventual goal, I think it is important to not jump ahead and try to implement those tactics before the underlying machinery is proven out. Essentially, you have to do it the \"hard way\" at least a few times before you are in a good enough position to know what can / should be automated away, by looking at similarities and differences in the arguments</p>",
        "id": 318502719,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672358449
    },
    {
        "content": "<p>(\"machinery\" here meaning primarily lemmas!)</p>",
        "id": 318502811,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672358517
    },
    {
        "content": "<p>tactics automate proofs, and proofs are made of lemmas, so until the lemmas are available and work well together the tactic can't be written</p>",
        "id": 318502872,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672358560
    },
    {
        "content": "<p>I think you will understand our position on tactics once you try proving some contentful result.</p>",
        "id": 318531650,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1672382572
    },
    {
        "content": "<p>Let me also mention that tactics tend to make proofs <em>more</em> readable, not less. Because there are less tactic calls, the crucial ones are not lost in a sea of trivial rewrites. My own style changed quite a lot in the past months and I finally am following <span class=\"user-mention\" data-user-id=\"246273\">@Bhavik Mehta</span>'s long told advice to use <code>simp</code> rather three lines <code>rw</code> calls.</p>",
        "id": 318531886,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1672382793
    },
    {
        "content": "<p>For readability inside IDE, I prefer long <code>rw</code> calls.</p>",
        "id": 318540243,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672388846
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"387244\">Yaël Dillies</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318531886\">said</a>:</p>\n<blockquote>\n<p>Let me also mention that tactics tend to make proofs <em>more</em> readable, not less. Because there are less tactic calls, the crucial ones are not lost in a sea of trivial rewrites. My own style changed quite a lot in the past months and I finally am following <span class=\"user-mention silent\" data-user-id=\"246273\">Bhavik Mehta</span>'s long told advice to use <code>simp</code> rather three lines <code>rw</code> calls.</p>\n</blockquote>\n<p>Part of my challenge is to also convince people around me that this is useful for our field (and it does). Some of them got their hands burnt trying to formalise things with Isabelle. Their complaints were that the prover had to be taught the basics and that they couldn't understand the arcane language of the prover. A bunch of rw's go a long way towards this than a simp in such cases.</p>",
        "id": 318566617,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672400155
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"387244\">Yaël Dillies</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318531886\">said</a>:</p>\n<blockquote>\n<p>Let me also mention that tactics tend to make proofs <em>more</em> readable, not less. Because there are less tactic calls, the crucial ones are not lost in a sea of trivial rewrites. My own style changed quite a lot in the past months and I finally am following <span class=\"user-mention silent\" data-user-id=\"246273\">Bhavik Mehta</span>'s long told advice to use <code>simp</code> rather three lines <code>rw</code> calls.</p>\n</blockquote>\n<p>Part of my challenge is to also convince people around me that this is usable for our field (and it is). Some of them got their hands burnt trying to formalise things with Isabelle. Their complaints were that the prover had to be taught the basics and that they couldn't understand the arcane language of the prover. A bunch of rw's go a long way towards this than a simp in such cases.</p>",
        "id": 318566742,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672400187
    },
    {
        "content": "<p>Sorry, on my phone. Zulip's interface is terrible</p>",
        "id": 318566852,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672400235
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318566617\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"387244\">Yaël Dillies</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318531886\">said</a>:</p>\n<blockquote>\n<p>Let me also mention that tactics tend to make proofs <em>more</em> readable, not less. Because there are less tactic calls, the crucial ones are not lost in a sea of trivial rewrites. My own style changed quite a lot in the past months and I finally am following <span class=\"user-mention silent\" data-user-id=\"246273\">Bhavik Mehta</span>'s long told advice to use <code>simp</code> rather three lines <code>rw</code> calls.</p>\n</blockquote>\n<p>Part of my challenge is to also convince people around me that this is useful for our field (and it does). Some of them got their hands burnt trying to formalise things with Isabelle. Their complaints were that the prover had to be taught the basics and that they couldn't understand the arcane language of the prover. A bunch of rw's go a long way towards this than a simp in such cases.</p>\n</blockquote>\n<p>\"They couldnt understand the arcane language of the prover\" vs \"doing everything by hand to the prover because I only have this one way to talk to it\" Seems precisely like you are just teaching the basics to the prover. If you were to tell these people that they are supposed to prove some complex looking polynomial is equivalent to another complex looking polynomial would you really want to use a rewrite chain instead of saying \"by ring\" ? That just sounds like you are making your life unnecessarily hard.</p>",
        "id": 318570379,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1672401509
    },
    {
        "content": "<p>Fair. I guess I am trying to find a trade off between readability and automation in tactics, and the trade off is at a point where I can read the proof and see all the key ideas falling into place.</p>",
        "id": 318572834,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672402288
    },
    {
        "content": "<p>Again in practice I think it will be very difficult to get anywhere near the amount of automation you would like.</p>",
        "id": 318576500,
        "sender_full_name": "Reid Barton",
        "timestamp": 1672403635
    },
    {
        "content": "<p>Yeah. The tactics are far less powerful than what we wish we had. It happened to me only twice in my life that a tactic closed a goal that I didn't see in my head why it holds.</p>",
        "id": 318588373,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672407582
    },
    {
        "content": "<p>Wow this got kind of busy, so I was exploring a bit (instead of watching this thread) and learning a lot of lean and I have some more thoughts.<br>\nOne way to approach complexity is something inherent in the function, so the conceptual lean function of nat.add has some complexity regardless of how how its actually defined (since definitions are effectively omega equivalent this is unavoidable).</p>\n<p>One way to define complexity is by encoding the problem in a well-known space, in my code I've used untyped lambda calculus (because lean is lambda calculus based - but also partly I underestimated how much of a pain confluence would be to prove). And then counting steps in the new space, in this case beta-reductions in lambda calculus (it can also be replaced with a cost function - from a quick literature search the time complexity of lambda calculus is a contentious topic - and beta-reductions should have different costs, for example I have adding 2 church numerals as a constant 6 reductions - which at first glance seems wrong, on the other hand unary numerals are basically linked lists and appending two linked lists together can be done in constant time so maybe its not as crazy as it seems). I haven't quite gotten there, but ideally we don't actually need lambda calculus and can do all of the theorem proving about lean functions, just using lambda calculus as the underlying shim.</p>\n<p>It culminates with <a href=\"https://github.com/calcu16/lean_complexity/blob/main/src/lambda_calculus/utlc/complexity.lean\">https://github.com/calcu16/lean_complexity/blob/main/src/lambda_calculus/utlc/complexity.lean</a>, which has the example</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">example</span> <span class=\"o\">:</span> <span class=\"n\">complexity_le</span> <span class=\"n\">nat.add</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"mi\">6</span><span class=\"o\">:</span><span class=\"n\">ℕ</span><span class=\"o\">))</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"n\">use</span> <span class=\"n\">encoding.nat.add</span><span class=\"o\">,</span>\n  <span class=\"n\">intros</span> <span class=\"n\">x</span> <span class=\"n\">y</span><span class=\"o\">,</span>\n  <span class=\"n\">apply</span> <span class=\"n\">utlc.encoding.nat.add_distance_le</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>while I did need to reference lambda calculus in the proof, I didn't in the definition. And hopefully there can be enough theorems that its not needed.</p>",
        "id": 318808166,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672540959
    },
    {
        "content": "<p>The downside of this approach is that it would be difficult (perhaps impossible) to prove differences in complexity between different algorithms that solve the same problem.</p>",
        "id": 318808784,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672541534
    },
    {
        "content": "<p>Ok, got a proof working for nat.sub that doesn't require knowing about the underlying shim but just the complexities of nat.iterate and nat.pred.<br>\n<a href=\"https://github.com/calcu16/lean_complexity/blob/main/src/lambda_calculus/complexity/nat.lean\">https://github.com/calcu16/lean_complexity/blob/main/src/lambda_calculus/complexity/nat.lean</a></p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">theorem</span> <span class=\"n\">sub_complexity</span><span class=\"o\">:</span> <span class=\"n\">complexity_le</span> <span class=\"n\">nat.sub</span>\n  <span class=\"o\">(</span><span class=\"n\">cast</span> <span class=\"o\">(</span><span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"mi\">2</span> <span class=\"bp\">*</span> <span class=\"n\">n</span> <span class=\"bp\">+</span> <span class=\"mi\">13</span><span class=\"o\">)</span> <span class=\"bp\">*</span> <span class=\"n\">m</span> <span class=\"bp\">+</span> <span class=\"mi\">4</span><span class=\"o\">))</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"k\">have</span> <span class=\"n\">hsub</span> <span class=\"o\">:</span> <span class=\"n\">nat.sub</span> <span class=\"bp\">=</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span> <span class=\"n\">nat.pred</span><span class=\"bp\">^</span><span class=\"o\">[</span><span class=\"n\">m</span><span class=\"o\">]</span> <span class=\"n\">n</span><span class=\"o\">),</span>\n  <span class=\"o\">{</span> <span class=\"n\">ext1</span> <span class=\"n\">n</span><span class=\"o\">,</span> <span class=\"n\">ext1</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n    <span class=\"n\">induction</span> <span class=\"n\">m</span> <span class=\"n\">generalizing</span> <span class=\"n\">n</span><span class=\"o\">,</span>\n    <span class=\"o\">{</span> <span class=\"n\">simp</span> <span class=\"o\">[</span><span class=\"n\">nat.sub</span><span class=\"o\">]</span> <span class=\"o\">},</span>\n    <span class=\"n\">simp</span> <span class=\"o\">[</span><span class=\"n\">nat.sub</span><span class=\"o\">],</span>\n    <span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"n\">m_ih</span><span class=\"o\">,</span> <span class=\"n\">f_iterate</span> <span class=\"n\">nat.pred</span><span class=\"o\">],</span>\n    <span class=\"n\">simp</span> <span class=\"o\">},</span>\n  <span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"n\">hsub</span><span class=\"o\">],</span>\n  <span class=\"n\">apply</span> <span class=\"n\">iteration_complexity_le</span> <span class=\"n\">pred_complexity</span><span class=\"o\">,</span>\n  <span class=\"n\">intros</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n  <span class=\"n\">induction</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n  <span class=\"o\">{</span> <span class=\"n\">simp</span> <span class=\"o\">},</span>\n  <span class=\"n\">simp</span> <span class=\"n\">at</span> <span class=\"bp\">*</span><span class=\"o\">,</span>\n  <span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"k\">show</span> <span class=\"o\">(</span><span class=\"n\">nat.pred</span><span class=\"bp\">^</span><span class=\"o\">[</span><span class=\"n\">m_n</span><span class=\"o\">]</span> <span class=\"n\">n</span><span class=\"o\">)</span> <span class=\"bp\">=</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">a</span> <span class=\"n\">b</span><span class=\"o\">,</span> <span class=\"n\">nat.pred</span><span class=\"bp\">^</span><span class=\"o\">[</span><span class=\"n\">b</span><span class=\"o\">]</span> <span class=\"n\">a</span><span class=\"o\">)</span> <span class=\"n\">n</span> <span class=\"n\">m_n</span><span class=\"o\">,</span> <span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">,</span>\n      <span class=\"bp\">←</span> <span class=\"n\">hsub</span><span class=\"o\">,</span>\n      <span class=\"k\">show</span> <span class=\"n\">n.sub</span> <span class=\"n\">m_n</span> <span class=\"bp\">=</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">m_n</span><span class=\"o\">,</span> <span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">[</span><span class=\"n\">has_sub.sub</span><span class=\"o\">],</span>\n      <span class=\"n\">nat.succ_eq_add_one</span><span class=\"o\">],</span>\n  <span class=\"n\">apply</span> <span class=\"n\">le_trans</span><span class=\"o\">,</span>\n  <span class=\"o\">{</span> <span class=\"n\">repeat</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">add_le_add</span> <span class=\"o\">},</span>\n    <span class=\"n\">apply</span> <span class=\"n\">mul_le_mul</span><span class=\"o\">,</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">nat.sub_le</span> <span class=\"o\">},</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">m_ih</span> <span class=\"o\">},</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">nat.zero_le</span> <span class=\"o\">},</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">le_refl</span> <span class=\"o\">}</span> <span class=\"o\">},</span>\n  <span class=\"n\">ring_nf</span><span class=\"o\">,</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>most of the proof is just math (unfortunately I couldn't convince linarith/nlinarith to do my dirty work), the main work is done by</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">apply</span> <span class=\"n\">iteration_complexity_le</span> <span class=\"n\">pred_complexity</span><span class=\"o\">,</span>\n</code></pre></div>\n<p>which in turn synthesizes the underyling untyped-lambda calculus, but from the nat.sub prover's perspective we just get back a recursive cost function that we need to prove is always less than or equal to our supplied function. Ideally we'd be able to prove directly from nat.sub's definition instead of that nat.iterate shim, but I'm not 100% sure how to manipulate the implicit structural recursion of lean into a a lean theorem (I tried to use nat.rec/nat.rec_on instead of nat.iterate but couldn't get it to work). Theorems for currying and working with pairs seem pretty straightforward, additionally we could probably add O notation on top of this pretty easily (and have an easier iterate lemma).</p>\n<p>Responding to <span class=\"user-mention\" data-user-id=\"466334\">@Shreyas Srinivas</span> </p>\n<blockquote>\n<p>Now let's say you are writing a paper where all you care about is that DFS runs in O(|V| + |E|) time. Should you spend time on sorting through these implementations or building your own?<br>\nAnother thought is that even in this model you could always have proofs of the form</p>\n</blockquote>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">complexity_le</span> <span class=\"n\">dfs</span> <span class=\"n\">O</span><span class=\"o\">(</span><span class=\"bp\">|</span><span class=\"n\">V</span><span class=\"bp\">|</span> <span class=\"bp\">+|</span><span class=\"n\">E</span><span class=\"bp\">|</span><span class=\"o\">)</span> <span class=\"bp\">-&gt;</span> <span class=\"n\">complexity_le</span> <span class=\"n\">my_alg</span> <span class=\"n\">O</span><span class=\"o\">(</span><span class=\"n\">f</span><span class=\"o\">)</span>\n</code></pre></div>\n<p>You do technically run the risk of proving the latter time complexity by proving bottom, but (a) proving lower bounds for an implementation of a mathematical functions is really hard (see P vs. NP) and (b) if it doesn't hold, that kind of invalidates the theorem anyway.</p>\n<p>Also if someone comes along later and fills in the proof of dfs, you would get a pure proof of my_alg for free.</p>",
        "id": 318937233,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672638515
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"562941\">Andrew Carter</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318937233\">said</a>:</p>\n<blockquote>\n<p>Ok, got a proof working for nat.sub that doesn't require knowing about the underlying shim but just the complexities of nat.iterate and nat.pred.<br>\n<a href=\"https://github.com/calcu16/lean_complexity/blob/main/src/lambda_calculus/complexity/nat.lean\">https://github.com/calcu16/lean_complexity/blob/main/src/lambda_calculus/complexity/nat.lean</a></p>\n<p><div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">theorem</span> <span class=\"n\">sub_complexity</span><span class=\"o\">:</span> <span class=\"n\">complexity_le</span> <span class=\"n\">nat.sub</span>\n  <span class=\"o\">(</span><span class=\"n\">cast</span> <span class=\"o\">(</span><span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"mi\">2</span> <span class=\"bp\">*</span> <span class=\"n\">n</span> <span class=\"bp\">+</span> <span class=\"mi\">13</span><span class=\"o\">)</span> <span class=\"bp\">*</span> <span class=\"n\">m</span> <span class=\"bp\">+</span> <span class=\"mi\">4</span><span class=\"o\">))</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"k\">have</span> <span class=\"n\">hsub</span> <span class=\"o\">:</span> <span class=\"n\">nat.sub</span> <span class=\"bp\">=</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span> <span class=\"n\">nat.pred</span><span class=\"bp\">^</span><span class=\"o\">[</span><span class=\"n\">m</span><span class=\"o\">]</span> <span class=\"n\">n</span><span class=\"o\">),</span>\n  <span class=\"o\">{</span> <span class=\"n\">ext1</span> <span class=\"n\">n</span><span class=\"o\">,</span> <span class=\"n\">ext1</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n    <span class=\"n\">induction</span> <span class=\"n\">m</span> <span class=\"n\">generalizing</span> <span class=\"n\">n</span><span class=\"o\">,</span>\n    <span class=\"o\">{</span> <span class=\"n\">simp</span> <span class=\"o\">[</span><span class=\"n\">nat.sub</span><span class=\"o\">]</span> <span class=\"o\">},</span>\n    <span class=\"n\">simp</span> <span class=\"o\">[</span><span class=\"n\">nat.sub</span><span class=\"o\">],</span>\n    <span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"n\">m_ih</span><span class=\"o\">,</span> <span class=\"n\">f_iterate</span> <span class=\"n\">nat.pred</span><span class=\"o\">],</span>\n    <span class=\"n\">simp</span> <span class=\"o\">},</span>\n  <span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"n\">hsub</span><span class=\"o\">],</span>\n  <span class=\"n\">apply</span> <span class=\"n\">iteration_complexity_le</span> <span class=\"n\">pred_complexity</span><span class=\"o\">,</span>\n  <span class=\"n\">intros</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n  <span class=\"n\">induction</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n  <span class=\"o\">{</span> <span class=\"n\">simp</span> <span class=\"o\">},</span>\n  <span class=\"n\">simp</span> <span class=\"n\">at</span> <span class=\"bp\">*</span><span class=\"o\">,</span>\n  <span class=\"n\">rw</span> <span class=\"o\">[</span><span class=\"k\">show</span> <span class=\"o\">(</span><span class=\"n\">nat.pred</span><span class=\"bp\">^</span><span class=\"o\">[</span><span class=\"n\">m_n</span><span class=\"o\">]</span> <span class=\"n\">n</span><span class=\"o\">)</span> <span class=\"bp\">=</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">a</span> <span class=\"n\">b</span><span class=\"o\">,</span> <span class=\"n\">nat.pred</span><span class=\"bp\">^</span><span class=\"o\">[</span><span class=\"n\">b</span><span class=\"o\">]</span> <span class=\"n\">a</span><span class=\"o\">)</span> <span class=\"n\">n</span> <span class=\"n\">m_n</span><span class=\"o\">,</span> <span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">,</span>\n      <span class=\"bp\">←</span> <span class=\"n\">hsub</span><span class=\"o\">,</span>\n      <span class=\"k\">show</span> <span class=\"n\">n.sub</span> <span class=\"n\">m_n</span> <span class=\"bp\">=</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">m_n</span><span class=\"o\">,</span> <span class=\"kd\">by</span> <span class=\"n\">simp</span><span class=\"o\">[</span><span class=\"n\">has_sub.sub</span><span class=\"o\">],</span>\n      <span class=\"n\">nat.succ_eq_add_one</span><span class=\"o\">],</span>\n  <span class=\"n\">apply</span> <span class=\"n\">le_trans</span><span class=\"o\">,</span>\n  <span class=\"o\">{</span> <span class=\"n\">repeat</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">add_le_add</span> <span class=\"o\">},</span>\n    <span class=\"n\">apply</span> <span class=\"n\">mul_le_mul</span><span class=\"o\">,</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">nat.sub_le</span> <span class=\"o\">},</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">m_ih</span> <span class=\"o\">},</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">nat.zero_le</span> <span class=\"o\">},</span>\n    <span class=\"n\">any_goals</span> <span class=\"o\">{</span> <span class=\"n\">apply</span> <span class=\"n\">le_refl</span> <span class=\"o\">}</span> <span class=\"o\">},</span>\n  <span class=\"n\">ring_nf</span><span class=\"o\">,</span>\n<span class=\"kd\">end</span>\n</code></pre></div><br>\n</p>\n</blockquote>\n<p>This is impressive. In general algorithms and complexity people might not see the value in complexity theory expressed in terms of lambda calculus, primarily because they are not accustomed to the model, but also because of the complexity blow up in beta reductions and complexity issues arising from space usage or persistent structures. It might prove to be just the thing that algorithm designers for functional languages want (the line of work starting at Chris Okasaki's thesis), especially considering how hard it is to understand the complexity of algorithms in languages with, say, lazy evaluation, compared to imperative RAM model style descriptions. Haskell people might really like any tool that makes it easy for them to reason about the complexity measures for their code.</p>",
        "id": 318980137,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672660936
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/318980137\">said</a>:</p>\n<blockquote>\n<p>This is impressive. In general algorithms and complexity people might not see the value in complexity theory expressed in terms of lambda calculus, primarily because they are not accustomed to the model, but also because of the complexity blow up in beta reductions and complexity issues arising from space usage or persistent structures. It might prove to be just the thing that algorithm designers for functional languages want (the line of work starting at Chris Okasaki's thesis), especially considering how hard it is to understand the complexity of algorithms in languages with, say, lazy evaluation, compared to imperative RAM model style descriptions. Haskell people might really like any tool that makes it easy for them to reason about the complexity measures for their code.</p>\n</blockquote>\n<p>It seems to me that the blow up (or lack thereof) in beta reductions is still open, but I'm not an expert here, but I did run into two papers while I was searching: <a href=\"https://arxiv.org/abs/1405.3311\">https://arxiv.org/abs/1405.3311</a> which based on the abstract seems to indicate the number of steps in a \"normal\" reduction is cost invariant, while <a href=\"https://arxiv.org/abs/cs/0511045\">https://arxiv.org/abs/cs/0511045</a> proposes a cost function for call-by-value. I don't actually enforce an evaluation strategy (many of the proofs use normal reduction, but then substitute previous internally which is essentially call-by-value). As I mentioned earlier, at least in the examples I've done so far, I think I could simulate all of the complexities in an imperative language (e.g. the constant time of nat.add doesn't bother me since I could use a linked list).</p>\n<p>However, any cost model could be plugged instead of counting the number of beta reductions and all of the rest of the machinery should still hold. One thought I had since yesterday is that complexity_le could be defined with a class of the underlying model, then people could prove complexities about any model instead of just utlc, for example in any model where dfs has a O(|V|+|E|) runtime (and some basic rules about composition) then my_alg has a runtime of O(f(n)).</p>\n<p>That being said, the reason I ended up going down the lambda calculus rabbit hole was because it seemed intuitively easier to prove composition properties of lambda calculus vs a RAM model or TM (and the nat input restriction on partrec seemed too restrictive). In a RAM machine you have to prove that pure functions that take pointers somehow only touch the memory they are supposed to touch and that pointers don't overlap, or if they do overlap its intentional etc. whereas with a functional model we get that for free.</p>",
        "id": 319024790,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672677726
    },
    {
        "content": "<p>I can't see this pointer issue causing us any trouble. Do you have any examples?</p>",
        "id": 319029296,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672679846
    },
    {
        "content": "<p>I am not entirely clear on how the papers circumvent the explosion in term size with beta reduction (because I only skimmed them. Time crunch). But the polynomial overhead alone is not something we can really accept. There are too many placeswhere polynomial factors are relevant.</p>",
        "id": 319030641,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672680533
    },
    {
        "content": "<p>I guess I'm having trouble envisioning general proofs of things like (a → b) → (a, c) → (b, c), because what if the (a → b) transformation messes with c. Similarly a function like memmove (3) has to choose the iteration order in order to avoid smashing existing data. Maybe its not difficult at all, but I'm not seeing the strategy/invariants. On the other hand for lambda calculus, just making sure everything is closed gets you pretty far, and making sure the \"data\" is in normal form gets you the rest of the way. That being said most of the machinery is replaceable,  so if you replace the \"g·(@encode α e a)\" and \"g ≡β (@encode _ e f)\" with a different model (or change the cost of ≡β) everything above it should still work.</p>\n<p>So I just skimmed the paper as well, but I think its conceptually similar to my argument about a constant time unary add. You can represent n copies of something in log(n) time/space (or equivalently 2^m copies of something in m time/space) and it only becomes a problem if you actually try and use them, but if you do use them then it takes n/2^m beta reductions anyway. Using the nat example, while adding a number in unary is O(1) comparing two numbers is O(n), while in a binary nat adding two numbers is O(log n) and comparing two numbers is also O(log n). I think both of these are achievable both in lambda calculus and practically.</p>\n<p>I think the polynomial term is probably overblown in practice, the biggest thing is that you would have to use trees instead of arrays which gives a log n factor, (although there are structures such as finger trees where even that is avoidable). Additionally if you access/mutate the memory in a structured manner (such as with map or fold) then it remains constant time. I would actually argue that constant time random access is a practical fiction anyway and given that we live in a 3D world with a speed-of-light upper bound, the best you can actually get is O(n^(1/3)) random access which is considerably worse than O(log n). Is there a particular (useful) algorithm (or class of algorithms) that you are thinking of that an imperative model and the lambda calculus equivalent would have more than a logarithmic difference?</p>",
        "id": 319033430,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672681929
    },
    {
        "content": "<p>The approach we hashed out here was a step counter with a <code>prog</code> type. A cost function, specific to the model, determines how operations are counted as they are built up from other operations. So for a data structure of type \\a, the respective insert/update operations are expressed in terms of the operations in <code>prog</code>. </p>\n<p>About your question: Ignoring polynomial factors is out of the question for fine grained complexity, most query-based models, complexity of dynamic algorithms, PCPs, parallel complexity classes, circuit complexity etc.</p>",
        "id": 319079908,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672684417
    },
    {
        "content": "<p>That's just the tip of the iceberg</p>",
        "id": 319079935,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672684433
    },
    {
        "content": "<p>In general, to be useful as a library for theorists, model specific complexities will be required, so just doing everything in utlc is not going to be either useful or easy to translate results into</p>",
        "id": 319080054,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672684489
    },
    {
        "content": "<p>In general, there are so many insights/results that are gained from one model and adapted to another through more clever results, that we will not be able to escape defining a gazillion models. Starting with the RAM model makes sense because it probably has the least \"moving parts\" and is core to undergraduate material, while also serving as the basis for many other models.</p>",
        "id": 319080464,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672684689
    },
    {
        "content": "<p><code>utlc</code> won't easily transfer to other models. So we will be spending so much time writing non-tri ial  translations, provided they exist in the first place</p>",
        "id": 319080795,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672684884
    },
    {
        "content": "<p>Yeah, the step here is a β-reduction, and at the moment every reduction has a cost of 1.</p>\n<p>| In general, to be useful as a library for theorists, model specific complexities will be required, so just doing everything in utlc is not going to be either useful or easy to translate results into<br>\nWhat I would like to prove is things about mathematical functions, as in the proof of sub there probably isn't actually a need for a prover to use utlc beyond a few primitives (or micro-optimizations).</p>\n<p>I don't think query-based models or dynamic algorithms have anymore than a polylog difference (happy to take a reference otherwise). For the most part I think the polynomial factor comes from a reduction from a TM and the strong church-turing thesis. PCPs have a problem of generating random numbers in any model (you could attach an RNG as a list of bits lambda calculus argument though and then make arguments about it over the space of all lists). Parallel complexity and circuit complexity are difficult with either approach, especially if the problem isn't trivially parallelizable (that being said there is a notion of a parallel β step for those that are).</p>\n<p>Given that lean is lambda calculus based, I would argue that a lambda-calculus based model has the least moving parts. Ideally most of the work can be done similar to sub where the utlc model isn't even touched, and so if there are other models with similar properties then the proofs transfer over directly. Even more so since the algorithms can be written in lean. The model is just a shim to prove things about mathematical functions.</p>",
        "id": 319081495,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672685360
    },
    {
        "content": "<p>Somewhat more concretely, I'm pretty sure I could prove (at some point, still need more machinery) that if the CS 125 RAM model has a cost function of w (or log S) for each step (except for division and multiplication, I'm not sure why division is one of the primitives) then a lambda calculus model could simulate it within a constant factor. That being said the reverse direction I'll grant is less obviously true.</p>",
        "id": 319082850,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672686127
    },
    {
        "content": "<p>Another reason to pursue RAM is, in addition to its direct applicability: We might attract more theorists, like mathlib attracted mathematicians, if we work in models that most theorists already understand</p>",
        "id": 319093106,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672692419
    },
    {
        "content": "<p>At least conceptually, RAM and TM are fundamental to algorithms and complexity. A lot of research would have to go towards addressing the issues with time and space complexity issues related to beta reduction, going by the papers you suggested.</p>",
        "id": 319093406,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672692582
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319093406\">said</a>:</p>\n<blockquote>\n<p>At least conceptually, RAM and TM are fundamental to algorithms and complexity. A lot of research would have to go towards addressing the issues with time and space complexity issues related to beta reduction, going by the papers you suggested.</p>\n</blockquote>\n<p>My experience in class was that a lot of algorithms were described in a recursive manner conceptually similar to lambda calculus, for example the Master Theorem is formulated in terms of a recurrence relation. Also while I don't think there is a polynomial difference between RAM and LC, there is very much a polynomial difference between RAM and TM.</p>\n<p>That being said, as I mentioned earlier with classes there is lots of room for multiple models (and if I can prove a linear relationship from RAM back to LC, then I'd get any RAM theorem's \"for free\"). Personally, I can vaguely see the path from where I am now to showing something like the Master theorem in UTLC, because of reasons like composability I don't see a similar path for RAM. But that doesn't mean you shouldn't try.</p>",
        "id": 319101707,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672698540
    },
    {
        "content": "<p>As far as I know a single beta reduction can have a quadratic overhead ( size of the lambda abstraction x size of the substituted term) and reduction to a normal form can have exponential overhead when translated to RAM. We probably need to adopt highly specific kinds of reduction rules to avoid the exponential time and space complexity blow up. This makes it a highly unnatural way to work with for theorists.</p>",
        "id": 319105130,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672701285
    },
    {
        "content": "<p>And these overheads completely ignore other complexity measures like space complexity (or sample complexity for learning algorithms)</p>",
        "id": 319105186,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672701328
    },
    {
        "content": "<p>The formulation needs to remain intuitive to people in the field. Recursive algorithms are an important subset of algorithms, but even there, you would run into issues where destructive updates and non destructive updates are simultaneously used, each having different complexities. All in just one model. Now if you ask theorists to learn PL methods to encode such ideas in LC, that is a sure shot way to lose their interest. The typical STOC/FOCS/SODA author does not care about functional programming language algorithms for a very good reason.</p>",
        "id": 319105449,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672701569
    },
    {
        "content": "<p>They are a very small niche in the field.</p>",
        "id": 319105465,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672701595
    },
    {
        "content": "<p>Expecting CS theorists to encode their work in utlc style would be akin to asking mathematicians to stick to constructive proofs</p>",
        "id": 319105600,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672701710
    },
    {
        "content": "<p>Perhaps there will come a time when substantial numbers of theorists take an interest in solving algorithmic problems efficiently in functional programming (something like an entire session in the top conferences for a few years in a row), but even then the vast majority of existing results will assume RAM or some such model.</p>",
        "id": 319105806,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672701853
    },
    {
        "content": "<p>OTOH, perhaps this UTLC model could be included among others in mathlib, as a way to encourage exploration</p>",
        "id": 319105972,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672702003
    },
    {
        "content": "<p>I would argue it might be somewhat akin to asking mathematicians to encode their theorems in dependent type theory. I think there could be a suitable stable of theorems such that anyone wouldn't even be aware or have to interact with the underlying model, in the same way that you could get reasonably far without understanding dependent type theory (certainly I'm confused about the casting I have to do).</p>\n<p>That being said, I understand a path forward for UTLC, I don't understand a path forward for RAM. If you know or can point to the equivalent transformations (f.x. f → Λ Λ ↓0·f·↓1 for iterate) for RAM I'm always happy to learn. But I don't really know lean (I ran through the tutorials in November) and I'm a software engineer by trade (I suppose ironically my C++ is orders of magnitude better than my Haskell, so if I could figure out a path forward for the RAM model I'd be more than happy to switch), so in terms of tangible results its either UTLC or nothing. But I'm also happy to collaborate on a multi-model environment if you understand the path forward for the RAM model.</p>",
        "id": 319106584,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672702496
    },
    {
        "content": "<p>I'm reminded of a comment one of my CS professors made about my infatuation with provable programming languages, \"the problem is that you have to actually prove to the computer you know what you are doing\".</p>",
        "id": 319106736,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672702591
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"562941\">Andrew Carter</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319106584\">said</a>:</p>\n<blockquote>\n<p>I would argue it might be somewhat akin to asking mathematicians to encode their theorems in dependent type theory. I think there could be a suitable stable of theorems such that anyone wouldn't even be aware or have to interact with the underlying model, in the same way that you could get reasonably far without understanding dependent type theory (certainly I'm confused about the casting I have to do).</p>\n<p>That being said, I understand a path forward for UTLC, I don't understand a path forward for RAM. If you know or can point to the equivalent transformations (f.x. f → Λ Λ ↓0·f·↓1 for iterate) for RAM I'm always happy to learn. But I don't really know lean (I ran through the tutorials in November) and I'm a software engineer by trade (I suppose ironically my C++ is orders of magnitude better than my Haskell, so if I could figure out a path forward for the RAM model I'd be more than happy to switch), so in terms of tangible results its either UTLC or nothing. But I'm also happy to collaborate on a multi-model environment if you understand the path forward for the RAM model.</p>\n</blockquote>\n<p>The path is as I described previously. Since we are talking about RAM, the projection operations would happen one after another and their complexities would be added up</p>",
        "id": 319107729,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672703523
    },
    {
        "content": "<p>Proving the correctness of imperative descriptions of algorithms is reasonably well understood. This is different from proving their implementation in an actual language. As a haskeller you might have used Algebraic Data Types to encode grammars for small languages. We can do something similar in lean. A toy imperative language that allows us to introduce variables, modify them, conditional statements, loop constructs, procedure definitions, and procedure calls. Then for RAM, we identify types and operations with unit cost, and compose the complexity of operations for the rest of the language</p>",
        "id": 319108024,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672703865
    },
    {
        "content": "<p>We can probably make it more intuitive with clever uses of notations and macros</p>",
        "id": 319108068,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672703898
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 319108092,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672703937
    },
    {
        "content": "<p>One way to circumvent the need to construct data values explicitly in the toy language, is to define a cost function for data types and value constructors instead.</p>",
        "id": 319108460,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672704254
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319107729\">said</a>:</p>\n<p>The path is as I described previously. Since we are talking about RAM, the projection operations would happen one after another and their complexities would be added up</p>\n<p>I think what is needed here is a function of type (RAM prog) → (RAM prog) → (RAM prog), \"running them one after another\" is actually already handwavy, since running the first program is going to pollute the memory of the next program (and the next program might smash the results of the first one). Even more so if you want to consider space complexity (what can be saved etc.).</p>",
        "id": 319108819,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672704624
    },
    {
        "content": "<p>Theorists, with the honourable exception of data structure designers can be hand wavy about that. But a safe way to account for space complexity in this case is to add explicit <code>modify</code> and <code>destroy</code> functions to data types, with the corresponding cost functions tagged. So you need explicit constructors and destructors and cost functions which compute current as well as maximum overall resource usage, as they traverse the program</p>",
        "id": 319109225,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672704984
    },
    {
        "content": "<p>Space complexity conscious theorists often need to specify where memory gets reused in any case. Adding destructors makes it explicit</p>",
        "id": 319109266,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672705059
    },
    {
        "content": "<p>I totally do expect sequential composition of that form, with one caveat. For a space complexity conscious definition of RAM, additional state in the form of variables names in memory with their storage cost annotation will have to be carried around.</p>",
        "id": 319110327,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672705722
    },
    {
        "content": "<p>For most theoretical algorithms however, we don't have to deal with such updates. These are implementation details. Typically, any given data structure has certain methods to access and update it (with the respective costs). Theorists are happy to operate on data at this level of abstraction</p>",
        "id": 319111091,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672706072
    },
    {
        "content": "<p>For the data structure folks who are trying to save every last bit of storage (succinct data structures for instance), we will need to use a modified version of <code>prog</code> which is annotated with some information about it's memory usage at each statement and checks for permitted memory accesses.</p>",
        "id": 319111560,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672706297
    },
    {
        "content": "<p>Starting with ram model time complexity and TM related theorems  is already a non-trivial task.</p>",
        "id": 319111721,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672706386
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319111721\">said</a>:</p>\n<blockquote>\n<p>Starting with ram model time complexity and TM related theorems  is already a non-trivial task.</p>\n</blockquote>\n<p>Which is why I started with utlc (which is itself non-trivial, but slightly more trivial in my opinion at least). But yeah, computers don't really do hand wavy.</p>",
        "id": 319113129,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672707453
    },
    {
        "content": "<p>There is a <a href=\"https://drops.dagstuhl.de/opus/volltexte/2021/13915/pdf/LIPIcs-ITP-2021-20.pdf\">paper</a> (discussed before on this Zulip) that demonstrates mechanization in Coq using lambda calculus, but it seems the semantics (in terms of TM's) is nontrivial.</p>\n<p>Note that ultimately, we want to prove interesting things about our computational model, so while we can make the model more and more complex, this will make showing the existence of a complete problem more and more difficult. Thus, it seems inevitable that at some point, you will need to get to some \"lower level\" (Such as TM's, circuits) even if you start at a high level language such as lambda calculus.</p>\n<p>By the way, I would like to mention that I also have a repo that has made some nontrivial progress towards defining polynomial time <a href=\"https://github.com/prakol16/lean_complexity_theory_polytime_trees\">here</a>. I have a new version that is making progress towards Cook-Levin (the key idea is to use circuits as the low level language, so that we get CIRCUIT-SAT as our NP-hard problem). I will say more about this soon-ish hopefully as soon as I clean up some of the code.</p>",
        "id": 319117631,
        "sender_full_name": "Praneeth Kolichala",
        "timestamp": 1672711787
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"437861\">@Praneeth Kolichala</span> , it would be nice if you could attend Martin's <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/318236300\">meeting</a> as well, I would like to make sure your plans are represented there</p>",
        "id": 319119229,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672713469
    },
    {
        "content": "<p>oops, I see now you already responded</p>",
        "id": 319119307,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1672713521
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"437861\">Praneeth Kolichala</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319117631\">said</a>:</p>\n<blockquote>\n<p>Note that ultimately, we want to prove interesting things about our computational model, so while we can make the model more and more complex, this will make showing the existence of a complete problem more and more difficult. </p>\n</blockquote>\n<p>To me, it seems to be a clear reason to not make the model more and more complex. Instead, we should make the API more and more rich.</p>",
        "id": 319157085,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672739233
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"562941\">Andrew Carter</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319108819\">said</a>:</p>\n<blockquote>\n<p>I think what is needed here is a function of type (RAM prog) → (RAM prog) → (RAM prog), \"running them one after another\" is actually already handwavy (...)</p>\n</blockquote>\n<p>Hitting the nail on the head!</p>",
        "id": 319158958,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672739995
    },
    {
        "content": "<p>We also need to make sure we are not doing something so niche that theorists in these fields don't care about it. An LC-based model is a highly esoteric choice in this sense, with highly non-trivial, to unacceptable translation overheads to other models.</p>",
        "id": 319165310,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672742252
    },
    {
        "content": "<p>FYI, <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> in his paper \"Formalizing computability theory via partial recursive functions\" writes:<br>\n<a href=\"/user_uploads/3121/hAXmtge3WktDd2p2v4D83lUj/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/hAXmtge3WktDd2p2v4D83lUj/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/hAXmtge3WktDd2p2v4D83lUj/image.png\"></a></div>",
        "id": 319219836,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672759882
    },
    {
        "content": "<p><a href=\"/user_uploads/3121/I0OnpRbhSrMkmU-Yr9VL1xRi/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/I0OnpRbhSrMkmU-Yr9VL1xRi/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/I0OnpRbhSrMkmU-Yr9VL1xRi/image.png\"></a></div>",
        "id": 319233909,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672763917
    },
    {
        "content": "<p>We don't have to reduce everything to TMs either</p>",
        "id": 319235676,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672764471
    },
    {
        "content": "<p>But for basic complexity theory TMs are unavoidable</p>",
        "id": 319235949,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672764549
    },
    {
        "content": "<p>They key would be start with TMs, prove equivalence to RAM and translations for complexity classes etc. And define classes for RAM from this. Beyond that point, stay in RAM</p>",
        "id": 319236177,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672764613
    },
    {
        "content": "<p>The one model rules all approach will never work in algorithms and complexity</p>",
        "id": 319238198,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672765248
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319238198\">said</a>:</p>\n<blockquote>\n<p>The one model rules all approach will never work in algorithms and complexity</p>\n</blockquote>\n<p>I agree, and a couple of more thoughts,<br>\n(1) I think if you can prove that a model supports the arrow laws within a certain amount of time/space, probably the same overall machinery can be reused for each model. I think I'm getting close to to the point where I can intuit the cost function either from a monad or maybe from pure lean directly. For example the proof for nat.mul is super mechanical except for (a) the choice of function decomposition (apply *_complexity_le) (b) proof the function is the same as nat.mul (lines 122-129) and (c) proofs that the recursive bounds are actually correct (lines 106 -11)<br>\n<a href=\"https://github.com/calcu16/lean_complexity/blob/e3992afdfca1dcdec2b7a37528d50660765c5470/src/lambda_calculus/utlc/complexity/nat.lean\">https://github.com/calcu16/lean_complexity/blob/e3992afdfca1dcdec2b7a37528d50660765c5470/src/lambda_calculus/utlc/complexity/nat.lean</a><br>\n(2) This means that machinery can be built out with any model in parallel with alternative models</p>\n<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/217875-Is-there-code-for-X.3F/topic/Complexity.20theory/near/319157085\">said</a>:</p>\n<blockquote>\n<p>To me, it seems to be a clear reason to not make the model more and more complex. Instead, we should make the API more and more rich.</p>\n</blockquote>\n<p>(3) I'm wondering if its the opposite at least in regards to the RAM model, RAM is essential assembly and assembly doesn't have an explicit notion of things like data structures (you can encode them of course), perhaps if we had a jvm model (or rust model - which tracks sharing better) it would be both more usable and easier to make composability arguments about.</p>",
        "id": 319515790,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1672894685
    }
]