[
    {
        "content": "<p>I thought it might make sense to start looking into possibly optimizing mathlib4 compile times now that they are non-negligible. My first observation though is an unexpected per-file overhead independent of the file size: <a href=\"/user_uploads/3121/JPibManQAImvRtGxdgPMANoV/image.png\">image.png</a> <br>\nThis slice takes about 140ms, though it might take longer for later files. Because what I think is happening in at least the first half is that we have the interpreter run an <code>initialize addLinter ...</code> block (from std4?)... and since a linter is a lambda and a lambda from the interpreter must contain certain metadata, the Lean object <code>mark_mt</code> is spending all that time on presumably is the entire <code>Environment</code>.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/JPibManQAImvRtGxdgPMANoV/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/JPibManQAImvRtGxdgPMANoV/image.png\"></a></div>",
        "id": 320441738,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1673354660
    },
    {
        "content": "<p>It's harder to tell what's happening in the second part, but it certainly looks very similar. Unfortunate that we would go through the effort of upgrading the Environment to MT and then immediately persistent afterwards</p>",
        "id": 320442562,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1673354943
    },
    {
        "content": "<p>Precompilation obviously would avoid this, as would storing linters in an (additional) attribute for the first part. I don't see a way to optimize this in the interpreter in general.</p>",
        "id": 320445999,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1673355981
    },
    {
        "content": "<p>Thanks for looking into this, indeed it seems like there should now be enough data for some optimization.</p>",
        "id": 320527872,
        "sender_full_name": "Heather Macbeth",
        "timestamp": 1673378718
    },
    {
        "content": "<p>Pardon the beginner question but how do you read this graph? Is time the y-axis (going down)? What is the x-axis?</p>",
        "id": 320530817,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1673379686
    },
    {
        "content": "<p>This is a \"flame graph.\" The width of each box is proportional to how long a particular function took, and stacked on top of it are the functions that that function itself called. This way, you can see how much time during a function's execution is attributed to waiting for other functions to complete.</p>",
        "id": 320531855,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1673380034
    },
    {
        "content": "<p>I <em>think</em> that this one is showing aggregate data. There are also versions where you see every single function call through time as a history of program execution.</p>",
        "id": 320532228,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1673380153
    },
    {
        "content": "<p>I think it's not aggregated actually... but Sebastian is the only one who can say there haha</p>",
        "id": 320572437,
        "sender_full_name": "James Gallicchio",
        "timestamp": 1673397881
    },
    {
        "content": "<p>It might be (it does say so at the bottom), but I'm not sure to be honest. This is Hotspot.</p>",
        "id": 320663332,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1673432805
    }
]