[
    {
        "content": "<p>This stuff is amazing. Jesse showed me a <code>gpt</code>-generated proof of a lemma in the Witt vector project. It's not extremely deep, but it's certainly not a <code>refl</code> lemma either. I'm impressed.</p>\n<p>One challenge for ATP's (I think Brian Conrad said this at some point) is: can they prove <code>infinitude_of_primes</code> without human input.<br>\nHow does <code>gpt</code> fair on that one? You have this tricky step where you have to cook up a prime number that is sufficiently big, and you basically have to come up with the <code>n! + 1</code> trick out of thin air. Does it go anywhere?</p>\n<p>And if you give it the <code>p := min_fac (n! + 1)</code>. Can it take it from there?</p>",
        "id": 221753066,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1609929448
    },
    {
        "content": "<p>With the data it was trained on it is highly unlikely, even with a lot of search I think. But eventually, the hope is that as we build up <em>a lot</em> of synthetic data by sampling proofs (with a fair amount of search) and training on that signal, the model will get a chance to build such intuitions. It also obviously depends on the type of statements we use for that.</p>",
        "id": 221758101,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1609933353
    },
    {
        "content": "<p>In the meantime, also I think there's a ton of value in having a system that can assist humans in a productive way, and what we have today is not far from it. If you really only have to provide these insights to formalize <code>infinitude_of_primes</code>, then it's probably going to make working with Lean much easier for a whole lot of people?</p>",
        "id": 221758180,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1609933435
    },
    {
        "content": "<p>Right, I think the more useful metric for the short term is not really \"how many top-level theorems does <code>gpt</code> prove\" but more like how much shorter can proofs become if you try to prove every goal using <code>gpt</code> (especially the goals created by <code>have</code>s, or side conditions to lemmas for <code>rw</code>, or <code>calc</code> steps, etc.)</p>",
        "id": 221758734,
        "sender_full_name": "Reid Barton",
        "timestamp": 1609933835
    },
    {
        "content": "<p>Sure, I definitely think that this can be very useful before we get <code>infinitude_of_primes_without_human_input</code>.</p>",
        "id": 221759243,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1609934230
    },
    {
        "content": "<p>But I also think it's a nice benchmark. It will certainly turn some heads.</p>",
        "id": 221759272,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1609934263
    },
    {
        "content": "<p>Agreed <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 221759429,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1609934392
    },
    {
        "content": "<p>when we say shorter proofs here, I took this to mean actual length of the text of the proof, and not e.g. size of the terms evaluated, correct?</p>",
        "id": 221762239,
        "sender_full_name": "matt rice",
        "timestamp": 1609936709
    },
    {
        "content": "<p>To put infinitude of primes in context, I think that theorem would have gotten Euclid (or whoever actually came up with it) the Fields medal of the time.  Until that point (in my revisionist history) the way one proves that a set is infinite is to find a generator for the set.  This instead is a proof where one shows that if there is a last element then a contradiction happens if you pick a very creative number.  The thing that is going to be hardest to teach AI is to come up with creative solutions to problems.  It might figure out (or have built in) proof-by-contradiction and that if a set isn't infinite then there is a finite set of numbers that whose product is a number.  But to know that it needs to take that product and subtract one.  That is human reasoning at its finest.  (Also, most humans mathematicians are taught that proof, so it is unclear how many of them would be able to come up with it on their own.)</p>",
        "id": 221770431,
        "sender_full_name": "Jason Rute",
        "timestamp": 1609942119
    },
    {
        "content": "<p>I very much agree with your final observation. <span aria-label=\"thumbs up\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"thumbs up\">:thumbs_up:</span></p>",
        "id": 221770604,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1609942227
    },
    {
        "content": "<p>The problem of course is that mathematicians are taught it and they think it's brilliant but 5 years later they think it's trivial and have forgotten the brilliance of the idea; by that point it seems difficult to persuade them that actually is difficult.</p>",
        "id": 221796119,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1609954144
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span> During Jason's talk, you asked about how <code>gpt</code> performs on <code>analysis</code> proofs --- indeed, analysis was the largest module (23K LOC) with the largest disparity in favor of the Transformer models from our evaluation. Here are the <code>analysis</code> proofs it found. We get some repeated tactic applications because the model is not conditioned on the previous tactic states, only the current one, but many of the proofs are nontrivial and diverge from their <code>mathlib</code> counterparts. </p>\n<p><a href=\"/user_uploads/3121/a_x3XSKBLrunFzbAfmo2mLEZ/gpt_analysis.txt\">gpt_analysis.txt</a> </p>\n<p>some samples:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">lemma</span> <span class=\"n\">norm_le_zero_iff</span> <span class=\"o\">:</span> <span class=\"bp\">∀</span> <span class=\"o\">{</span><span class=\"n\">α</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_1</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">_inst_1</span> <span class=\"o\">:</span> <span class=\"n\">normed_group</span> <span class=\"n\">α</span><span class=\"o\">]</span> <span class=\"o\">{</span><span class=\"n\">g</span> <span class=\"o\">:</span> <span class=\"n\">α</span><span class=\"o\">},</span> <span class=\"bp\">∥</span><span class=\"n\">g</span><span class=\"bp\">∥</span> <span class=\"bp\">≤</span> <span class=\"mi\">0</span> <span class=\"bp\">↔</span> <span class=\"n\">g</span> <span class=\"bp\">=</span> <span class=\"mi\">0</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n   <span class=\"n\">intros</span><span class=\"o\">,</span>\n   <span class=\"n\">simpa</span> <span class=\"o\">[</span><span class=\"n\">le_antisymm_iff</span><span class=\"o\">,</span> <span class=\"n\">norm_nonneg</span><span class=\"o\">]</span> <span class=\"n\">using</span> <span class=\"bp\">@</span><span class=\"n\">norm_eq_zero</span> <span class=\"n\">α</span> <span class=\"n\">_</span> <span class=\"n\">g</span><span class=\"o\">,</span>\n<span class=\"kd\">end</span>\n\n<span class=\"kd\">lemma</span> <span class=\"n\">asymptotics.is_O_with.add</span> <span class=\"o\">:</span> <span class=\"bp\">∀</span> <span class=\"o\">{</span><span class=\"n\">α</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_1</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">F</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_4</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">E'</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_6</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">_inst_2</span> <span class=\"o\">:</span> <span class=\"n\">has_norm</span> <span class=\"n\">F</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">_inst_4</span> <span class=\"o\">:</span> <span class=\"n\">normed_group</span> <span class=\"n\">E'</span><span class=\"o\">]</span> <span class=\"o\">{</span><span class=\"n\">g</span> <span class=\"o\">:</span> <span class=\"n\">α</span> <span class=\"bp\">→</span> <span class=\"n\">F</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">l</span> <span class=\"o\">:</span> <span class=\"n\">filter</span> <span class=\"n\">α</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">c₁</span> <span class=\"n\">c₂</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">f₁</span> <span class=\"n\">f₂</span> <span class=\"o\">:</span> <span class=\"n\">α</span> <span class=\"bp\">→</span> <span class=\"n\">E'</span><span class=\"o\">},</span> <span class=\"n\">is_O_with</span> <span class=\"n\">c₁</span> <span class=\"n\">f₁</span> <span class=\"n\">g</span> <span class=\"n\">l</span> <span class=\"bp\">→</span> <span class=\"n\">is_O_with</span> <span class=\"n\">c₂</span> <span class=\"n\">f₂</span> <span class=\"n\">g</span> <span class=\"n\">l</span> <span class=\"bp\">→</span> <span class=\"n\">is_O_with</span> <span class=\"o\">(</span><span class=\"n\">c₁</span> <span class=\"bp\">+</span> <span class=\"n\">c₂</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"o\">:</span> <span class=\"n\">α</span><span class=\"o\">),</span> <span class=\"n\">f₁</span> <span class=\"n\">x</span> <span class=\"bp\">+</span> <span class=\"n\">f₂</span> <span class=\"n\">x</span><span class=\"o\">)</span> <span class=\"n\">g</span> <span class=\"n\">l</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n   <span class=\"n\">intros</span><span class=\"o\">,</span>\n   <span class=\"n\">filter_upwards</span> <span class=\"o\">[</span><span class=\"n\">h₁</span><span class=\"o\">,</span> <span class=\"n\">h₂</span><span class=\"o\">],</span>\n   <span class=\"n\">intros</span> <span class=\"n\">x</span> <span class=\"n\">hx₁</span> <span class=\"n\">hx₂</span><span class=\"o\">,</span>\n   <span class=\"n\">apply</span> <span class=\"n\">le_trans</span> <span class=\"o\">(</span><span class=\"n\">norm_add_le</span> <span class=\"n\">_</span> <span class=\"n\">_</span><span class=\"o\">),</span>\n   <span class=\"n\">apply</span> <span class=\"n\">le_trans</span> <span class=\"o\">(</span><span class=\"n\">add_le_add</span> <span class=\"n\">hx₁</span> <span class=\"n\">hx₂</span><span class=\"o\">),</span>\n   <span class=\"n\">rw</span> <span class=\"n\">add_mul</span><span class=\"o\">,</span>\n<span class=\"kd\">end</span>\n\n<span class=\"kd\">lemma</span> <span class=\"n\">differentiable_on_const_add_iff</span> <span class=\"o\">:</span> <span class=\"bp\">∀</span> <span class=\"o\">{</span><span class=\"bp\">𝕜</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_1</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">_inst_1</span> <span class=\"o\">:</span> <span class=\"n\">nondiscrete_normed_field</span> <span class=\"bp\">𝕜</span><span class=\"o\">]</span> <span class=\"o\">{</span><span class=\"n\">E</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_2</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">_inst_2</span> <span class=\"o\">:</span> <span class=\"n\">normed_group</span> <span class=\"n\">E</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">_inst_3</span> <span class=\"o\">:</span> <span class=\"n\">normed_space</span> <span class=\"bp\">𝕜</span> <span class=\"n\">E</span><span class=\"o\">]</span> <span class=\"o\">{</span><span class=\"n\">F</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u_3</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">_inst_4</span> <span class=\"o\">:</span> <span class=\"n\">normed_group</span> <span class=\"n\">F</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">_inst_5</span> <span class=\"o\">:</span> <span class=\"n\">normed_space</span> <span class=\"bp\">𝕜</span> <span class=\"n\">F</span><span class=\"o\">]</span> <span class=\"o\">{</span><span class=\"n\">f</span> <span class=\"o\">:</span> <span class=\"n\">E</span> <span class=\"bp\">→</span> <span class=\"n\">F</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">s</span> <span class=\"o\">:</span> <span class=\"n\">set</span> <span class=\"n\">E</span><span class=\"o\">}</span> <span class=\"o\">(</span><span class=\"n\">c</span> <span class=\"o\">:</span> <span class=\"n\">F</span><span class=\"o\">),</span> <span class=\"n\">differentiable_on</span> <span class=\"bp\">𝕜</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"o\">(</span><span class=\"n\">y</span> <span class=\"o\">:</span> <span class=\"n\">E</span><span class=\"o\">),</span> <span class=\"n\">c</span> <span class=\"bp\">+</span> <span class=\"n\">f</span> <span class=\"n\">y</span><span class=\"o\">)</span> <span class=\"n\">s</span> <span class=\"bp\">↔</span> <span class=\"n\">differentiable_on</span> <span class=\"bp\">𝕜</span> <span class=\"n\">f</span> <span class=\"n\">s</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n   <span class=\"n\">intros</span> <span class=\"n\">x</span> <span class=\"n\">hx</span><span class=\"o\">,</span>\n   <span class=\"n\">intros</span> <span class=\"n\">E</span> <span class=\"n\">unique_diff_on_univ</span><span class=\"o\">,</span>\n   <span class=\"n\">intros</span><span class=\"o\">,</span>\n   <span class=\"n\">simp</span> <span class=\"o\">[</span><span class=\"n\">differentiable_on</span><span class=\"o\">,</span> <span class=\"n\">differentiable_within_at_univ</span><span class=\"o\">],</span>\n<span class=\"kd\">end</span>\n</code></pre></div>",
        "id": 221971207,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1610039849
    },
    {
        "content": "<p>Fun observation: in the first example above, it used <code>@norm_eq_zero</code> with the correct number of arguments (and even the \"dreadful <code>@ _</code> idiom\"), but the string <code>@norm_eq_zero</code> never appears in <code>mathlib</code> (commit <code>33483a3d</code>).</p>",
        "id": 221972620,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1610040499
    },
    {
        "content": "<p>Thanks for this information! I am excited about the potential for gpt for interactive use. I know when/how to use intro and cases, so, for me, the real win is when we have a tool that suggests theorems and rewrite rules. So here are some useful queries for the model:</p>\n<ul>\n<li>If I were to use simp or rw right now, what additional rewrite rules do you think I should add?</li>\n<li>If I were to use apply or refine or exact within the next three or four steps, what is your best guess as to the theorem I would use?</li>\n</ul>",
        "id": 221975770,
        "sender_full_name": "Jeremy Avigad",
        "timestamp": 1610041744
    },
    {
        "content": "<p>Yes, this is was also my remark during the talk.</p>",
        "id": 221975946,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1610041821
    },
    {
        "content": "<p>I guess another useful query:</p>\n<ul>\n<li>If I were to use <code>cases</code> or <code>rcases</code> on something other than a hypothesis, what would it be?</li>\n</ul>",
        "id": 221976425,
        "sender_full_name": "Jeremy Avigad",
        "timestamp": 1610042041
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span>. Anything that can be prompted by a prefix of the full completion will work as expected. So as an example we can create a specialized tactic <code>gptf-rw</code> that will \"force\" the use of <code>rw</code> as a tactic and the model will gadly complete the steps. For <code>cases</code> or <code>rcases</code> we can do the same. There is no trivial way to specify (do not use the hypotheses) but we can query the model and filter out the response to achieve that goal.</p>",
        "id": 221978483,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610042859
    },
    {
        "content": "<p>Does that make sense?</p>",
        "id": 221978501,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610042866
    },
    {
        "content": "<p>This is great feedback (thanks <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span> and <span class=\"user-mention\" data-user-id=\"110031\">@Patrick Massot</span>) we commit to working on exactly that in the coming weeks and will circle back once we have something working.</p>",
        "id": 221978611,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610042898
    },
    {
        "content": "<p>The main blocker right now is one commit to the lean repository, after that we'll be able to share the system with a broader set of people very easily cc <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span></p>",
        "id": 221978687,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610042941
    },
    {
        "content": "<p>is there a PR you're referring to?</p>",
        "id": 221979155,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610043126
    },
    {
        "content": "<p>Yes, Jesse's lean-tpe branch here: <a href=\"https://github.com/jesse-michael-han/lean/tree/lean-tpe\">https://github.com/jesse-michael-han/lean/tree/lean-tpe</a></p>",
        "id": 221979296,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610043192
    },
    {
        "content": "<p>But it needs clean-up I believe :)</p>",
        "id": 221979321,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610043205
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"238605\">Kevin Lacker</span> <a href=\"#narrow/stream/270209-Lean-Together.202021/topic/LeanStep.20(Jason.20.2B.20Jesse's.20talk)/near/221979155\">said</a>:</p>\n<blockquote>\n<p>is there a PR you're referring to?</p>\n</blockquote>\n<p>all the changes have been incorporated into <a href=\"https://github.com/leanprover-community/lean/pull/510\">this PR</a> by <span class=\"user-mention\" data-user-id=\"121918\">@Edward Ayers</span></p>",
        "id": 222087717,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1610121047
    },
    {
        "content": "<p>Was late in watching this talk. Am I right in saying that the gpt tactic is just producing a string and doesn't by design know how to produce a string that is syntactically correct, so if it does produce meaningful strings it's because it \"learnt\" to do that and not because it's been programmed to do so?</p>",
        "id": 222184727,
        "sender_full_name": "Chris Hughes",
        "timestamp": 1610219547
    },
    {
        "content": "<p>Yes that is correct.  It “learnt” in the sense of machine learning what a valid tactic string looks like. It uses a string-to-string machine learning algorithm called a Transformer that is often used for natural language where there is no formal grammar.</p>",
        "id": 222185142,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610220154
    },
    {
        "content": "<p>If you are curious, both of these threads have some discussion on transformers, including the GPT transformer. <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Papers.20on.20Neural.20Conjecturing\">#Machine Learning for Theorem Proving &gt; Papers on Neural Conjecturing</a> and <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper\">#Machine Learning for Theorem Proving &gt; GPT-f paper</a>.</p>",
        "id": 222185372,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610220513
    },
    {
        "content": "<p>in a very fuzzy nonrigorous way, I suspect that a lot of the \"power\" of the gpt is spent on learning the lean syntax, and if there were some way to integrate a tree search that knew about valid lean syntax with an AI model doing AI things, it would be more effective</p>",
        "id": 222269108,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610352933
    },
    {
        "content": "<p>like you can teach a gpt to play chess but it isn't as good as the monte carlo based ones</p>",
        "id": 222269131,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610352957
    },
    {
        "content": "<p>If you would print chess moves in regular chess notation, one move per line. Is it possible to \"measure\" how much \"power\" gpt spends on figuring out the syntax?</p>",
        "id": 222269823,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1610353463
    },
    {
        "content": "<p>FYI - I have not used anything from this site but do keep it on reference for future needs. The site <a href=\"https://huggingface.co/\">https://huggingface.co/</a> is about solving NLP. They have models using GPT. I found one when searching for chess: <a href=\"https://huggingface.co/shtoshni/gpt2-chess-uci\">https://huggingface.co/shtoshni/gpt2-chess-uci</a> . Take all of this with a grain of salt.</p>",
        "id": 222279277,
        "sender_full_name": "EricGT",
        "timestamp": 1610360350
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"238605\">@Kevin Lacker</span> in the current result we don't have any tree-search as explained during the talk. But we're working on adding that which should give an healthy boost. Only the valid tactics would be kept as is the case in the original GPT-f paper. Interestingly, in AlphaGo MCTS was almost exclusively used to train the value function (the policy network was probably not even needed as the \"valid\" action space in each board state is fairly limited). In formal maths we have the increased difficulty that the action space even limited to \"valid\" steps is enumerable, so we need a strong enough policy to productively expand each node. But same as is the case for games, lot of the \"proving power\" will probably come from a trained value function, which does not even exist yet in the Lean setup.</p>",
        "id": 222294692,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610370704
    },
    {
        "content": "<p>All in all, completely agreed with your statement, just wanted to provide a bit of additional context.</p>",
        "id": 222294715,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1610370721
    },
    {
        "content": "<p>how would you train a value function? the value is something like, the value for lemma X is an estimate of whether proving lemma X would be helpful in concluding theorem Y?</p>",
        "id": 222325583,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610384535
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"238605\">Kevin Lacker</span> <a href=\"#narrow/stream/270209-Lean-Together.202021/topic/LeanStep.20(Jason.20.2B.20Jesse's.20talk)/near/222325583\">said</a>:</p>\n<blockquote>\n<p>how would you train a value function? the value is something like, the value for lemma X is an estimate of whether proving lemma X would be helpful in concluding theorem Y?</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"238605\">@Kevin Lacker</span> You should check out the GPT-f paper, or the discussion at <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT-f.20paper\">#Machine Learning for Theorem Proving &gt; GPT-f paper</a>.  But some simple approach (not too different from AlphaGo is to train a binary classifier to say if the model's tree search is able to find a proof at a particular goal state which came up during some sort of iterative training (expert iteration or RL).  (For transformer models you can use tricks for binary classification as done in the GPT-f paper.)</p>",
        "id": 222326003,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610384718
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/270209-Lean-Together.202021/topic/LeanStep.20(Jason.20.2B.20Jesse's.20talk)/near/222269823\">said</a>:</p>\n<blockquote>\n<p>If you would print chess moves in regular chess notation, one move per line. Is it possible to \"measure\" how much \"power\" gpt spends on figuring out the syntax?</p>\n</blockquote>\n<p>I only tried playing chess against GPT-3, so I don't have any fancy analysis, but \"syntax\" was probably the wrong thing for me to say. GPT-3 playing chess will almost always say moves that are the correct syntax, but sometimes it will make illegal moves, like it will try to move its pawn from e2 to e4 but it already moved that pawn earlier and it \"forgot\". I am sure you could make a better one by training solely on chess games but imo this sort of failure hints that the GPT architecture is doing some work to detect legal moves that could just be done by hard-coding them</p>",
        "id": 222326192,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610384794
    },
    {
        "content": "<p>Thanks for the anecdote</p>",
        "id": 222326874,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1610385096
    },
    {
        "content": "<p>As for the syntax discussion, my thinking on this has evolved over time.  It is certainly true that a model could be given what is called an \"inductive bias\" forcing it to give correct syntax.  The HOList models have a stronger inductive bias for example.  Also, even with transformer models there are ways to supply tree position encodings rather than text position encoding However there are some considerations:</p>\n<ol>\n<li>It is a lot easier to use a text model like a transformer, since one doesn't have to engineering what formal syntax looks like.  For example, if our tool we presented last week used the HOList technology, it would be a lot more engineering to make a DSL for tactics (and even more so for Lean since Lean needs term and name parameters way more than HOL Light tactics, and HOList doesn't handle those well).</li>\n<li>There is an economy of scale thing going on.  OpenAI and other companies (like HuggingFace) have a lot of infrastructure around pertained text models since (1) they are easy to pre-train (you basically dump a large part of the internet into the model) and (2) they are extremely flexible.  This makes it easy to create a new model for some esoteric purpose like theorem proving or symbolic mathematics.  Someone could take time to gather lots of tree structured data for a tree position transformer, but this would take a lot of curation work.  (Also, it would be a lot of work to parse lean text into a syntax tree, especially with Lean's change-as-you-go parser.)</li>\n<li>It provides flexibility.  We are not using low level formal lean syntax, but high level pretty printed and human written syntax.  This is shorter and easier to fit into our model.  This is easy with a text transformer since it doesn't need to know the exact syntax used.</li>\n<li>Transformers can also learn unspoken syntax rules, such as good variable names and common ways to write code.  The output looks more human like than if it just outputted an AST.</li>\n</ol>",
        "id": 222328673,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610385896
    },
    {
        "content": "<p>As for chess, note, this sort of unbiased approach is also used for chess with MuZero.  (They use conv nets instead of transformers, but the idea is similar.)  One doesn't need to program in the rules of chess to the model.  Instead the model observes human games enough to learn the action space, and then improves its understanding via RL.  It is interesting that illegal moves and really dumb moves basically look the same to the model.  I think in that paper, they argue this even helps learning in the long run.</p>",
        "id": 222328910,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610385983
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"238605\">Kevin Lacker</span> <a href=\"#narrow/stream/270209-Lean-Together.202021/topic/LeanStep.20(Jason.20.2B.20Jesse's.20talk)/near/222269108\">said</a>:</p>\n<blockquote>\n<p>in a very fuzzy nonrigorous way, I suspect that a lot of the \"power\" of the gpt is spent on learning the lean syntax, and if there were some way to integrate a tree search that knew about valid lean syntax with an AI model doing AI things, it would be more effective</p>\n</blockquote>\n<p>It's well known that transformers will tend to produce syntactically correct outputs if trained sufficiently, to the point that structured decoding is not really needed.</p>\n<p>When it comes to neural theorem proving using <code>gptf</code>, there are two search procedures: a search for a candidate sequence of tokens by the decoding procedure wrapping the transformer, and a search for candidate proof by the tree search which wraps the decoding procedure wrapping the transformer. The outer search already knows about valid Lean tactics, but the inner search over sequences of tokens does not know about Lean syntax. One can certainly try making this inner search Lean-syntax-aware and having it mask out invalid tokens during decoding. This can improve the predictions returned by the inner search. In practice, however, the model usually predicts valid Lean syntax (interactive tactic invocations in <code>mathlib</code> tend to not have very complicated parse trees) and most failures occur in the outer search, when the tactic execution fails to make progress. Currently, I don't think we would see much benefit from such domain-specific engineering in the decoding process.</p>\n<p>If we were to ask much more of the transformer, say predicting entire <em>proof terms</em> instead of tactic steps, then we might see more benefits from structured decoding.</p>",
        "id": 222329316,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1610386190
    },
    {
        "content": "<p>MuZero does know about the tree structure of the chess game, though, it isn't just reading in a sequence of chess moves and putting it into a GPT</p>",
        "id": 222795973,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610664045
    },
    {
        "content": "<p>\"tree search\" isn't quite the right way to describe a theorem prover, though, a set of possible proofs really maps to a DAG rather than the way a set of possible chess games maps to a tree and that seems like a big difference</p>",
        "id": 222796145,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1610664159
    },
    {
        "content": "<p>Board games are also DAGs, but tree search is still the term usually used.</p>",
        "id": 222796410,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610664340
    },
    {
        "content": "<p>Lean GPT-f is also not currently outputting more than one tactic at a time, so it isn’t that different from outputting a chess move.  A complete proof, sequence of tactics comes from a tree search.</p>",
        "id": 222797054,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610664719
    },
    {
        "content": "<p>But to be clear, there are many differences between Muzero and GPT-f.  The only point I wanted to make is that technically muzero also (like GPT-f) doesn’t know what a valid move is a priori.  It has to be learned.  (One not small difference also is that muzero is constrained to output 64x64 possible moves (many not valid) while GPT-f can output any text string.)</p>",
        "id": 222798359,
        "sender_full_name": "Jason Rute",
        "timestamp": 1610665498
    }
]