[
    {
        "content": "<p>In digging through the Lean 4 source code and trying to lean from it, I have stumble upon the token cache. It caches the mostly recently parsed <code>Token</code> so that it can be reused again later (for performance reasons). It also used to do parser selection in the Pratt parser. This is described in detail in the sources here:  <a href=\"https://github.com/leanprover/lean4/blob/7ca2f70c2f977ca42c9df1acea86a9aa4e872928/src/Lean/Parser/Basic.lean#L34-L41\">https://github.com/leanprover/lean4/blob/7ca2f70c2f977ca42c9df1acea86a9aa4e872928/src/Lean/Parser/Basic.lean#L34-L41</a>.  </p>\n<p>In standard Lean, (from what I can tell), a token is one of the following primitives:</p>\n<ul>\n<li>A string literal (e.g. <code>\"hello!\\n\"</code>)</li>\n<li>A character literal (e.g., <code>'a'</code> or <code>'\\n'</code>)</li>\n<li>A numeric literal (e.g., <code>123</code>, <code>0b0101</code>, <code>0oAC</code>, or <code>0xAF</code>)</li>\n<li>A scientific literal (e.g., <code>1e4</code>)</li>\n<li>A name literal (e.g., <code> </code>foo <code>, </code> <code>«+» </code>)</li>\n<li>An identifier (e.g., <code>foo</code>, <code>«+»</code>)</li>\n<li>A user-defined custom symbol (e.g., <code>ℕ</code>)</li>\n</ul>\n<p>My question is: Why not just cache the head <code>Syntax</code> element (e.g., <code>atom</code>/<code>ident</code>/<code>node</code>) instead of having a separate notion of a <code>Token</code>? Alternatively, why not have <code>Token</code> be a category parser so it can be easily altered/extended?</p>\n<p>I imagine there are a performance and complexity reasons for this choice and I would like to learn what they are. Also I am curious whether or not having more fine grained control over tokens is something the broader Lean community would be interested in or if this a more niche desire.</p>",
        "id": 238844877,
        "sender_full_name": "Mac",
        "timestamp": 1621036785
    },
    {
        "content": "<p>From what I can tell from the sources, a token <em>is</em> a <code>Syntax</code></p>",
        "id": 238845231,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621037037
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> ????</p>\n<p>The type <code>Token</code> is an abbreviation for <code>String</code>: <a href=\"https://github.com/leanprover/lean4/blob/7ca2f70c2f977ca42c9df1acea86a9aa4e872928/src/Lean/Parser/Basic.lean#L94\">https://github.com/leanprover/lean4/blob/7ca2f70c2f977ca42c9df1acea86a9aa4e872928/src/Lean/Parser/Basic.lean#L94</a></p>\n<p>If you mean that tokens are parsed as a <code>Syntax</code> --- this is true. But only a subset of <code>Syntax</code> is a <code>Token</code> and that subset is hard coded into the way the parsers work, creating trouble when custom syntax clashes with it.</p>",
        "id": 238845818,
        "sender_full_name": "Mac",
        "timestamp": 1621037525
    },
    {
        "content": "<p>See one of my previous Zulip threads on <a href=\"#narrow/stream/270676-lean4/topic/Error.20with.20Digits.20in.20Custom.20Syntax/near/235404986\">Digits in Custom Syntax</a> or <a href=\"https://github.com/leanprover/lean4/issues/242\">Lean 4 GitHub issue #242</a> for examples of where this problem pops up.</p>",
        "id": 238845978,
        "sender_full_name": "Mac",
        "timestamp": 1621037647
    },
    {
        "content": "<p>I was referring to <code>tokenFn</code>, which doesn't seem to deal directly with the <code>Token</code> type and just parses a <code>Syntax</code></p>",
        "id": 238846646,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621038289
    },
    {
        "content": "<p>Ah, but <code>tokenFn</code> doesn't necessarily parse a syntax. It first tries to pop one from the token cache. Only if there is none does it parse one (through <code>tokenFnAux</code>). And <code>tokenFnAux</code> is hard-coded to parse the specific set of Syntaxes that constitutes a token. There is no way to alter this set, and all higher parsers and combinators boil down to tokens (outside of a few specific ones).</p>",
        "id": 238847003,
        "sender_full_name": "Mac",
        "timestamp": 1621038614
    },
    {
        "content": "<p>yes, that sounds right</p>",
        "id": 238847066,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621038680
    },
    {
        "content": "<p>I imagine that there is some performance cost to making the tokenizer dynamic</p>",
        "id": 238847098,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621038721
    },
    {
        "content": "<p>but you will have to wait for <span class=\"user-mention\" data-user-id=\"110024\">@Sebastian Ullrich</span> to give an authoritative answer</p>",
        "id": 238847218,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621038785
    },
    {
        "content": "<p>I don't think the token cache matters that much, it's just memoizing the function</p>",
        "id": 238847274,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621038814
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/About.20Tokens.20.28and.20the.20Token.20Cache.29/near/238847098\">said</a>:</p>\n<blockquote>\n<p>I imagine that there is some performance cost to making the tokenizer dynamic</p>\n</blockquote>\n<p>That's probably true. I'm curious as to what those are,  especially considering that from the discussion in <a href=\"#narrow/stream/270676-lean4/topic/Error.20with.20Digits.20in.20Custom.20Syntax/near/235405784\">Digits in Custom Syntax</a> that local token sets on the roadmap somewhere in the future. So it doesn't seem the burden is too insurmountable or the idea too far off the beaten path.<br>\nIf it's not. It's something I might want to take a look into playing with (and perhaps implementing a way myself to make it more dynamic).</p>",
        "id": 238847462,
        "sender_full_name": "Mac",
        "timestamp": 1621038944
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/About.20Tokens.20.28and.20the.20Token.20Cache.29/near/238847274\">said</a>:</p>\n<blockquote>\n<p>I don't think the token cache matters that much, it's just memoizing the function</p>\n</blockquote>\n<p>True, but its clearly important for performance reasons and for the Pratt parser. Thus preserving the notion of a cacheable head <code>Token</code> seems important. That is why I would think one  possible way of making the cache more dynamic is generalizing it to cache a general head <code>Syntax</code> instead of restricted subset of a <code>Token</code>.</p>",
        "id": 238847653,
        "sender_full_name": "Mac",
        "timestamp": 1621039109
    },
    {
        "content": "<p>But there is no such constraint</p>",
        "id": 238847669,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621039138
    },
    {
        "content": "<p>the token cache stores a <code>Syntax</code></p>",
        "id": 238847676,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621039148
    },
    {
        "content": "<p>True, the cache, by its definition already supports this.</p>",
        "id": 238847688,
        "sender_full_name": "Mac",
        "timestamp": 1621039154
    },
    {
        "content": "<p>it's literally just the return value of <code>tokenFn</code> at some particular recent argument</p>",
        "id": 238847705,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621039173
    },
    {
        "content": "<p>The question is making the the parser that uses it more dynamic.</p>",
        "id": 238847711,
        "sender_full_name": "Mac",
        "timestamp": 1621039180
    },
    {
        "content": "<p>i.e. making what <code>tokenFnAux</code> parses dynamic.</p>",
        "id": 238847768,
        "sender_full_name": "Mac",
        "timestamp": 1621039214
    },
    {
        "content": "<p>Sure, that would entail putting an additional indirection in front of <code>tokenFnAux</code> to use <code>parserCategory</code></p>",
        "id": 238847771,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621039216
    },
    {
        "content": "<p>Yep, that is what I'm thinking. I'm curious what the performance concerns there would be.</p>",
        "id": 238847802,
        "sender_full_name": "Mac",
        "timestamp": 1621039255
    },
    {
        "content": "<p>I'm 100% sure this is not something they would trust anyone else to implement though</p>",
        "id": 238847810,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621039266
    },
    {
        "content": "<p>That's fair. Regardless, though, if its a viable route I might play around with it on my own.</p>",
        "id": 238847893,
        "sender_full_name": "Mac",
        "timestamp": 1621039343
    },
    {
        "content": "<p>Also, I am not going ahead a toying with it myself right now as I imagined others might (like you) might have good insight as to complications and considerations that would probably be useful to hear before prototyping.</p>",
        "id": 238848208,
        "sender_full_name": "Mac",
        "timestamp": 1621039650
    },
    {
        "content": "<p>Furthermore, this discussion might prove useful to the Lean 4 developers whenever they get around to similar features in the future.</p>",
        "id": 238848300,
        "sender_full_name": "Mac",
        "timestamp": 1621039710
    },
    {
        "content": "<p>In essence, I am trying to model this discussion of the first step of the new <a href=\"https://github.com/leanprover/lean4/blob/master/CONTRIBUTING.md#unexpected-pull-requests\">Contribution Guidelines</a>. Mostly because I suspect this feature is far on the backburner, but something at least I am very interested in.</p>",
        "id": 238848408,
        "sender_full_name": "Mac",
        "timestamp": 1621039811
    },
    {
        "content": "<p>As far as I can tell there isn't any fundamental issue with making <code>tokenFn</code> dynamic. There isn't even really a well defined tokenization step in the parser, <code>tokenFn</code> is just one particularly low level parser and all parsers are ultimately operating on the source string</p>",
        "id": 238848433,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621039836
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/About.20Tokens.20.28and.20the.20Token.20Cache.29/near/238848433\">said</a>:</p>\n<blockquote>\n<p>As far as I can tell there isn't any fundamental issue with making <code>tokenFn</code> dynamic.</p>\n</blockquote>\n<p>I was getting a similar feeling, but I am asking here and to see if there were things I was missing or if I was right, and also to see what recommendations there were for approaching this issue.</p>\n<p>Thanks, by the way, for your input! <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 238848682,
        "sender_full_name": "Mac",
        "timestamp": 1621040087
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/270676-lean4/topic/About.20Tokens.20.28and.20the.20Token.20Cache.29/near/238847771\">said</a>:</p>\n<blockquote>\n<p>Sure, that would entail putting an additional indirection in front of <code>tokenFnAux</code> to use <code>parserCategory</code></p>\n</blockquote>\n<p>One thing to note here though is that category parsers are Pratt parsers which in Lean, according to the documentation, use the head token to select alternatives. Thus parsing tokens using the current category parser implementation would cause a cycle (I believe).</p>",
        "id": 238849238,
        "sender_full_name": "Mac",
        "timestamp": 1621040261
    },
    {
        "content": "<p>true, you would need a more simple minded selection procedure like <code>first [p1, p2, ...]</code> (maybe with priorities) because there is nothing to index a token parser on</p>",
        "id": 238856182,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621042010
    },
    {
        "content": "<p>Luckily, this does not need to be done entirely from scratch, the current Pratt parser implementation already supports non-indexed parser (in a list) alongside token-indexed ones, so it would just need to be a stripped down version that doesn't have the token-indexed option.</p>",
        "id": 238856301,
        "sender_full_name": "Mac",
        "timestamp": 1621042125
    },
    {
        "content": "<p>Whether or not the Pratt parser uses the index could even be an (optional) parameter to parser (that is passed on down)</p>",
        "id": 238856656,
        "sender_full_name": "Mac",
        "timestamp": 1621042442
    },
    {
        "content": "<p>you probably want to skip the pratt parser stuff though, since I don't think leading / trailing parsers makes a lot of sense</p>",
        "id": 238856790,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621042524
    },
    {
        "content": "<p>true, it could probably just use the longest match parser for disambiguation</p>",
        "id": 238857042,
        "sender_full_name": "Mac",
        "timestamp": 1621042793
    },
    {
        "content": "<p>Using a non-Pratt parser for category would require augmenting <code>ParserCategory</code> to have two alternatives <code>pratt</code> and <code>longest</code> (or <code>token</code>/<code>simple</code>). Luckily, Lean already seems designed for such an option and this seems like a feasible extension.</p>",
        "id": 238858445,
        "sender_full_name": "Mac",
        "timestamp": 1621044049
    },
    {
        "content": "<p>though, thinking about it more, it should definitely somehow integrate the token trie.</p>",
        "id": 238859299,
        "sender_full_name": "Mac",
        "timestamp": 1621044848
    }
]