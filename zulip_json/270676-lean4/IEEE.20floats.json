[
    {
        "content": "<p>\"Does Lean 4 have IEEE floats\", I am asked by someone in my department (Sheehan Olver). Further questions seem to indicate that they want to make statements of the form \"if I know various facts about <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant=\"double-struck\">R</mi><mo>→</mo><mi mathvariant=\"double-struck\">R</mi></mrow><annotation encoding=\"application/x-tex\">f:\\R\\to \\R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">R</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">R</span></span></span></span> then I can prove that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>f</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f&#x27;(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0019em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7519em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span> is within <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">ϵ</span></span></span></span> of <code>F([x]+[h])/[h]</code> where <code>F : float -&gt; float</code> is float -&gt; real -(f)-&gt; real -&gt; float, and [x] is the float associated to real x. For this one seems to need maps between <code>float</code> and <code>real</code> in both directions, plus some statements about error bounds when e.g. moving from real to float to real, and when comparing float addition to real addition (and similar for subtraction and multiplication and whatever else is defined on <code>float</code>). This all must be basic stuff in (some theory or other whose name I don't know). I am dimly aware that Lean 4 has floats, but am also very aware that I don't really know what I'm talking about. On the other hand I can just pretend that mathlib is ported to Lean 4 and then it has reals and these I understand. </p>\n<p>What work is needed to rigorously prove basic things about floats? Is it a huge project which will involve new ideas, or just a case of porting some library which has been done 100 times over already in other systems? Will it be a huge amount of work? Is it on anyone's radar?</p>",
        "id": 272592908,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645364393
    },
    {
        "content": "<p>FYI here’s a Jupiter notebook of the kind of results I’d like to verify:</p>\n<p><a href=\"https://nbviewer.org/github/Imperial-MATH50003/MATH50003NumericalAnalysis/blob/main/notebooks/Differentiation.ipynb\">https://nbviewer.org/github/Imperial-MATH50003/MATH50003NumericalAnalysis/blob/main/notebooks/Differentiation.ipynb</a></p>",
        "id": 272593362,
        "sender_full_name": "Sheehan Olver",
        "timestamp": 1645364873
    },
    {
        "content": "<p>We do have floats here <a href=\"https://leanprover-community.github.io/mathlib_docs/find/Float/src\">src#Float</a> however they are right now designed as an opaque data type that via some <code>@extern</code> declarations on its operations effectively links to the C implementation of IEEE floats and has absolutely no logic regarding the actual IEEE stuff implemented in Lean.</p>\n<p>There has been some discussion on the topic of integrating Floats with Reals in Lean, lots of links regarding this topic can be found over here <a href=\"#narrow/stream/113489-new-members/topic/Real.20to.20Float.20and.20computable.20numbers/near/266659575\">https://leanprover.zulipchat.com/#narrow/stream/113489-new-members/topic/Real.20to.20Float.20and.20computable.20numbers/near/266659575</a> it appears some stuff regarding this has been done in both Lean and other theorem provers. But I'm not aware of any implementation of the mentioned stuff in Lean 4 right now.</p>",
        "id": 272593427,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1645364924
    },
    {
        "content": "<p>Also since the question seems to be of numeric nature <span class=\"user-mention\" data-user-id=\"346070\">@Tomas Skrivan</span> might be able to help further here, he's done a bit of work in that direction already.</p>",
        "id": 272593634,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1645365125
    },
    {
        "content": "<p>Unfortunately, I know very little about proving facts about floats. I'm mostly interested in using Lean/dependent types for (semi)automatic code transformation and I don't have a good idea how to do that with regards to floats. So I did not spend much time studying this problem.</p>\n<p>A quick Google search revealed Coq library <a href=\"https://flocq.gitlabpages.inria.fr/\">Flocq</a> about formalization of floats, looking at the size and complexity could reveal the difficulty/amount of work needed.</p>",
        "id": 272599210,
        "sender_full_name": "Tomas Skrivan",
        "timestamp": 1645371554
    },
    {
        "content": "<p>Ugh they wrote a whole book about it <span aria-label=\"scared\" class=\"emoji emoji-1f628\" role=\"img\" title=\"scared\">:scared:</span></p>",
        "id": 272599403,
        "sender_full_name": "Tomas Skrivan",
        "timestamp": 1645371770
    },
    {
        "content": "<p>This is the floating point model used by the Verified Software Toolchain, which supports reasoning about floats (and numeric algorithms in general):</p>\n<p><a href=\"https://flocq.gitlabpages.inria.fr/\">https://flocq.gitlabpages.inria.fr/</a></p>",
        "id": 272604542,
        "sender_full_name": "Tim Carstens",
        "timestamp": 1645377223
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span>  Reasoning about floats is \"easy\" in the sense that people know how to do it; it is \"hard\" in the sense that \"numerical methods\" is a field dedicated to efficiently using and reasoning about them</p>\n<p>We have a small but active community of numerical people using VST + Coq in industry; happy to make introductions if your colleague has a project they'd like to get underway</p>",
        "id": 272604758,
        "sender_full_name": "Tim Carstens",
        "timestamp": 1645377480
    },
    {
        "content": "<p>I'm familiar with IEEE floating point (I'm one of the maintainers of the floating-point functions in glibc, for example), but I've never done floating-point formalization - though I did do an <a href=\"#narrow/stream/208328-IMO-grand-challenge/topic/IMO.202020\">IMO 2020 Q2 solution by numerical bounds</a> which illustrates how you can prove concrete numerical bounds in Lean right now with <code>norm_num</code> (without floating point being involved at all in the formalization itself) as long as you do most of the work to reduce to specific integer / rational arithmetic cases outside of Lean.</p>\n<p>I don't think any new ideas are needed for floating-point formalization - it's been done before in e.g. Coq and HOL - just a lot of work to set things up. You can see the sort of lemmas involved in proving things about error propagation in e.g. the <a href=\"https://www.mpfr.org/algorithms.pdf\">MPFR algorithms document</a>. Although the inequalities involved are mathematically routine, you definitely need mathlib first in order to do that sort of reasoning about real numbers at all.</p>\n<p>How complicated your setup for defining floating-point is may depend on what properties of IEEE floating-point you want to model. Do you want to model floating-point exceptions? If you want to prove correctness of an implementation of a standard library function, which has to satisfy rules about which exception flags are raised, then exceptions matter (including the different options IEEE 754 provides for tininess detection); if you want to prove bounds on the accuracy of a numerical algorithm, maybe they don't. Do you want to cover rounding modes other than roundTiesToEven? Again, that matters for correctness of standard library functions (the sort of functions I tend to be implementing with floating point!), but maybe not for a lot of numerical purposes. Do you care about decimal floating point? That's also defined in IEEE 754; it's more of a niche interest, but maximal generality would mean including it (and probably other bases as well, though IEEE 754 only covers binary and decimal).</p>\n<p>What about alternate exception handling? Interchange encodings, as opposed to just dealing with floating-point values without regard to how they are encoded? Choice of NaN payload for results, which isn't generally specified in IEEE 754? If for example you want to provide a formal model of instruction set semantics for some processor architecture, all those things become relevant, and so allowing for them might be relevant when designing how to represent floating-point formally even if they're not your immediate interests. Do you want to model how the elementary arithmetic operations are actually mostly formatOf operations in IEEE 754 (results need not be in the same format as the arguments), although most processors don't implement that directly?</p>\n<p>Do you want your Lean definitions of floating-point operations to be computable, or only suitable for proving things? The mathlib style would definitely be not to worry about computability, and to leave it to tactics to do computation if you ever want to prove something like \"this floating-point operation, with these floating-point numbers as arguments, produces this floating-point number as a result\" that involves concrete numbers rather than general bounds. All the operations in IEEE 754 can in principle be implemented computably (i.e. so that both the results are provably correct and Lean can see they do terminate), even the recommended transcendental operations in clause 9, but it may be much simpler if you don't care about computability.</p>",
        "id": 272605552,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1645378382
    },
    {
        "content": "<p>I definitely want other rounding modes as it makes it possible to implement interval arithmetic in floating point. An example is one of the problem sheet questions which asks to rigorously compute e using floating point rounding; if this was made verifiable it would make it possible to verify other special functions. So that is an example of concrete numbers and bounds.</p>\n<p>But other theorems (such as the error bound of finite differences) require just abstract bounds.</p>\n<p>I don’t think decimal IEEE is needed. The applications with concrete numbers would be to make computations done in hardware verifiable. This is important if one wants to cite the results in a paper as an afterthought: that is, in the first instance one would do the computation using hardware and only verify when writing up (or potentially say “it is verifiable” without actually doing it).</p>",
        "id": 272607765,
        "sender_full_name": "Sheehan Olver",
        "timestamp": 1645381037
    },
    {
        "content": "<p>There is some work in mathlib on IEEE floats, but it is very experimental and although I was hoping someone would pick it up (there are some really easy sorrys in there) no one ever did: <a href=\"https://leanprover-community.github.io/mathlib_docs/find/fp.float/src\">src#fp.float</a></p>",
        "id": 272640217,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645418049
    },
    {
        "content": "<p>We could port Flocq if it comes to it, but this is definitely a long term project and there has historically not been much interest in pushing in this direction</p>",
        "id": 272640327,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645418188
    },
    {
        "content": "<p>Certainly it is much more practical for the near term to use verified computation on <code>Rat</code> instead and do height reduction by adding approximation when necessary to keep the numbers manageable; the floating point unit will be cold but you can still do similar tricks to FP computation, interval arithmetic and all that</p>",
        "id": 272640426,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645418357
    },
    {
        "content": "<p>Using literal IEEE floats is a rats nest of problems because sometimes compilers do double rounding via conversion to 80 bit floats internally (sometimes, under some optimization settings), and every processor has its own idea for what to do about NaN payloads, even though there is an IEEE operation that makes these bits observable... And custom rounding modes are of course important for interval arithmetic but support for this in C is nonexistent which means you have to either write custom assembly or use compiler specific intrinsics...</p>",
        "id": 272640601,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645418544
    },
    {
        "content": "<p>Not to mention the issues with IEEE operations themselves, like <code>==</code> being broken both because it's not reflexive (<code>let x := NaN in x == x</code> is false) and it fails congruence (<code>let zero := 0, neg_zero := -zero in zero == neg_zero &amp;&amp; 1/zero != 1/neg_zero</code> is true)</p>",
        "id": 272640698,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645418675
    },
    {
        "content": "<blockquote>\n<p>Do you want to model how the elementary arithmetic operations are actually mostly formatOf operations in IEEE 754 (results need not be in the same format as the arguments), although most processors don't implement that directly?</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"266253\">@Joseph Myers</span> Could you elaborate on this one? What do you mean by formatOf, is this conversion to/from integral types or conversion between floating point types of different bit width? And why would they be a majority of operations?</p>",
        "id": 272641005,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645419083
    },
    {
        "content": "<blockquote>\n<p>All the operations in IEEE 754 can in principle be implemented computably (i.e. so that both the results are provably correct and Lean can see they do terminate), even the recommended transcendental operations in clause 9</p>\n</blockquote>\n<p>How did they achieve this? Requiring correct rounding for a transcendental operation is essentially a real number inequality, which is not computable in general; so asserting that some implementation of float64 sin() is correct amounts to 2^64 exact real number inequalities (of the form <code>0.123 &lt;= sin(0.144) &lt; 0.124</code>), which, although it might be possible to prove if you are lucky and all 2^64 real numbers land suitably far from their claimed bounds that you can establish these inequalities, still sounds like more work than people could ever have put into the problem.</p>",
        "id": 272641416,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645419490
    },
    {
        "content": "<p>Not to mention that many processors disagree about the last few digits of these approximate transcendental functions. Hell, Intel got the value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span></span></span></span> wrong when implementing <code>FSIN</code> so you get really terrible results if you use it at large values</p>",
        "id": 272641604,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645419650
    },
    {
        "content": "<p>I’m not an expert but in Julia sin(x) just uses the Taylor series/Horner:</p>\n<p><a href=\"https://github.com/JuliaLang/julia/blob/0b48b91c9881823d860eaf0a605072b7f20a4cbb/base/special/trig.jl#L76\">https://github.com/JuliaLang/julia/blob/0b48b91c9881823d860eaf0a605072b7f20a4cbb/base/special/trig.jl#L76</a></p>\n<p>The errors here are likely to be provable.</p>\n<p>For concrete cases you are probably right: floats are a subset of rationals so inequalities can be verified that way. Though there may be practical issues with the fact that one gets extremely large numerators/denominators.</p>",
        "id": 272699354,
        "sender_full_name": "Sheehan Olver",
        "timestamp": 1645455804
    },
    {
        "content": "<p>Math tells us <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sin(q)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mclose\">)</span></span></span></span> is never rational for rational <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi><mo mathvariant=\"normal\">≠</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">q \\ne 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\"><span class=\"mrel\"><span class=\"mord vbox\"><span class=\"thinbox\"><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"inner\"><span class=\"mord\"><span class=\"mrel\"></span></span></span><span class=\"fix\"></span></span></span></span></span><span class=\"mrel\">=</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span>, so any convergent sequence of bounds on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sin(q)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mclose\">)</span></span></span></span> will eventually answer the question of whether <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi><mo>&lt;</mo><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r &lt; \\sin(q)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mclose\">)</span></span></span></span> for rational <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span>. But predicting in advance how close <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sin(q)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mclose\">)</span></span></span></span> could be to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span> (and so how well we have to approximate <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\sin(q)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">sin</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"mclose\">)</span></span></span></span>) is some more difficult question in analytic/transcendental number theory.</p>",
        "id": 272700931,
        "sender_full_name": "Reid Barton",
        "timestamp": 1645456523
    },
    {
        "content": "<p>Floats and floating point arithmetic are more structured than general rationals. Again I’m not an expert but a quick experiment shows that for 100,000 random floats that the  true sin(x) when rounded up or down is equal to the Julia sin(x).  (It isn’t necessarily rounding in the right direction)</p>\n<div class=\"codehilite\" data-code-language=\"Julia\"><pre><span></span><code><span class=\"n\">julia</span><span class=\"o\">&gt;</span> <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">100_000</span><span class=\"p\">);</span> <span class=\"c\"># 100k random Float64</span>\n\n<span class=\"n\">julia</span><span class=\"o\">&gt;</span> <span class=\"n\">all</span><span class=\"p\">(</span><span class=\"nd\">@.</span><span class=\"p\">(</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"kt\">Float64</span><span class=\"p\">(</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">big</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))))</span> <span class=\"c\"># some are not correctly rounded</span>\n<span class=\"nb\">false</span>\n\n<span class=\"n\">julia</span><span class=\"o\">&gt;</span> <span class=\"n\">all</span><span class=\"p\">(</span><span class=\"nd\">@.</span><span class=\"p\">(</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"kt\">Float64</span><span class=\"p\">(</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">big</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)),</span> <span class=\"nb\">RoundUp</span><span class=\"p\">)</span> <span class=\"o\">||</span> <span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"kt\">Float64</span><span class=\"p\">(</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">big</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)),</span> <span class=\"nb\">RoundDown</span><span class=\"p\">)))</span> <span class=\"c\"># all are between two nearest floats</span>\n<span class=\"nb\">true</span>\n</code></pre></div>",
        "id": 272704623,
        "sender_full_name": "Sheehan Olver",
        "timestamp": 1645458342
    },
    {
        "content": "<p>Right, this is the <a href=\"http://perso.ens-lyon.fr/jean-michel.muller/Intro-to-TMD.htm\">Table maker's dilemma</a>, which is essentially a variation on the busy beaver problem for concrete n. Not necessarily undecidable, but you don't know going into it how bad it will be, and claimed solutions are very short (the assertion that N digits of precision suffice for all numbers in the range) and <em>very</em> hard to verify</p>",
        "id": 272713220,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645462691
    },
    {
        "content": "<p>If you allow incorrect rounding then the problem becomes <em>much</em> easier, because then you can apply regular error analysis to argue that some polynomial approximation is within epsilon of the answer where epsilon is set to 1 ULP or maybe 1/2 ULP</p>",
        "id": 272713746,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645462932
    },
    {
        "content": "<p>I found this quite interesting as well. I didn't know about that line in the standard. A quick search came up with <a href=\"https://hal-ens-lyon.archives-ouvertes.fr/ensl-01529804/document\">CR-libm</a>, which was one of the first <code>libm</code>'s with formally verified correct rounding for transcendental functions. The basic approach seems to be using clever tricks to prove that a given polynomial approximation gives correctly rounded results for some subset of the input, and then <a href=\"https://www.vinc17.net/research/papers/arith13.pdf\">exhaustively checking the rest</a>. Notably, <code>pow</code> is <strong>not</strong> guaranteed to be rounded correctly, since the search space is twice as large and there's no clever tricks.</p>",
        "id": 272714022,
        "sender_full_name": "Dylan MacKenzie (ecstatic-morse)",
        "timestamp": 1645463080
    },
    {
        "content": "<p>(Table 1 in that first link gives the domain for which results are guaranteed to be correctly rounded for each function)</p>",
        "id": 272714248,
        "sender_full_name": "Dylan MacKenzie (ecstatic-morse)",
        "timestamp": 1645463181
    },
    {
        "content": "<p>Floats might be rational but float addition can't be rational addition because the latter is associative.</p>",
        "id": 272714310,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1645463227
    },
    {
        "content": "<p>well, the IEEE rule is you perform the operation in exact real arithmetic and then round with the specified rounding mode</p>",
        "id": 272714575,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645463378
    },
    {
        "content": "<p>so associativity is something like <code>f(f(x + y) + z) = f(x + f(y + z))</code> where <code>+</code> is addition of rationals <code>f</code> is a weird function that is monotonic and idempotent and not much else</p>",
        "id": 272714695,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645463450
    },
    {
        "content": "<p>so the fact that it fails shouldn't be too surprising</p>",
        "id": 272714719,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1645463468
    },
    {
        "content": "<p>Ah, I guess the only clever trick is that most of those functions overflow/underflow to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord\">∞</span></span></span></span> for some interval, so you don't have to check the full 64-bit domain. The ones that don't (e.g. <code>sin</code>) are only guaranteed within some interval.</p>",
        "id": 272716143,
        "sender_full_name": "Dylan MacKenzie (ecstatic-morse)",
        "timestamp": 1645464341
    },
    {
        "content": "<p>\"formatOf indicates that the name of the operation specifies the floating-point destination format, which might be different from the floating-point operands’ formats. There are formatOf versions of these operations for every supported arithmetic format.\". That is, those operations aren't homogeneous; the operands and the result need not be of the same format (e.g. you can add two binary128 operands, producing a binary32 result, with only a single rounding involved), though they do need to have the same radix. This applies to the operations: addition, subtraction, multiplication, division, squareRoot, fusedMultiplyAdd (conversion operations are also described as formatOf operations, since the type of the result is independent of the type of the input there).</p>",
        "id": 272754039,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1645497360
    },
    {
        "content": "<p>The state of the art for correct rounding of special functions has advanced somewhat since crlibm, see e.g. <a href=\"https://hal.inria.fr/hal-03240179\">this paper</a>. So worst cases for correct rounding for binary64 can be found rather more efficiently than by an exhaustive search, and while a corresponding search isn't yet feasible for binary128, a smaller search is possible that bounds the internal precision needed rather better than the bounds that can be obtained from more general results not involving such a search at all.</p>\n<p>(For all the transcendental functions with correctly rounded versions recommended in IEEE 754, it can be deduced from known results that they never produce a rational result for a rational argument for nontrivial reasons; mostly this follows from Lindemann's theorem that exp(x) is transcendental for nonzero algebraic x. And there are versions of Lindemann's theorem in the literature that give explicit, if rather large, bounds, at least one referenced in the paper linked above.)</p>",
        "id": 272754561,
        "sender_full_name": "Joseph Myers",
        "timestamp": 1645498002
    }
]