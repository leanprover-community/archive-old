[
    {
        "content": "<p>Is there a method for writing proofs about computational complexity in lean? I have not been able to find anything of the sort but I am also not very sure where to look.</p>",
        "id": 264319554,
        "sender_full_name": "Parker Bjur",
        "timestamp": 1639064750
    },
    {
        "content": "<p>There is an apparently convenient way to do so in Coq through the λ calculus: <a href=\"https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=13915\">https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=13915</a></p>",
        "id": 264320056,
        "sender_full_name": "Anne Baanen",
        "timestamp": 1639064947
    },
    {
        "content": "<p>See also these previous threads:</p>\n<ul>\n<li><a href=\"#narrow/stream/116395-maths/topic/complexity.20theory\">https://leanprover.zulipchat.com/#narrow/stream/116395-maths/topic/complexity.20theory</a></li>\n<li><a href=\"#narrow/stream/113488-general/topic/Ph.2ED.2E.20on.20formalizing.20complexity.20classes\">https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Ph.2ED.2E.20on.20formalizing.20complexity.20classes</a></li>\n<li><a href=\"#narrow/stream/217875-Is-there.20code.20for.20X.3F/topic/Complexity.20theory\">https://leanprover.zulipchat.com/#narrow/stream/217875-Is-there.20code.20for.20X.3F/topic/Complexity.20theory</a></li>\n<li><a href=\"#narrow/stream/113489-new-members/topic/Computability.2C.20P.20.28and.20NP.29\">https://leanprover.zulipchat.com/#narrow/stream/113489-new-members/topic/Computability.2C.20P.20.28and.20NP.29</a></li>\n</ul>",
        "id": 264320394,
        "sender_full_name": "Anne Baanen",
        "timestamp": 1639065073
    },
    {
        "content": "<p>Thank You!</p>",
        "id": 264321904,
        "sender_full_name": "Parker Bjur",
        "timestamp": 1639065660
    },
    {
        "content": "<p>This is a long shot but ... is anyone working on any complexity theory in Lean?</p>",
        "id": 265910621,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640261250
    },
    {
        "content": "<p>How difficult would it be to transcribe the Fabian Kunze et al's library from Coq to Lean? Any rough estimate anyone?<br>\n<a href=\"https://github.com/uds-psl/cook-levin\">https://github.com/uds-psl/cook-levin</a></p>",
        "id": 266004248,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640354812
    },
    {
        "content": "<p>I don't see a good reason to actually transcribe it, those proofs don't look easy to follow. Probably you should just use the theorems as inspiration</p>",
        "id": 266025566,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640382181
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266025566\">said</a>:</p>\n<blockquote>\n<p>I don't see a good reason to actually transcribe it, those proofs don't look easy to follow. Probably you should just use the theorems as inspiration</p>\n</blockquote>\n<p>Thank you for an answer!</p>\n<p>On which level would you recommend me to get inspired by it? Should I break down the problem to equivalent lemmata one-to-one?</p>",
        "id": 266026173,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640383155
    },
    {
        "content": "<p>I've been working on a PR for the mu-recursive definition of computability. It seems like <a href=\"https://github.com/uds-psl/cook-levin\">https://github.com/uds-psl/cook-levin</a> and <a href=\"https://arxiv.org/pdf/1102.5495.pdf\">https://arxiv.org/pdf/1102.5495.pdf</a> both use definitions that have a similar approach to the model of computation. Perhaps after this PR is made, we can build up from there in terms of defining the time complexity of various algorithms.</p>",
        "id": 266031639,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1640390337
    },
    {
        "content": "<p>Ok, the PR is <a href=\"https://github.com/leanprover-community/mathlib/pull/11046\">#11046</a>. <span class=\"user-mention\" data-user-id=\"417654\">@Martin Dvořák</span> , if you or anyone else wants to contribute, feel free to push to this branch. It feels like someone comes along every few months looking for a complexity library and not finding one. Maybe the issue is that we just need to get started, and once people see there's something to build off of, it'll grow from there.</p>",
        "id": 266032132,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1640390903
    },
    {
        "content": "<p>That's exactly what happened with graph theory BTW. Consider making your own stream!</p>",
        "id": 266032352,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1640391257
    },
    {
        "content": "<p>Great initiative!</p>",
        "id": 266054882,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640431060
    },
    {
        "content": "<p>It seems to me that I don't have permission to create new streams. Will any admin or moderator help me?</p>",
        "id": 266056104,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640433202
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282271\">Bolton Bailey</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266031639\">said</a>:</p>\n<blockquote>\n<p>I've been working on a PR for the mu-recursive definition of computability. It seems like <a href=\"https://github.com/uds-psl/cook-levin\">https://github.com/uds-psl/cook-levin</a> and <a href=\"https://arxiv.org/pdf/1102.5495.pdf\">https://arxiv.org/pdf/1102.5495.pdf</a> both use definitions that have a similar approach to the model of computation. Perhaps after this PR is made, we can build up from there in terms of defining the time complexity of various algorithms.</p>\n</blockquote>\n<p>How big is the time overhead of partial recursive functions over Turing Machines? And over RAM, is it known?</p>",
        "id": 266056534,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640433791
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282271\">Bolton Bailey</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266032132\">said</a>:</p>\n<blockquote>\n<p>Ok, the PR is <a href=\"https://github.com/leanprover-community/mathlib/pull/11046\">#11046</a>. <span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> , if you or anyone else wants to contribute, feel free to push to this branch. It feels like someone comes along every few months looking for a complexity library and not finding one. Maybe the issue is that we just need to get started, and once people see there's something to build off of, it'll grow from there.</p>\n</blockquote>\n<p>Idea if more than 1 person wants to contribute: open PRs to that branch instead of committing and pushing directly to it (as if it were a master branch)</p>",
        "id": 266064466,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640447307
    },
    {
        "content": "<p>Would someone more knowledgeable mind laying down a roadmap on this subject? I am having a hard time understanding the structure of the Coq repo</p>",
        "id": 266080256,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640474366
    },
    {
        "content": "<p>I can sketch some preliminary roadmap but somebody more knowledgeable will have to check and correct after me. Would it be for thee, knowledgeable person, easier this way?</p>",
        "id": 266081232,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640476222
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"417654\">@Martin Dvořák</span> I'd say go for it :D. It's a brand new branch so np</p>",
        "id": 266081273,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640476323
    },
    {
        "content": "<p>My rough sketch of the roadmap towards complexity theory in Lean is the following. We will follow the approach of Kunze et al.<br>\nPaper: <a href=\"https://drops.dagstuhl.de/opus/volltexte/2021/13915/pdf/LIPIcs-ITP-2021-20.pdf\">https://drops.dagstuhl.de/opus/volltexte/2021/13915/pdf/LIPIcs-ITP-2021-20.pdf</a><br>\nCode (Coq): <a href=\"https://github.com/uds-psl/coq-library-complexity\">https://github.com/uds-psl/coq-library-complexity</a><br>\nPlease note that we won't use the letter L for the name of the complexity class (deterministic logarithmic time).</p>\n<p>(1) Define the notion of a decision problem in the weak call-by-value λ-calculus L.<br>\n(2) Define the class P in L.<br>\n(3) Define the class NP in L.<br>\n(4) Define polytime reductions in L.<br>\n(5) Prove that, if A polytime reduces to B in P, then A in P.<br>\n(6) Define the classes NP-hard and NP-complete in L.<br>\n(7) Define a natural NP-complete problem in L.<br>\n(8) Prove that, if NP-hard A polytime reduces to B, then B is NP-hard.<br>\n(9) Define the Abstract heap machines and Turing machines — just that we can state the auxiliary decision problems about them; we will not program in them; the reductions will be always programmed in L.<br>\n(10) State and prove the polytime reductions (very challenging).<br>\n(11) State your favourite version of the tiling problem.<br>\n(12) Prove the NP-hardness of the tiling problem using the tools above (the vanilla TM will have to be reduced to it).<br>\n(13) Reduce your tiling problem to some form of SAT.<br>\n(14) We can continue building the complexity theory from here on without getting our hands dirty with the intricacies of various computational models.</p>",
        "id": 266114221,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640532555
    },
    {
        "content": "<p>Turing machines already exist in the file <code>computability.turing_machine</code>.</p>",
        "id": 266114722,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1640533337
    },
    {
        "content": "<p>I sent an e-mail to Fabian Kunze requesting his help.</p>",
        "id": 266114950,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640533779
    },
    {
        "content": "<p>I don't understand why ye are grinning. I did my best I could do with my very superficial understanding of Kunze et al's approach.</p>",
        "id": 266115210,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640534216
    },
    {
        "content": "<p>I am aware of not providing much of a hint regarding the implementation.</p>",
        "id": 266115220,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640534266
    },
    {
        "content": "<p>It looks like an impressive and ambitious plan of work. I don’t know enough to make suggestions, but I’m very pleased to see that someone’s taking this on.</p>",
        "id": 266115718,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1640534952
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266115210\">said</a>:</p>\n<blockquote>\n<p>I don't understand why ye are grinning.</p>\n</blockquote>\n<p>Sorry, I didn't mean it in a mocking way. I was just happy that you actually did it <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span><br>\nZulip's mobile app doesn't have the \"on hover\" functionality that shows the actual emoji \"word\"</p>",
        "id": 266116466,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640536063
    },
    {
        "content": "<p>I disagree with the idea of requiring everything to be done in L. Not only is that not the computational model we have chosen for the existing computability material, it is also overly restrictive; users should be able to use any computational basis they want that is relevantly equivalent to turing machines or RAM model or whatever is the best computational basis for complexity theory. Currently, <code>computability.turing_machine</code> contains 3 different computational bases that are all equivalent (although the framework for making use of the equivalences for application to complexity classes like P does not exist yet), and users should be able to write e.g. polynomial time TM2 programs. The language L is also polytime equivalent to TM0 so you should be able to write programs in L if you want to, but I would want to allow as much freedom in this area as possible.</p>",
        "id": 266129313,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640555446
    },
    {
        "content": "<p>Perhaps Fagin's equivalence of NP and sentences in existential 2nd order logic is something that  should be more prominent. (In this vein, polynomial-time solvability becomes definability in the 1st order logic).</p>",
        "id": 266132817,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640560792
    },
    {
        "content": "<blockquote>\n<p>I disagree with the idea of requiring everything to be done in L.</p>\n</blockquote>\n<p>A big advantage of L is that is much more ergonomic to use, since Lean quite directly maps to L and most features are directly supported (higher-order functions, partial applications, etc.) with pretty much the runtime and space complexity you'd expect.  You can directly compute the runtime of List.filter as defined in Lean.  It is also straightforward to show that concrete Lean definitions are computable in L; this is a lot more exhausting in the current computability library (particularly with recursive functions, or n-ary functions with n&gt;=3).  I don't even want to imagine a formalization using turing machines.</p>\n<blockquote>\n<p>users should be able to use any computational basis they want that is relevantly equivalent to turing machines</p>\n</blockquote>\n<p>L is one such model, I'm not sure I see your point.</p>",
        "id": 266163148,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640601892
    },
    {
        "content": "<p>Well, the work on TMs is not get complete as far as connecting them to the primrec model. I would like to see a library closer to what we have for proving primrec, which does cover higher order stuff like list.filter.</p>",
        "id": 266163363,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640602147
    },
    {
        "content": "<p>The underlying computational model is somewhat orthogonal to this, you can build a higher order notion of time complexity on any model</p>",
        "id": 266163390,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640602191
    },
    {
        "content": "<p>From what I can tell of the complete implementation of L used to derive time bounds, it's not all that different from what you get from constructions like TM_to_partrec. It still has a pretty concrete operational model with an explicit evaluation context, so it's not as compositional as pure lambda calculus. The differences don't seem big enough to make it worth introducing yet another computational model and connecting it to the other models</p>",
        "id": 266163649,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640602454
    },
    {
        "content": "<p>I realize that you've put a lot of effort into this development, but I don't think it's a good direction.  The <code>primrec</code>/... stuff is really unergonomic to work with, e.g. for <code>list.filter</code> you need several lemmas showing that list.filter is primrec/computable/partrec if the predicate is primrec/computable/partrec.  If we wanted to tackle time complexity, I feel like we'd end up with another computational model and set of theorems anyhow.</p>",
        "id": 266163852,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640602614
    },
    {
        "content": "<p>I do think we can borrow some ideas from Kunze et al for the \"frontend\" part of this, like h.o. time complexity</p>",
        "id": 266164178,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640602883
    },
    {
        "content": "<p><code>primrec</code> is limited to first order functions on naturals, which is why there is the proliferation of theorems. With <code>nat.partrec.code</code> it becomes more practical to express that a higher order function is \"nice\" once and for all by proving that it is h.o. equivalent to a single <code>code</code>. This option is not available in <code>primrec</code> itself because it is still bootstrapping</p>",
        "id": 266164306,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640603016
    },
    {
        "content": "<p>In other words, I think we have the tools we need to simulate everything you would do with a computational model like L (in particular all the typeclasses and such that are necessary for an ergonomic library) without actually changing the basis</p>",
        "id": 266164415,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640603122
    },
    {
        "content": "<p>I think we need to do some design work on that frontend though; I don't think transliterating the coq code precisely will actually work all that well in lean, we will need to make some modifications but it's going in the right direction</p>",
        "id": 266164522,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640603235
    },
    {
        "content": "<blockquote>\n<p>It still has a pretty concrete operational model with an explicit evaluation context, so it's not as compositional as pure lambda calculus.</p>\n</blockquote>\n<p>Yes, proving that the time complexity of <code>f t</code> is the runtime of <code>t</code> + runtime of <code>f</code> on the resulting value is the same in either formalism.  What you get with L is in my eyes the following:</p>\n<ol>\n<li>You get a single theorem for computability (namely, a definition of a lambda term that compute the function for all values).  Properties like complexity, termination, etc. are then theorems about this lambda term.  This also works for higher-order function without any friction.  (To be fair, we could also do this with partial recursive functions and maybe turing machines---but I'm not sure if there's a canonical turing machine that suffices for complexity analysis).</li>\n<li>It's obvious how to represent data in L.  <code>List.take 10 xs</code> has <code>O(10)</code> runtime, no matter what the type of the elements in <code>xs</code> is.  Depending on how lists / function arguments / return values are represented in a Turing machine, this might entail copying the elements, the whole list, etc. (Though we could also solve that with other computational models, like random-access machines.)</li>\n</ol>",
        "id": 266164668,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640603374
    },
    {
        "content": "<blockquote>\n<p>I think we need to do some design work on that frontend though; I don't think transliterating the coq code precisely will actually work all that well in lean, we will need to make some modifications but it's going in the right direction</p>\n</blockquote>\n<p>Completely agreed on that.  Transliterating is certainly the wrong way.  But I think their ideas are good and sound, and deserve to be used as inspiration.</p>",
        "id": 266164765,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640603450
    },
    {
        "content": "<p>I think there is value in having a more complex intermediate language than L for the sake of having nice primitive operations with crafted time bounds. For example, throwing in the natural numbers with arithmetic operations on simply typed lambda calculus</p>",
        "id": 266165175,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640603891
    },
    {
        "content": "<blockquote>\n<p>In other words, I think we have the tools we need to simulate everything you would do with a computational model like L (in particular all the typeclasses and such that are necessary for an ergonomic library) without actually changing the basis</p>\n</blockquote>\n<p>I think it's clear that we could make a nicer frontend for showing computability, that just seems like an engineering problem.  The part that I don't see is how such a frontend would scale to showing time or space complexity.  Maybe you have already this figured out though.</p>",
        "id": 266165188,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640603906
    },
    {
        "content": "<p>Certainly it is nice to be able to say that <code>list.take 10 xs</code> has time <code>O(10)</code>. It would be even better if it takes exactly 10 steps (and lots of other basic functions like it also have exactly the right number of steps)</p>",
        "id": 266165220,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640603978
    },
    {
        "content": "<p>we probably won't be able to get that for everything, but a big haskell-ish language would make it easier to do the \"writing concrete programs\" part, which I expect to be much more important / time consuming than any work needed to set up the model and prove it is polynomially simulated by a TM or partial recursive function.</p>",
        "id": 266165300,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640604097
    },
    {
        "content": "<blockquote>\n<p>For example, throwing in the natural numbers with arithmetic operations on simply typed lambda calculus</p>\n</blockquote>\n<p>Practically speaking, this won't make any difference at all for any common NP problems because all natural numbers will be bounded by a very low bound.  Even the most basic implementation will be fast enough there.  We just need to replace <code>Nat</code> by <code>Num</code> (which is straightforward in the L approach).</p>",
        "id": 266165418,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640604226
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110043\">Gabriel Ebner</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266165188\">said</a>:</p>\n<blockquote>\n<p>I think it's clear that we could make a nicer frontend for showing computability, that just seems like an engineering problem.  The part that I don't see is how such a frontend would scale to showing time or space complexity.  Maybe you have already this figured out though.</p>\n</blockquote>\n<p>I've been thinking mostly about time complexity here. I would borrow Kunze's work for this part. Regarding space complexity, I haven't thought much about it, and we might need a RAM model to get all the results that complexity theorists expect here</p>",
        "id": 266165494,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640604278
    },
    {
        "content": "<blockquote>\n<p>Certainly it is nice to be able to say that <code>list.take 10 xs</code> has time <code>O(10)</code>. It would be even better if it takes exactly 10 steps (and lots of other basic functions like it also have exactly the right number of steps)</p>\n</blockquote>\n<p>It won't take exactly 10 steps in any model of computation (for a non-specialized take implementation), because decrementing natural numbers has some non-constant runtime.</p>",
        "id": 266165532,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640604330
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110043\">Gabriel Ebner</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266165418\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>For example, throwing in the natural numbers with arithmetic operations on simply typed lambda calculus</p>\n</blockquote>\n<p>Practically speaking, this won't make any difference at all for any common NP problems because all natural numbers will be bounded by a very low bound.  Even the most basic implementation will be fast enough there.  We just need to replace <code>Nat</code> by <code>Num</code> (which is straightforward in the L approach).</p>\n</blockquote>\n<p>Yes, but that's not the point. The idea behind the haskell-ish intermediate language is to calibrate the time bounds to be exactly some nice numbers. For example, to set the cost of <code>m * n</code> to <code>m.size * n.size + 1</code> or whatever we can get away with (and not have to introspect further on how these operations work)</p>",
        "id": 266165642,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640604448
    },
    {
        "content": "<p>The actual replacement of <code>Num</code> for <code>Nat</code> will happen in the once-and-for-all proof that this IL is polytime simulated by a TM</p>",
        "id": 266165710,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640604493
    },
    {
        "content": "<blockquote>\n<p>but a big haskell-ish language would make it easier to do the \"writing concrete programs\" part</p>\n</blockquote>\n<p>The whole idea about the L approach (and I believe what they do / want to do in Coq) is that this can be almost fully automated, since the host language maps so closely to the object language.  You write a function in pure idiomatic Lean, and then <code>deriving instance Computable for List.filter</code> and you have a realizing lambda term + theorems for the runtime of the cons and the nil case.</p>",
        "id": 266165723,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640604534
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110043\">Gabriel Ebner</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266165532\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>Certainly it is nice to be able to say that <code>list.take 10 xs</code> has time <code>O(10)</code>. It would be even better if it takes exactly 10 steps (and lots of other basic functions like it also have exactly the right number of steps)</p>\n</blockquote>\n<p>It won't take exactly 10 steps in any model of computation (for a non-specialized take implementation), because decrementing natural numbers has some non-constant runtime.</p>\n</blockquote>\n<p>This depends on how tight we need the simulation to be. If we can afford a log factor in the simulation then we can make decrementing numbers O(1)</p>",
        "id": 266165867,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640604699
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266165642\">said</a>:</p>\n<blockquote>\n<p>Yes, but that's not the point. The idea behind the haskell-ish intermediate language is to calibrate the time bounds to be exactly some nice numbers. For example, to set the cost of <code>m * n</code> to <code>m.size + n.size + 1</code> or whatever we can get away with (and not have to introspect further on how these operations work)</p>\n</blockquote>\n<p>That would be a wonderful breakthough.  I believe the best known multiplication algorithms are O(n log n).</p>",
        "id": 266165963,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640604784
    },
    {
        "content": "<p>Generally I think the focus on precise numbers is a distraction here.  Even in the best case there's going to be lots of annoying +1s and -1s that nobody cares about.  The runtime should be stated as a big-O, which is I think very much the standard in complexity theory.  Just imagine you'd have to account for all the index computations in a sorting algorithm...</p>",
        "id": 266166191,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640605067
    },
    {
        "content": "<p>Sure, we can't build in everything so that's inevitable</p>",
        "id": 266166266,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640605098
    },
    {
        "content": "<p>Right, and if it's inevitable we might as well accept it and use big-O everywhere.</p>",
        "id": 266166306,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640605146
    },
    {
        "content": "<p>but unfortunately I think that in practice we won't be able to use big-O nearly as much as the books would have you believe</p>",
        "id": 266166314,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640605161
    },
    {
        "content": "<p>lots of proofs require that you fix the big-O constant after the fact in some way that is not easily explained by any concrete definition of big-O</p>",
        "id": 266166384,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640605221
    },
    {
        "content": "<p>Seriously, the biggest issue with L in my eyes is with subpolynomial complexity.  I mean sure, you can prove that an algorithm has O(n log n) runtime <em>in L</em>, but I'm not sure how convincing that result is to somebody else.  Then there's the question of what LOGSPACE is in L.</p>",
        "id": 266166419,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640605297
    },
    {
        "content": "<p>Right. I think that for the question of what computational basis to use for our definitions, we should be looking at the tightest complexity classes, because those can sometimes distinguish the bases</p>",
        "id": 266166485,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640605370
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266166384\">said</a>:</p>\n<blockquote>\n<p>lots of proofs require that you fix the big-O constant after the fact in some way that is not easily explained by any concrete definition of big-O</p>\n</blockquote>\n<p>Do you have any concrete examples?</p>",
        "id": 266166486,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640605373
    },
    {
        "content": "<p>You generally need the existential constant to lie outside all variables, including parameters to the algorithm. This is generally very cumbersome to write, and it is easy to leave out a variable and then get stuck when you try to use the function recursively in a proof much later</p>",
        "id": 266166619,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640605537
    },
    {
        "content": "<p>I see what you mean (in a paper I would always interpret big-O to quantify over everything unless specified otherwise), but I don't think it means big-O is problematic.  It just means we always need to quantify over all arguments, i.e. <code>is_O (fun (m,n) =&gt; runtime (m + n)) (fun (m,n) =&gt; (m+n) * (m+n).log)</code>, which is indeed a bit verbose (but fixable).</p>",
        "id": 266167180,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640606216
    },
    {
        "content": "<p>Here's a more concrete example. Suppose we have a function like:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">def</span> <span class=\"n\">foo</span> <span class=\"o\">:</span> <span class=\"n\">Nat</span> <span class=\"bp\">-&gt;</span> <span class=\"n\">Nat</span>\n<span class=\"bp\">|</span> <span class=\"mi\">0</span> <span class=\"bp\">=&gt;</span> <span class=\"mi\">0</span>\n<span class=\"bp\">|</span> <span class=\"n\">n</span><span class=\"bp\">+</span><span class=\"mi\">1</span> <span class=\"bp\">=&gt;</span> <span class=\"k\">do</span> <span class=\"n\">f</span> <span class=\"n\">n</span><span class=\"bp\">;</span> <span class=\"n\">g</span> <span class=\"n\">n</span><span class=\"bp\">;</span> <span class=\"n\">foo</span> <span class=\"n\">n</span>\n</code></pre></div>\n<p>Let's suppose that <code>f n</code> is <code>O(n)</code> and <code>g n</code> is <code>O(n^2)</code>.If we wanted to keep big-O everywhere, we might want to argue like so:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>time(f n) is O(n)\ntime(g n) is O(n^2)\ntime(foo n) is O(n^3)\ntime(f n; g n; foo n) is O(n + n^2 + n^3) = O(n^3)\n</code></pre></div>\n<p>but of course this doesn't work because of the recursion. So we have to introduce an explicit constant:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>time(f n) is O(n)\ntime(g n) is O(n^2)\ntime(foo n) &lt;= c n^3\ntime(f n; g n; foo n) is O(n + n^2 + c n^3) &lt;= c (n+1)^3\n</code></pre></div>\n<p>but this also doesn't work because that last step isn't valid. The correct way to prove this is:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>time(f n) is O(n)\ntime(g n) is O(n^2)\ntime(f n) + time(g n) is O(n^2)\nSuppose time(f n) + time(g n) &lt;= d n^2\nPick c := d + 1 (or something)\nProve by induction: time(foo n) &lt;= c n^3\n\ntime(f n; g n; foo n) =\ntime(f n) + time(g n) + time(foo n) &lt;=\nd n^2 + (d+1) n^3 &lt;= (d+1) (n+1)^3\n</code></pre></div>\n<p>The big-O is really not helping here; if we just had explicit constants for everything, for example if we knew <code>time(f n) &lt;= 4 n</code> and <code>time(g n) &lt;= 37 n^2</code> then this proof would be a lot simpler.</p>",
        "id": 266167537,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640606557
    },
    {
        "content": "<p>Plus, you never know when a more precise bound might come in handy</p>",
        "id": 266167706,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640606740
    },
    {
        "content": "<p>The whole business about L seems to be geared towards various questions related to complexity of various λ-calculi. For the classical complexity theory this seems to be more of a sidetrack (which however might have technical benefits).</p>",
        "id": 266167799,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640606829
    },
    {
        "content": "<p>I really don't want to prove anything about church rosser here, that seems like entirely a sidetrack if the goal is P and NP</p>",
        "id": 266167880,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640606891
    },
    {
        "content": "<p>(To be fair, you don't need to take such a sidetrack if formalizing L; but it does seem likely to come up if you start thinking about compositionality in the lambda calculus)</p>",
        "id": 266167911,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640606957
    },
    {
        "content": "<blockquote>\n<p>if we just had explicit constants for everything</p>\n</blockquote>\n<p>Luckily, we can eliminate the big-O again if we need to fall back to explicit constants.  (Just like we can fall back to ε-δ-proofs if more general methods fail.)</p>",
        "id": 266168088,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640607123
    },
    {
        "content": "<p>Isn't the problem we see with big O here due to the true nature of big O being asymptotic? One needs to argue that O(n^k)&lt;O(n^{k+1}) as n-&gt;oo.</p>\n<p>(OK in sense of proving something is in P).</p>",
        "id": 266168128,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640607177
    },
    {
        "content": "<p>One sweeps these explicit constants into exponents, that's OK is only done fixed number to times.</p>",
        "id": 266168166,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640607238
    },
    {
        "content": "<blockquote>\n<p>Isn't the problem we see with big O here due to the true nature of big O being asymptotic?</p>\n</blockquote>\n<p>This doesn't make any real difference.  If <code>f n = O(g n)</code> as <code>n → ∞</code>, then there exists a <code>c</code> such that <code>f n ≤ c * (g n + 1)</code> and vice versa. (assuming f,g≥0)</p>",
        "id": 266168318,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640607383
    },
    {
        "content": "<p>I think it would be okay to have theorems of the form <code>\\exists N, \\all n &gt;= N, time(f n) &lt;= 37 n</code>, where we fix the <code>c</code> but not <code>N</code></p>",
        "id": 266168359,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640607463
    },
    {
        "content": "<p>since I don't think that <code>N</code> interferes with recursive proofs as much as <code>c</code> does</p>",
        "id": 266168402,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640607503
    },
    {
        "content": "<p>That's logically equivalent to a big-O so I'm not opposed.  But it's a bit unfortunate if we can't reuse the <code>is_O</code> definition.</p>",
        "id": 266168417,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640607531
    },
    {
        "content": "<p>is it? big-O requires a multiplicative constant</p>",
        "id": 266168434,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640607560
    },
    {
        "content": "<p>or do you mean something like <code>time(f n) - 37 n in O(1)</code></p>",
        "id": 266168462,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640607598
    },
    {
        "content": "<p>We can still have <code>is_O</code>-stated theorems for \"presentation\" purposes (and who knows, maybe they will be usable in some proofs) but for the really hard time complexity theorems I will expect us to skip those and go straight for the ones with algebraically more complicated but logically simpler bounds</p>",
        "id": 266168568,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640607704
    },
    {
        "content": "<blockquote>\n<p>The whole business about L seems to be geared towards various questions related to complexity of various λ-calculi.</p>\n</blockquote>\n<p>No, that's certainly not the intention.  The idea behind L is that it is much closer to a functional language (like Lean) in syntax and operational semantics, so you don't have to do as much encoding as with Turing machines.  Like Mario is saying, a lot of basic operations (like applying a function or reading the first element of a list, etc.) take exactly one step in L, but can take several or even many steps in a Turing machine because of copying, skipping, etc.</p>",
        "id": 266168569,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640607704
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266168568\">said</a>:</p>\n<blockquote>\n<p>We can still have <code>is_O</code>-stated theorems for \"presentation\" purposes (and who knows, maybe they will be usable in some proofs) but for the really hard time complexity theorems I will expect us to skip those and go straight for the ones with algebraically more complicated but logically simpler bounds</p>\n</blockquote>\n<p>I'm not sure I understand you.  Are you saying that <code>37</code> is better than <code>c</code>, because presumably <code>norm_num</code> can simplify it?  While I recognize the pain, I feel like the solution here should be better automation, and not theorems with hard-coded bounds.</p>",
        "id": 266168758,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640607861
    },
    {
        "content": "<p>Not (just) because norm_num can work with it, but also because we don't need to deal with existential elimination and the resulting scoping. It's a lot easier to automate such a proof if everything is in the empty context</p>",
        "id": 266168809,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640607934
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266168434\">said</a>:</p>\n<blockquote>\n<p>is it? big-O requires a multiplicative constant</p>\n</blockquote>\n<p>Yes, but for the purposes of proving something in P, one can replace a constant by a sufficiently slowly growing function, e.g. <code>n</code>, or <code>log n</code>.</p>",
        "id": 266168896,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640608019
    },
    {
        "content": "<p>So <code>Σ' c, time (f n) ≤ c * n^2</code> would be ok then?  (No elimination, no scope.)  If you allow classical logic, you can also avoid the existential elimination with an epsilon.</p>",
        "id": 266168910,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640608031
    },
    {
        "content": "<p>I think that still suffers from the scoping over parameters problem. The normal way to write that would imply that <code>c</code> can depend on anything in scope, whereas if it is an explicit <code>37</code> then you know it doesn't. I think lean might still be able to work with it if it can unfold the value to <code>37</code>, but it can still possibly trigger scoping issues if it is not reduced</p>",
        "id": 266169075,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640608221
    },
    {
        "content": "<p>Another way to hide the argument is to have <code>time_f_constant := 37</code> and then <code>time (f n) ≤ time_f_constant * n^2</code>, where you can control the parameters needed in <code>time_f_constant</code></p>",
        "id": 266169104,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640608308
    },
    {
        "content": "<p>Maybe I'm naive, but is this so much easier than <code>rcases time_f_spec with ⟨time_f_constant, N, time_f_le⟩</code> at the beginning of the proof?</p>",
        "id": 266169247,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1640608451
    },
    {
        "content": "<p>I guess I'm talking about completely eliminating big-O, in the sense that O(n)&lt;n^2 as n-&gt;oo, etc. This works for showing something in P.</p>",
        "id": 266169293,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640608516
    },
    {
        "content": "<p>For showing things are in P, I think you can avoid constants altogether and instead use closure properties for most things</p>",
        "id": 266169362,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640608593
    },
    {
        "content": "<p>The big-O stuff only comes up for tighter bounds like sorting in O(n log n)</p>",
        "id": 266169378,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640608621
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span> Maybe. I haven't worked it all out, so consider me moderately dubious but willing to try things and see what sticks</p>",
        "id": 266169485,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640608714
    },
    {
        "content": "<p>This is a subject I'm very interested in and I will try to follow the discussions as hard as I can.</p>\n<p>Can we have a separate stream <code>complexity theory</code> for it? It's getting more and more laborious to backtrack every discussion made about this topic. Different topics are spread over different streams and mixed/smashed into same threads.</p>\n<p>Sorry for my ignorance, but what's the core idea for computing time and space complexity formally? The proofs that I've done/seen all my entire life about these were extremely informal, like counting loops inside loops and making some logical connection with the input size.</p>",
        "id": 266173538,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640612754
    },
    {
        "content": "<p>Streams are usually made to divert attention away from a subject which isn't interesting to most people. Simply cut that thread into smaller ones if you feel like you can't follow the conversation.</p>",
        "id": 266173637,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1640612880
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"316505\">Dima Pasechnik</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266132817\">said</a>:</p>\n<blockquote>\n<p>Perhaps Fagin's equivalence of NP and sentences in existential 2nd order logic is something that  should be more prominent. (In this vein, polynomial-time solvability becomes definability in the 1st order logic).</p>\n</blockquote>\n<p>As a definition? Or an additional theorem about our classes?</p>",
        "id": 266191969,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640627885
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"451983\">Arthur Paulino</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266173538\">said</a>:</p>\n<blockquote>\n<p>Sorry for my ignorance, but what's the core idea for computing time and space complexity formally? The proofs that I've done/seen all my entire life about these were extremely informal, like counting loops inside loops and making some logical connection with the input size.</p>\n</blockquote>\n<p>The basic idea is pretty simple: count all the basic steps evaluated by a program. You can very often get exact formulas for this, like <code>2 n^2 + 3 x - 7</code>; formally this is often an easier task than doing big-O analysis, although it can start to help once the formulas get too complicated. Once the tools to state such theorems are in place, the proofs are pretty trivial, it usually amounts to proofs like <code>\\sum i &lt; n, 1 + 2 + (3 i - 1) = O(n^2)</code>.</p>",
        "id": 266226072,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640660063
    },
    {
        "content": "<p>For space complexity, I'm not sure what the best setting is. Probably we want a computational model with a finite memory like an FSM (but with parametric memory size <code>k</code> possibly depending on the input <code>n</code>, instead of <code>O(1)</code>), and then a space complexity bound talks about whether such a limited model is capable of computing the desired function.</p>",
        "id": 266226182,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1640660237
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266191969\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"316505\">Dima Pasechnik</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266132817\">said</a>:</p>\n<blockquote>\n<p>Perhaps Fagin's equivalence of NP and sentences in existential 2nd order logic is something that  should be more prominent. (In this vein, polynomial-time solvability becomes definability in the 1st order logic).</p>\n</blockquote>\n<p>As a definition? Or an additional theorem about our classes?</p>\n</blockquote>\n<p>either way would be useful - IMHO it's a very convenient way to think about the class NP, as it does not invoke weird stuff such as nondeterministic Turing machines.</p>",
        "id": 266339123,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640778898
    },
    {
        "content": "<p>What is the \"natural\" NP-complete problem in this formalism?</p>",
        "id": 266340713,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640780338
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266340713\">said</a>:</p>\n<blockquote>\n<p>What is the \"natural\" NP-complete problem in this formalism?</p>\n</blockquote>\n<p>the usual, e.g. SAT. For instance, see <a href=\"https://hal.archives-ouvertes.fr/hal-00017602/document\">https://hal.archives-ouvertes.fr/hal-00017602/document</a></p>",
        "id": 266341709,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640781375
    },
    {
        "content": "<p>this brings up the question of a \"natural reduction\" - in the classical theory, Karp vs Turing reductions. Here we have \"1st order reductions\", as well.</p>",
        "id": 266341917,
        "sender_full_name": "Dima Pasechnik",
        "timestamp": 1640781576
    },
    {
        "content": "<p>I will be free to start working on that after my Qualifying Exam on 2022-01-25. However, I'd need a lot of guidance in order to succeed in this task.</p>",
        "id": 266416089,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1640857706
    },
    {
        "content": "<p>I am super curious to see how it would be built up in Lean <span aria-label=\"eyes\" class=\"emoji emoji-1f440\" role=\"img\" title=\"eyes\">:eyes:</span></p>",
        "id": 266506605,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1640961356
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266114221\">said</a>:</p>\n<blockquote>\n<p>(1) Define the notion of a decision problem in the weak call-by-value λ-calculus L.<br>\n(2) Define the class P in L.<br>\n(3) Define the class NP in L.<br>\n(4) Define polytime reductions in L.<br>\n(5) Prove that, if A polytime reduces to B in P, then A in P.<br>\n(6) Define the classes NP-hard and NP-complete in L.<br>\n(7) Define a natural NP-complete problem in L.<br>\n(8) Prove that, if NP-hard A polytime reduces to B, then B is NP-hard.<br>\n(9) Define the Abstract heap machines and Turing machines — just that we can state the auxiliary decision problems about them; we will not program in them; the reductions will be always programmed in L.<br>\n(10) State and prove the polytime reductions (very challenging).<br>\n(11) State your favourite version of the tiling problem.<br>\n(12) Prove the NP-hardness of the tiling problem using the tools above (the vanilla TM will have to be reduced to it).<br>\n(13) Reduce your tiling problem to some form of SAT.<br>\n(14) We can continue building the complexity theory from here on without getting our hands dirty with the intricacies of various computational models.</p>\n</blockquote>\n<p>Hi, I'm one out the authors of the Cook-Levin mechanisation in Coq.</p>\n<p>I think you are missing (or at last not explicitly acknowledging) the part that an 'time-invariant' simulation of L in terms of Turing machines is needed (if the model of computations is to be choosen L): As NP-complete talks more or less about simulating all poly-time verifiers  with a single problem, one needs to transform the L-algorithms into TMs, i.e. implement the abstract heap machine as TM. (Or is that the part (10)?)<br>\nThis part is quite hard: We developed an framework to verify Turing machines (See our <a href=\"https://www.ps.uni-saarland.de/Publications/details/ForsterEtAl:2019:VerifiedTMs.html\">publication with Wuttke</a>).</p>",
        "id": 266794676,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641294123
    },
    {
        "content": "<p>On a more general note:</p>\n<p>Our experience there is also why we think that explicitly constructing TMs (or other low-level models of computation, like mu-recursive functions) just does not scale: There is so much overhead, like\"how do I encode different types of data? (lists, terms, programms...)\", or \"How to verify simple composition of functions and other straightforward, non-recursive (or \"non-loop-containing\") programs without to much effort?\" and of course \"How can I verify loops/recursion\"?<br>\nMaybe, with a lot of engineering work, one can come up with a scaling verification framework, but our conclusion is that functional programming is such a sweet-spot in ITPs that one would waste time not using a functional language whenever possible.</p>\n<p>It might be very much possible to support sublinear space in a lambda calculus, for example by introducing oracles for the input. No matter the model of computation, it seems that sublinear space will make it a lot more tedious to have composability of programs, as one can not use the same space measure for a program \"on top level\" and \"a program called as a subroutine of another program\".</p>",
        "id": 266794692,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641294127
    },
    {
        "content": "<p>My judgement is that 1,2,3,4,6,7,9 are trivial.<br>\n5 and 8 are very easy once one has a basic understanding on how to compose programs.</p>\n<p>Proving that (7) is indeed NP-complete of course needs an universal L-program that runs in poly-time, for instance implementing the heap machine in L. With a good verification framework for the model of computation (in this case, L), this is not very hard, but without a good framework, this is very, very tedious, and maybe even unfeasible.</p>\n<p>(I commented on 10 above, if that is the L-to-TM-reduction, i.e. the implementation of an L-interpreter as a TM. It took us a very talented student with an excellent bachelors thesis on this topic to come up with a verification framework for TMs, and after his thesis, it still was the work of over 100 hours of work to implement and verify the TMs needed for this step. )</p>\n<p>11 and 12 were also tackled in a bachelor thesis with another extraordinary student, and needed a lot of proof Engineering to solve the many, many cases that can occur when simulating a computation with a tiling problem. </p>\n<p>13 again is trivial, assuming one can write and verify basic programs in the model of computation (of course, as always, assuming one also can verify running times).</p>",
        "id": 266796135,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295008
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"468318\">@Fabian Kunze</span> I'm curious whether by \"TM\" you mean in the strict sense defined by turing, or a more type-level programming friendly version. I think that while TMs construed literally are not good for verification, it is possible to get a more abstract model with user-chosen state spaces to do TM proofs without as much pain. I think that it is possible to get these looking something like WHILE programs or finite state machines with O(1) programs in between, which seems like a good balance between the native logic part used to write the programs, and the explicit step representation you need to do time and space bounds (within a constant factor).</p>",
        "id": 266796165,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295042
    },
    {
        "content": "<p>Yes, we abstract away the TM behind while-program like \"combinators\" that construct TMs in the background:<br>\n\"The Turing machine verification framework allows giving algorithms in the style of a<br>\nregister-based while-language, and a corresponding machine is automatically constructed<br>\nbehind the scenes. Separate correctness and verification proofs are then inclusion proofs<br>\nbetween the automatically derived and the user-given relations for the constructed machine\"</p>\n<p>But there still is the problem that the \"registers\" contain asingle tape/strings, and that one might want to change the alphabet when composing machines/</p>",
        "id": 266796407,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295233
    },
    {
        "content": "<p>As an example of what can be done with such a framework, see the program <a href=\"https://github.com/leanprover-community/mathlib/blob/master/src/computability/tm_to_partrec.lean#L837-L887\"><code>tm_to_partrec</code></a> and surrounding section. That one is implemented in a multi-stack machine, but if you have ideas for a more abstract model that still supports some nice operations I'm all ears</p>",
        "id": 266796454,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295293
    },
    {
        "content": "<p>You <em>can</em> change the alphabet when composing machines</p>",
        "id": 266796468,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295313
    },
    {
        "content": "<p>Ah. I think, based on your description, that one difference between our formalisms is that the mathlib one actually changes the model of computation rather than using combinators over the original model (and there are general theorems about reduction from one model to another)</p>",
        "id": 266796621,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295424
    },
    {
        "content": "<p>Yes, we change the alphabet, it is all possible, and we are quite happy with our final, hoare-like verification framework that hides most, or even all of those details.</p>",
        "id": 266796630,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295430
    },
    {
        "content": "<p>Yes, we do only have a shallow representation of the while language, not a deep one. One aspect was that this allows to add new data typ[es (lists of other things, for example) on the fly, as long as one person wrote the \"basic\" combinators to interact with that new data.</p>",
        "id": 266796746,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295500
    },
    {
        "content": "<p>Partrec seems to be very close to L in my eyes by the way, and I have no doubt that this approach can work.</p>",
        "id": 266796861,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295578
    },
    {
        "content": "<p>That's my impression as well. I think that <code>tm_to_partrec</code> automaton is a rough equivalent of your L-to-TM reduction</p>",
        "id": 266796904,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295630
    },
    {
        "content": "<p>Is the theoretical problem of coming up with a time measure for partrec-functions that is invariant w.r.t. turing machines solved?</p>",
        "id": 266796938,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295676
    },
    {
        "content": "<p>what do you mean by invariant?</p>",
        "id": 266796990,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295694
    },
    {
        "content": "<p>that the TM and part rec function simulate wach other with just polynomial overhead</p>",
        "id": 266797012,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295714
    },
    {
        "content": "<p>probably, count all mu-rec-loop-executions, is it?</p>",
        "id": 266797031,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295725
    },
    {
        "content": "<p>As long as you can accept a O(n) slowdown (e.g. anything P or larger) then you can do the simulation</p>",
        "id": 266797044,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295734
    },
    {
        "content": "<p>the slowdown of implementing mu-rec programs in TMs is only linear?</p>",
        "id": 266797086,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295784
    },
    {
        "content": "<p>sorry, other way arround]</p>",
        "id": 266797103,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295799
    },
    {
        "content": "<p>We don't have the actual time bounds proved yet, but the current setup should have O(1) from TM0 to TM1, O(n) slowdown for TM2 from TM1, and O(log n) slowdown for partrec from TM2</p>",
        "id": 266797149,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295811
    },
    {
        "content": "<p>TM2 is multi-stack machines</p>",
        "id": 266797165,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295827
    },
    {
        "content": "<p>Ah, nice.</p>",
        "id": 266797191,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641295850
    },
    {
        "content": "<p>Well, to be precise the last step is not partrec, it is <a href=\"https://github.com/leanprover-community/mathlib/blob/master/src/computability/tm_to_partrec.lean#L68-L114\">this model</a> which is proven equivalent to the partrec model but has a more explicit cost model</p>",
        "id": 266797392,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641295987
    },
    {
        "content": "<p>partrec itself is mostly only focused on computability in the abstract, some of those algorithms are really bad (like double exponential) if run literally</p>",
        "id": 266797448,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296041
    },
    {
        "content": "<p>but that's why <a href=\"https://github.com/leanprover-community/mathlib/blob/master/src/computability/partrec.lean#L149-L159\">partrec</a> is defined as a Prop with no computational content, you shouldn't be extracting an actual algorithm from it</p>",
        "id": 266797570,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296115
    },
    {
        "content": "<p>The TM2 model looks interesting. Is there documentation/intuition on what kind of types are allowed as elements on the stacks, and what kind of \"elementary\" operations are permitted?</p>",
        "id": 266797586,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641296131
    },
    {
        "content": "<p>The stack element can be any finite type</p>",
        "id": 266797598,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296146
    },
    {
        "content": "<p>The elementary operations are basic operations on stacks <a href=\"https://github.com/leanprover-community/mathlib/blob/master/src/computability/turing_machine.lean#L1765-L1790\">here</a></p>",
        "id": 266797707,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296206
    },
    {
        "content": "<p>and sigma is a finite type, i assume?</p>",
        "id": 266797741,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641296240
    },
    {
        "content": "<p>yes, that's the \"local state\"</p>",
        "id": 266797762,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296253
    },
    {
        "content": "<p>in the simulation using basic TMs the basic states are pairs of a local state value and a label</p>",
        "id": 266797892,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296338
    },
    {
        "content": "<p>Very nice abstractions, in hindsight, more transformations when going from single-tape to multi-tape TMs would have made our life simpler.</p>",
        "id": 266797901,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641296345
    },
    {
        "content": "<p>One of the things I like a lot about the TM models is that the type <code>Λ</code> of \"labels\" (aka TM states) is <em>not</em> required to be finite, but rather finitely supported (only a finite number of labels are reachable from any given label). That means that label types can be simple infinite inductive types like <a href=\"https://github.com/leanprover-community/mathlib/blob/master/src/computability/tm_to_partrec.lean#L761-L769\">this one</a>, and we prove separately that for any starting point of interest there is a finite set of reachable states from it. In that example, we have a single label space covering all partrec programs, and finiteness is guaranteed by the fact that you only visit programs corresponding to subterms of the original program</p>",
        "id": 266798473,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641296821
    },
    {
        "content": "<p>We are quite happy with the fact that in our hoare-style framework, we only need one lemma for each total TM, following the execution once, to get all properties of the tm we need (and the lemmas for the Hoare rules handle the well-formatted-conditions in the background.)</p>",
        "id": 267042801,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641463054
    },
    {
        "content": "<p>what properties are those? Functional correctness and a time bound? Do you have an example?</p>",
        "id": 267045017,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1641464512
    },
    {
        "content": "<p>Yes, the specification of functional correctness and the running time. Well-formedness is already \"baked in\" into the type of turing machines, for instance that the alphabet is finite etc.</p>",
        "id": 267603681,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1641917273
    },
    {
        "content": "<p>Is anybody going to work on the implementation in foreseeable future?</p>",
        "id": 268176724,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642328277
    },
    {
        "content": "<p>I am going to continue working on it in <a href=\"https://github.com/leanprover-community/mathlib/pull/11046\">#11046</a>. Notwithstanding all the discussion here, I'm just going to start by trying to finish a function <code>time</code> with the same type as <a href=\"https://leanprover-community.github.io/mathlib_docs/find/nat.partrec.code.eval\">docs#nat.partrec.code.eval</a>, I'm not sure how long this will take me, but it seems like the best combination of achievability and forward progress to me.</p>",
        "id": 268204371,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1642363982
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266226072\">said</a>:</p>\n<blockquote>\n<p>The basic idea is pretty simple: count all the basic steps evaluated by a program. You can very often get exact formulas for this, like <code>2 n^2 + 3 x - 7</code>; formally this is often an easier task than doing big-O analysis, although it can start to help once the formulas get too complicated. Once the tools to state such theorems are in place, the proofs are pretty trivial, it usually amounts to proofs like <code>\\sum i &lt; n, 1 + 2 + (3 i - 1) = O(n^2)</code>.</p>\n</blockquote>\n<p>What if you have, say, <code>f(a, b) = a + b</code>, would you say that this function is <code>O(a)</code> (or <code>O(b)</code> depending on how addition is defined) or <code>O(1)</code>?</p>",
        "id": 268338141,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1642475469
    },
    {
        "content": "<p>It depends on whether that is a primitive operation or not. If it is primitive, then it is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>. For a lot of purposes it might be reasonable to take it as such; the Word RAM model of computation allows addition to be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>, although the addends must be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(w)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span> bits where w, the \"word size\", is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ω</mi><mo stretchy=\"false\">(</mo><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Omega(\\log n)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Ω</span><span class=\"mopen\">(</span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">)</span></span></span></span></p>",
        "id": 268339480,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642477128
    },
    {
        "content": "<p>If addition is not primitive, then I would expect <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>log</mi><mo>⁡</mo><mi>a</mi><mo>+</mo><mi>log</mi><mo>⁡</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(\\log a + \\log b)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mclose\">)</span></span></span></span></p>",
        "id": 268339507,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642477173
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268339480\">said</a>:</p>\n<blockquote>\n<p>It depends on whether that is a primitive operation or not. If it is primitive, then it is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>. For a lot of purposes it might be reasonable to take it as such; the Word RAM model of computation allows addition to be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>, although the addends must be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(w)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span> bits where w, the \"word size\", is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ω</mi><mo stretchy=\"false\">(</mo><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Omega(\\log n)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Ω</span><span class=\"mopen\">(</span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">)</span></span></span></span></p>\n</blockquote>\n<p>Why Omega?</p>",
        "id": 268748415,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642708603
    },
    {
        "content": "<p>Could this Isabelle/HOL project be used as inspiration for us?<br>\n<a href=\"https://github.com/wimmers/poly-reductions\">https://github.com/wimmers/poly-reductions</a></p>\n<p>I don't know whether I understand what they did. They have the polytime reductions. Unfortunately, they don't have any problem proved to be NP-complete yet. Nevertheless, it is a part of their plan and their framework should be capable of that. Am I right?</p>",
        "id": 268752385,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642710428
    },
    {
        "content": "<p>I think it's usually stated as an inequality <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>≤</mo><msup><mn>2</mn><mi>w</mi></msup></mrow><annotation encoding=\"application/x-tex\">n\\le 2^w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7719em;vertical-align:-0.136em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6644em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span></span></span></span></span></span></span>, the asymptotics of it don't matter so much. It's a hypothesis, but a fairly reasonable one for word RAM: it's saying that the entire problem fits in the address space of the machine</p>",
        "id": 268756914,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642712449
    },
    {
        "content": "<p>For a regular computer, this is saying that 64-bit machines can work on problems of size at most 2^64</p>",
        "id": 268756985,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642712483
    },
    {
        "content": "<p>For asymptotic complexity results, this is convenient because it means that as <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> grows so does <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span>, so you can do larger operations in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span></p>",
        "id": 268757163,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642712561
    },
    {
        "content": "<p>But can we choose word size to be much higher than Omega(log n) to reduce the time complexity in some cases?</p>",
        "id": 268757179,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642712566
    },
    {
        "content": "<p>Yes, but the complexity bounds are stated as a function of both <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> in this case</p>",
        "id": 268757274,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642712615
    },
    {
        "content": "<p>Oh!</p>",
        "id": 268757316,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642712635
    },
    {
        "content": "<p>For a really intense application of word RAM capabilities check out <a href=\"https://en.wikipedia.org/wiki/Fusion_tree\">fusion trees</a></p>",
        "id": 268757324,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642712640
    },
    {
        "content": "<p>After thinking about the partial recursive implementation more, I'm not sure this approach actually works. If we are working over the naturals and the only primitive operation for mutating a natural is the successor function, it seems like there's no way for addition to be O(log n), since to construct <code>a + b</code> from <code>a</code> and <code>b</code>, you have to apply the successor at least <code>(b - a)</code> times. It also seems like you can't represent a natural in binary, since if you fix the number of registers you need for a natural at k, you can represent at most <code>t ^ k</code> numbers in <code>O(t)</code> time. I may need to rethink the approach.</p>",
        "id": 268758422,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1642713134
    },
    {
        "content": "<p>Basic question, possibly very stupid:</p>\n<p>What is the difference between the partially-recursive functions and the call-by-value lambda-calculus, please?</p>",
        "id": 268759113,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642713445
    },
    {
        "content": "<p>That's a good question. I was a bit confused when <span class=\"user-mention\" data-user-id=\"468318\">@Fabian Kunze</span> said that mu-recursive functions wouldn't scale, after saying that the call-by-value lambda calculus was what he and his co-authors used, since I would have said that those models of computation are similar, or at least that it wouldn't be too hard to prove slowdown lemmas about one with respect to the other. Perhaps he or someone else can elaborate on the differences, or if this is indeed difficult?</p>",
        "id": 268759971,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1642713823
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span> This is why you probably shouldn't define <code>time</code> based on the partrec evaluation rules. It's only intended for defining computability, not complexity, and there are several reductions involved that are exponential or double exponential</p>",
        "id": 268760906,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642714228
    },
    {
        "content": "<p>Any of the TM-based computation models should all be polynomially equivalent: <code>TM0</code>, <code>TM1</code>, <code>TM2</code>. You can also probably code binary in the <a href=\"https://leanprover-community.github.io/mathlib_docs/find/turing.to_partrec.code\">docs#turing.to_partrec.code</a> model, but the built in nats in that model are unary</p>",
        "id": 268761501,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642714465
    },
    {
        "content": "<p>It occurs to me that if only the naturals were represented in binary and I could do an O(1) doubling or halving operation on them, I would have stacks and one could use multiple stacks to implement a Turing machine, so I imagine time in this model of computation would be within a polynomial factor of the usual definition.</p>",
        "id": 268761503,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1642714466
    },
    {
        "content": "<p>Take a look at <a href=\"https://leanprover-community.github.io/mathlib_docs/computability/tm_to_partrec.html#simulating-sequentialized-partial-recursive-functions-in-tm2\">https://leanprover-community.github.io/mathlib_docs/computability/tm_to_partrec.html#simulating-sequentialized-partial-recursive-functions-in-tm2</a>, that's exactly what it does</p>",
        "id": 268761708,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642714561
    },
    {
        "content": "<p>There are computational models similar to primitive recursive functions which compute functions in P, you have to switch out the <code>prec</code> constructor with \"recursion on notation\" aka <a href=\"https://leanprover-community.github.io/mathlib_docs/find/nat.binary_rec\">docs#nat.binary_rec</a></p>",
        "id": 268762107,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642714731
    },
    {
        "content": "<p>Regarding the observation with \"not enough space to madd numbers\":<br>\nThat exactly is the complication, function composition does not behave like before, where one cold just concatenate the resource consumption functions.</p>\n<p>For TMs, one approach is to have input (and output) tapes, on which one can only read (or write and move one step to the right). I have not looked into it that deep, but as far as I understood,  you can arrange things such that one can compose \"online\" algorithms, for instance one can think of addition as a function that traverses two binary representations and, on the fly,. returns the stream of bits of the sum. If the function called on that result directly \"consumes\" the output (or, more explicitly, pauses and resumes the computation of the bits as needed), one can add numbers inside an log(n) algorithm.</p>\n<p>Another approach (which I think is more common) is to represent numbers not as numbers, but as algorithms that, given an index i, return the i-the bit of the number. This allows to represent numbers of size up to n in log(n) space (But only a constant amount of them, because the code has to exists in the program itself). As long as one is only interested in space, this very inefficient representation does work.</p>\n<p>But this inability of writing down intermediate results explicitly makes formalizations of space really complicated.</p>",
        "id": 268804964,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1642750703
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282271\">Bolton Bailey</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268759971\">said</a>:</p>\n<blockquote>\n<p>That's a good question. I was a bit confused when <span class=\"user-mention silent\" data-user-id=\"468318\">Fabian Kunze</span> said that mu-recursive functions wouldn't scale, after saying that the call-by-value lambda calculus was what he and his co-authors used,</p>\n</blockquote>\n<p>I'm sorry, I was inconsitent: when I said \"[I have no doubts that] partrec [] can work\", this was meant as a retraction of my previous statement that I was sceptical using murec for time-complexity. I used those two names, partrec and murec, interchangeably, as they mean the same.</p>\n<p>The one complication I see is that without an abstraction layer for encodings (gödel-numbers?), the statements and programs get a bit messy when programming an universal machine. With the lambda calculus, we more or less get any Algebraic data type \"for free\" using Scott encodings (wich are similar to church encodings, but vor cbv-languages).</p>",
        "id": 268805372,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1642751130
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266165642\">said</a>:</p>\n<blockquote>\n<p>The idea behind the haskell-ish intermediate language</p>\n</blockquote>\n<p>Do you have any more concrete thoughts how a haskell-ish intermediate language might look Mario?</p>",
        "id": 268861857,
        "sender_full_name": "Yannick Forster",
        "timestamp": 1642780589
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/266797149\">said</a>:</p>\n<blockquote>\n<p>We don't have the actual time bounds proved yet, but the current setup should have O(1) from TM0 to TM1, O(n) slowdown for TM2 from TM1, and O(log n) slowdown for partrec from TM2</p>\n</blockquote>\n<p>Do you see a chance to find a good notion of space complexity for <code>partrec</code>?</p>",
        "id": 268862026,
        "sender_full_name": "Yannick Forster",
        "timestamp": 1642780660
    },
    {
        "content": "<p>I think the most natural thing would be to use a computational model which is similar in structure to <code>partrec</code> but uses binary strings as the basic data type instead of <code>nat</code></p>",
        "id": 268898755,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1642796783
    },
    {
        "content": "<p>Ok, thanks Fabian for responding, I guess I'll start thinking about a binary-string <code>partrec</code>.</p>",
        "id": 268906109,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1642800788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"444411\">Yannick Forster</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268862026\">said</a>:</p>\n<blockquote>\n<p>Do you see a chance to find a good notion of space complexity for <code>partrec</code>?</p>\n</blockquote>\n<p>I think that space-complexity for <code>partrec</code> can look very similar to time-complexity for <code>partrec</code> in structure. Instead of <code>time (comp f g) = time f + time g</code> we have something like <code>space (comp f g) = max (space f) (space g)</code> (with appropriate inputs to <code>f</code> and <code>g</code>).</p>",
        "id": 268906443,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1642800979
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"468318\">Fabian Kunze</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268804964\">said</a>:</p>\n<blockquote>\n<p>For TMs, one approach is to have input (and output) tapes, on which one can only read (or write and move one step to the right).</p>\n</blockquote>\n<p>Would the TM always have a constant amount of tapes?</p>",
        "id": 268911193,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642803644
    },
    {
        "content": "<p>Would the number of tapes be baked into the TM type?</p>",
        "id": 268912072,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1642804144
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268898755\">said</a>:</p>\n<blockquote>\n<p>I think the most natural thing would be to use a computational model which is similar in structure to <code>partrec</code> but uses binary strings as the basic data type instead of <code>nat</code></p>\n</blockquote>\n<p>What makes <code>partrec</code> tedious in my opinion is that you don't have a lambda, so all programs are combinatorial and passing around input is not as easy as in functional programming languages. But, of course, with a lambda you buy into the harder simulation proof. And why restrict to binary strings? Intuitively I'd rather argue for binary trees (like in the computability book by Jones. On top of that I would imagine that one can build an even more expressive simply typed language with natural numbers, lists, options and trees (which is immediately reduced to the simpler case with just one base type)</p>",
        "id": 268960216,
        "sender_full_name": "Yannick Forster",
        "timestamp": 1642861481
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"282271\">Bolton Bailey</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268906443\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"444411\">Yannick Forster</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/268862026\">said</a>:</p>\n<blockquote>\n<p>Do you see a chance to find a good notion of space complexity for <code>partrec</code>?</p>\n</blockquote>\n<p>I think that space-complexity for <code>partrec</code> can look very similar to time-complexity for <code>partrec</code> in structure. Instead of <code>time (comp f g) = time f + time g</code> we have something like <code>space (comp f g) = max (space f) (space g)</code> (with appropriate inputs to <code>f</code> and <code>g</code>).</p>\n</blockquote>\n<p>It's not obvious to me that this works. (But of course it would be great if it would!) To implement on a Turing machine, you'd probably need some kind of environment implemented with pointers, and pointers can grow linear-logarithmically instead of just linearly. At least, that's what happens with lambda calculus and why for e.g. the full lambda calculus space complexity is still an open problem. For RAM machines, defining space is also way more subtle  than one might think (that was the content of the first paper by Slot and van Emde Boas, where they came up with the conjecture that all reasonable models of computation can simulate each other with polynomial overhead in time and constanct-factor (i.e. linear) overhead in space</p>",
        "id": 268960371,
        "sender_full_name": "Yannick Forster",
        "timestamp": 1642861714
    },
    {
        "content": "<p>Hey <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> , I'm noticing that the definition of <a href=\"https://leanprover-community.github.io/mathlib_docs/find/turing.to_partrec.code.eval\">docs#turing.to_partrec.code.eval</a> for <code>code.fix</code> could be changed to<br>\n<code>| (code.fix f) := pfun.fix $ λ v, pure (if v.head = 0 then sum.inl v.tail else sum.inr v.tail)</code><br>\nWhich is shorter than what is there currently. I believe this essentially changes it from a \"while\" to a \"do-while\" semantics. I think this doesn't change the model too much and might be cleaner in the long run (if I can fix what breaks in this file). Is there a reason I'm missing for it to be defined the way we have it?</p>",
        "id": 270335764,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1643778935
    },
    {
        "content": "<p>you don't use <code>f</code> in that snippet?</p>",
        "id": 270335855,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1643779031
    },
    {
        "content": "<p>Uhh, oops, you're right, just a sec</p>",
        "id": 270335878,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1643779079
    },
    {
        "content": "<p>I think this is what I meant:<br>\n<code>| (code.fix f) := pfun.fix $ λ v, if v.head = 0 then pure (sum.inl v.tail) else ((f.eval v.tail).map sum.inr) </code></p>",
        "id": 270336176,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1643779322
    },
    {
        "content": "<p>I guess it doesn't end up that much shorter</p>",
        "id": 270336426,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1643779539
    },
    {
        "content": "<p>I think we should establish some regular Zoom meetings on formalizing complexity theory in Lean.<br>\nMany of us are interested in having such a library, but it will require a lot of collaboration which seems that we haven't started yet.</p>",
        "id": 318138491,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672164473
    },
    {
        "content": "<p>I suggest we start in the 3rd week of 2023. Indicate your time preferences please:<br>\n<a href=\"https://doodle.com/meeting/participate/id/ep84nEQe\">https://doodle.com/meeting/participate/id/ep84nEQe</a></p>",
        "id": 318236300,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672226471
    },
    {
        "content": "<p>Also note that graph algorithms are of interest to me, if you happen to need one at some point.</p>",
        "id": 318241892,
        "sender_full_name": "Yaël Dillies",
        "timestamp": 1672228763
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/318236300\">said</a>:</p>\n<blockquote>\n<p>I suggest we start in the 3rd week of 2023. Indicate your time preferences please:<br>\n<a href=\"https://doodle.com/meeting/participate/id/ep84nEQe\">https://doodle.com/meeting/participate/id/ep84nEQe</a></p>\n</blockquote>\n<p>I signed up but mostly because this has been a topic of interest for me for a while, not necessarily because I'm actually competent enough to contribute something from a purley mathematical POV so if my vote ends up being a deal breaker just ignore me.</p>",
        "id": 318250885,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1672232217
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"395550\">Henrik Böving</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/318250885\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/318236300\">said</a>:</p>\n<blockquote>\n<p>I suggest we start in the 3rd week of 2023. Indicate your time preferences please:<br>\n<a href=\"https://doodle.com/meeting/participate/id/ep84nEQe\">https://doodle.com/meeting/participate/id/ep84nEQe</a></p>\n</blockquote>\n<p>I signed up but mostly because this has been a topic of interest for me for a while, not necessarily because I'm actually competent enough to contribute something from a purley mathematical POV so if my vote ends up being a deal breaker just ignore me.</p>\n</blockquote>\n<p>Complexity theory is broad enough as a subject that it is unlikely that  everyone knows all the relevant math beforehand, beyond the basic stuff.</p>",
        "id": 318277684,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1672242011
    },
    {
        "content": "<p>My knowledge ends at \"There is P and NP and there is a couple of fun reductions to SAT  you can do to show things are in NP\" :P</p>",
        "id": 318282831,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1672243937
    },
    {
        "content": "<p>:O I'm very interested as well! It seems I can't sign up for the meeting though. May I have an invite?</p>",
        "id": 318353815,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1672283986
    },
    {
        "content": "<p>Everyone is invited! Just vote in the poll, please.</p>",
        "id": 318371141,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672300292
    },
    {
        "content": "<p>Oh apologies I was doing something wrong -- I've signed up now</p>",
        "id": 318374104,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1672302105
    },
    {
        "content": "<p>No need to apologize. Just clarify, please, did you just indicate that only one time works for you?</p>",
        "id": 318374286,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672302195
    },
    {
        "content": "<p>Doodle asks every participant for a subset of time slots.</p>",
        "id": 318374441,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672302273
    },
    {
        "content": "<p>Don't forget to record the meetings and post on YouTube.</p>",
        "id": 318377629,
        "sender_full_name": "Patrick Johnson",
        "timestamp": 1672303922
    },
    {
        "content": "<p>Is it necessary?</p>",
        "id": 318377896,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672304034
    },
    {
        "content": "<p>Recording meetings makes many people afraid of embarrassing themselves.</p>",
        "id": 318378188,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672304156
    },
    {
        "content": "<p>Participants may request to remove any problematic part of the video before posting on youtube, but it will rarely happen. Most Lean meetings are public. See for example mathlib4 porting meetings at leanprover community youtube channel. People who can't participate for whatever reason should have access to video materials.</p>",
        "id": 318399312,
        "sender_full_name": "Patrick Johnson",
        "timestamp": 1672313192
    },
    {
        "content": "<p>2c: I would suggest the first meetings not to be recorded because it's important that participants feel as comfortable to speak as possible. And the group still has to find its dynamics. Recording will be more natural when there's a minimal shape to begin with. (I'm not participating, but I have some experience starting out informal study groups)</p>",
        "id": 318416427,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1672319795
    },
    {
        "content": "<p>Please don't let anything hinder the spontaneity of the movement. It's an important ingredient!</p>",
        "id": 318416700,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1672319895
    },
    {
        "content": "<p>I'd very much like to attend but this is the same week as the POPL conference (Principles of Programming Languages). I only marked times as available when they didn't conflict with this (i.e. before 9:00 AM).</p>\n<p>On the other hand, it seems there are quite a few talks about Lean, which presumably means other Lean people will be there. So while I can't really do a Zoom call, if other people will be at POPL I am willing to do an in-person meeting!</p>",
        "id": 318903929,
        "sender_full_name": "Praneeth Kolichala",
        "timestamp": 1672605731
    },
    {
        "content": "<p>I am sorry I chose an inconvenient week for the first call. Given how many people have responded to the poll, I will not change it now. I will be happy if you join us for the next meetings!</p>",
        "id": 318949099,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1672647140
    },
    {
        "content": "<p>This discussion <a href=\"#narrow/stream/113488-general/topic/elementary.20probability.20advice/near/306030991\">on elementary probability</a> is very relevant to us. Most theoretical CS people may not be fluent in measure theory either.</p>",
        "id": 319818095,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673023492
    },
    {
        "content": "<p>We shall meet on Monday 2023-01-16 at 15:00 GMT.</p>",
        "id": 320405042,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673342038
    },
    {
        "content": "<p>Can someone with Zoom Pro generate a Zoom link for our meeting please? My links are limited to 40 minutes.</p>",
        "id": 320405909,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673342384
    },
    {
        "content": "<p>If you use the add global time button when composing message you can add a time which will be rendered as everyones local time, eg: <time datetime=\"2023-01-16T15:00:00Z\">2023-01-16T15:00:00Z</time></p>",
        "id": 320443790,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1673355247
    },
    {
        "content": "<p>Yes <time datetime=\"2023-01-16T15:00:00Z\">2023-01-16T15:00:00+00:00</time> is correct.</p>",
        "id": 320444669,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673355520
    },
    {
        "content": "<p>Hot news! The Cook-Levin theorem has been proved in Isabelle!<br>\n<a href=\"https://www.isa-afp.org/entries/Cook_Levin.html\">https://www.isa-afp.org/entries/Cook_Levin.html</a></p>",
        "id": 320537019,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673381861
    },
    {
        "content": "<p>Wow!! Only two days ago as well :O</p>",
        "id": 320537861,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1673382210
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"468318\">@Fabian Kunze</span> Can you please tell us a brief comparison with your proof in Coq?</p>",
        "id": 320542822,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673384088
    },
    {
        "content": "<p>The first 360 odd pages in the proof doc are just for establishing the basic machinery of TMs and other basic stuff. I wonder how much we can shave this in size with effective use of mathlib</p>",
        "id": 320548042,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673386036
    },
    {
        "content": "<p>and how much of this machinery related to TMs is reusable for the rest of the theorems in that book</p>",
        "id": 320548224,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673386109
    },
    {
        "content": "<p>A significant chunk of the TM related proofs is about writing TMs for specific languages and proving their correctness and complexity.</p>",
        "id": 320548444,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673386198
    },
    {
        "content": "<p>In general I would usually expect isabelle results to be shorter than ours because they can cut proofs quite short thanks to superior automation.</p>",
        "id": 320548609,
        "sender_full_name": "Henrik Böving",
        "timestamp": 1673386251
    },
    {
        "content": "<p>I don't mean shorter through automation, but shorter in an amortized sense, because we have a library which might come in handy for proving this theorem, and the theorems we develop for this might come in handy elsewhere.</p>",
        "id": 320650952,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673428414
    },
    {
        "content": "<p>Can somebody please generate the Zoom link?</p>",
        "id": 320796051,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673470684
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/320796051\">said</a>:</p>\n<blockquote>\n<p>Can somebody please generate the Zoom link?</p>\n</blockquote>\n<p>I need to check if our institutions still have licenses.</p>",
        "id": 320816724,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673479714
    },
    {
        "content": "<p>I'll be able to check if I can create a room by Friday evening</p>",
        "id": 320816774,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673479748
    },
    {
        "content": "<p>I think I can generate one under my institution</p>",
        "id": 320816998,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1673479911
    },
    {
        "content": "<p><a href=\"https://usc.zoom.us/j/96278860441?pwd=YXFjdlJ2aFVlOUovMUVscitNWHZCdz09\">https://usc.zoom.us/j/96278860441?pwd=YXFjdlJ2aFVlOUovMUVscitNWHZCdz09</a></p>",
        "id": 320817069,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1673479931
    },
    {
        "content": "<p>Does this work lol</p>",
        "id": 320817079,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1673479938
    },
    {
        "content": "<p>Over the past couple weeks, I've been working on my reworked definitions of polynomial time computation. I've uploaded the repository <a href=\"https://github.com/prakol16/circuits\">here</a> with a reasonably detailed README. I feel like I have a reasonable path towards Cook-Levin, although the machinery for circuits is still quite basic. Essentially, we need to define uniform circuit families and show that composition (which is mostly straightforward with circuits) can actually be computed in polynomial time.</p>\n<p>If anyone would like to help out or contribute, that would be very appreciated (especially now that winter break has ended, I will probably be busy again).</p>",
        "id": 320855917,
        "sender_full_name": "Praneeth Kolichala",
        "timestamp": 1673500014
    },
    {
        "content": "<p>Your README looks great! When I have more time, I will READYOU more carefully.</p>",
        "id": 320867245,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673507550
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"355764\">Hanting Zhang</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/320817069\">said</a>:</p>\n<blockquote>\n<p><a href=\"https://usc.zoom.us/j/96278860441?pwd=YXFjdlJ2aFVlOUovMUVscitNWHZCdz09\">https://usc.zoom.us/j/96278860441?pwd=YXFjdlJ2aFVlOUovMUVscitNWHZCdz09</a></p>\n</blockquote>\n<p>Is it all right that I can join the call already now?</p>",
        "id": 320867680,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673507787
    },
    {
        "content": "<p>uhhhh yeah idk how to restrict it</p>",
        "id": 320868337,
        "sender_full_name": "Hanting Zhang",
        "timestamp": 1673508120
    },
    {
        "content": "<p>Do other people use the same link?</p>",
        "id": 320868528,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673508205
    },
    {
        "content": "<p>Can I distribute the link without a risk that the recipients will land into unrelated meetings?</p>",
        "id": 320893321,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673516815
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/320542822\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"468318\">Fabian Kunze</span> Can you please tell us a brief comparison with your proof in Coq?</p>\n</blockquote>\n<p>The author of the new proof in Isabelle compares his proof to the proof in Coq as follows:<br>\n<a href=\"/user_uploads/3121/Av_8_ATe9nMHRIxE_yh96r_V/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/Av_8_ATe9nMHRIxE_yh96r_V/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/Av_8_ATe9nMHRIxE_yh96r_V/image.png\"></a></div>",
        "id": 321015891,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673551329
    },
    {
        "content": "<p><a href=\"/user_uploads/3121/91P4mtevTE51q7CeJB6wIMfM/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/91P4mtevTE51q7CeJB6wIMfM/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/91P4mtevTE51q7CeJB6wIMfM/image.png\"></a></div>",
        "id": 321015982,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673551354
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"417654\">Martin Dvořák</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/320542822\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"468318\">Fabian Kunze</span> Can you please tell us a brief comparison with your proof in Coq?</p>\n</blockquote>\n<p>We use a different intermediate problem to get from n-Tape TMs to SAT: Instead of oblivious 2-tape TMs, we use 1-tape TMs, and then look at the configuration history, which is a 2-dimensional table: Line $i$ line corresponds to the tape, head position and state after $i$ computation steps, and each cell in that line is more or less one cell of the tape. Then, we express the transition function of the Turing machine as a set of 2x3 cell large \"dominos\", and each 2x3 cell in the configuration history must be in this set of allowed states.<br>\nThis means that validity of the computation is a very local property, and we can just AND that each position in the table is one of the different allowed dominos.<br>\nBoth the intermediate problem used by us, and by the authors of the Isabelle formalization, are  standard proof ideas sketched in text books <br>\non complexity theory.</p>",
        "id": 321027812,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1673555196
    },
    {
        "content": "<p>On a higher level, our approach of using the lambda calculus as a model of computation enables us to more or less generate the witnesses of computability (i.e., the programs computing reduction functions etc) directly from the (functional) specification of those functions in Coq. We still have to verify many Turing machines (a TM interpreting lambda-calculus, and a translation from multi-tape TMs to single-tape-TMs), but at least not all the reduction functions need to be modeled as TMs.</p>",
        "id": 321028311,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1673555381
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"437861\">Praneeth Kolichala</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/320855917\">said</a>:</p>\n<blockquote>\n<p>I feel like I have a reasonable path towards Cook-Levin, although the machinery for circuits is still quite basic. Essentially, we need to define uniform circuit families and show that composition (which is mostly straightforward with circuits) can actually be computed in polynomial time.</p>\n</blockquote>\n<p>Interesting, I would not have thought that circuits are a natural path towards formalizing cook-levin, doesn't one still need another model of computation to define \"uniformity\" of circuit families?</p>",
        "id": 321028899,
        "sender_full_name": "Fabian Kunze",
        "timestamp": 1673555593
    },
    {
        "content": "<p>Yes, we still need a model of computation to define uniformity (and this is the main model of computation; it is based on  an inductive definition akin to the current <a href=\"https://leanprover-community.github.io/mathlib_docs/find/nat.primrec\">docs#nat.primrec</a>). The main thing is that (hopefully) we avoid having to deal with TMs directly.</p>",
        "id": 321063540,
        "sender_full_name": "Praneeth Kolichala",
        "timestamp": 1673571076
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"437861\">Praneeth Kolichala</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/321063540\">said</a>:</p>\n<blockquote>\n<p>Yes, we still need a model of computation to define uniformity (and this is the main model of computation; it is based on  an inductive definition akin to the current <a href=\"https://leanprover-community.github.io/mathlib_docs/find/nat.primrec\">docs#nat.primrec</a>). The main thing is that (hopefully) we avoid having to deal with TMs directly.</p>\n</blockquote>\n<p>Depends on your goal. If it is to just prove variants of complexity results in your model, then this will certainly do. There will have to be suitable adaptations of statements. For instance, in the time/space hierarchy theorems. If however your goal is to build a larger library of results which the TCS community finds useful, and builds upon further, then TM and RAM are unavoidable. There are relatively few  uniform translations of complexity results from one model to another, and usually one can do better for various classes of problems. Most people in this field care about them. There was a similar discussion in another thread.</p>",
        "id": 321064179,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673571394
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 321064254,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673571439
    },
    {
        "content": "<p>If you wish to try model agnostic approaches, then descriptive complexity might be a nice approach.</p>",
        "id": 321064998,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1673571823
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/113488-general/topic/Computational.20Complexity.20Theory/near/321064998\">said</a>:</p>\n<blockquote>\n<p>If you wish to try model agnostic approaches, then descriptive complexity might be a nice approach.</p>\n</blockquote>\n<p>I've actually been playing around with something in this vein (I use vein loosely here, mostly its the idea of quantifying over models) (there is another thread in Is there code for X? with some of my earlier thoughts leading up to here). Basically define a model of computation and then we can write theorems about all models.<br>\n<a href=\"https://github.com/calcu16/lean_complexity/blob/67ebab10624a272cf7c6c2a5345d26b2096e1c55/src/complexity/basic.lean\">https://github.com/calcu16/lean_complexity/blob/67ebab10624a272cf7c6c2a5345d26b2096e1c55/src/complexity/basic.lean</a><br>\nFor better or worse I've been trying to abuse instances so that in theory I could autogenerate a  complexity bound from the lean definitions.<br>\nfor example I've defined nat.mul generically on any model to have complexity based on an equivalent formulation using nat.iterate</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">instance</span> <span class=\"n\">mul_complexity</span> <span class=\"o\">[</span><span class=\"n\">has_encoding</span> <span class=\"n\">m</span> <span class=\"n\">ℕ</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">has_encoding</span> <span class=\"n\">m</span> <span class=\"o\">(</span><span class=\"n\">ℕ</span><span class=\"bp\">×</span><span class=\"n\">ℕ</span><span class=\"o\">)]</span>\n  <span class=\"o\">[</span><span class=\"n\">cf</span><span class=\"o\">:</span> <span class=\"n\">has_complexity</span> <span class=\"n\">m</span>\n    <span class=\"o\">(</span><span class=\"n\">curry</span> <span class=\"o\">(</span><span class=\"n\">compose</span>\n      <span class=\"n\">prod.fst</span>\n      <span class=\"o\">((</span><span class=\"n\">compose</span>\n        <span class=\"o\">(</span><span class=\"n\">uncurry</span> <span class=\"o\">(</span><span class=\"n\">nat.iterate</span> <span class=\"o\">(</span><span class=\"n\">fork</span> <span class=\"o\">(</span><span class=\"n\">uncurry</span> <span class=\"n\">nat.add</span><span class=\"o\">)</span> <span class=\"n\">prod.snd</span><span class=\"o\">)))</span>\n        <span class=\"o\">(((</span><span class=\"n\">fork</span> <span class=\"n\">prod.snd</span> <span class=\"o\">(</span><span class=\"n\">fork</span> <span class=\"o\">(</span><span class=\"bp\">@</span><span class=\"n\">const</span> <span class=\"n\">ℕ</span> <span class=\"o\">(</span><span class=\"n\">ℕ</span><span class=\"bp\">×</span><span class=\"n\">ℕ</span><span class=\"o\">)</span> <span class=\"mi\">0</span><span class=\"o\">)</span> <span class=\"n\">prod.fst</span><span class=\"o\">))))))))]:</span>\n  <span class=\"n\">has_complexity</span> <span class=\"n\">m</span> <span class=\"n\">nat.mul</span> <span class=\"o\">:=</span> <span class=\"bp\">...</span>\n</code></pre></div>\n<p>Then assuming you have a model with the complexities of the necessary primitives defined for a model the complexity will \"pop\" out.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">def</span> <span class=\"n\">complexity</span> <span class=\"o\">{</span><span class=\"n\">α</span> <span class=\"n\">β</span> <span class=\"n\">γ</span><span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">has_equiv</span> <span class=\"n\">β</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">preorder</span> <span class=\"n\">γ</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">has_add</span> <span class=\"n\">γ</span><span class=\"o\">]</span>\n  <span class=\"o\">(</span><span class=\"n\">m</span><span class=\"o\">:</span> <span class=\"n\">complexity.model</span> <span class=\"n\">α</span> <span class=\"n\">β</span> <span class=\"n\">γ</span><span class=\"o\">)</span> <span class=\"o\">{</span><span class=\"n\">δ</span><span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">complexity.has_encodable_function</span> <span class=\"n\">m</span> <span class=\"n\">δ</span><span class=\"o\">]</span>\n  <span class=\"o\">(</span><span class=\"n\">f</span><span class=\"o\">:</span> <span class=\"n\">δ</span><span class=\"o\">)</span> <span class=\"o\">[</span><span class=\"n\">c</span><span class=\"o\">:</span> <span class=\"n\">complexity.has_complexity</span> <span class=\"n\">m</span> <span class=\"n\">f</span><span class=\"o\">]:</span>\n  <span class=\"n\">complexity.cost_function'</span> <span class=\"n\">m</span> <span class=\"n\">δ</span> <span class=\"o\">:=</span> <span class=\"n\">c.value.cost</span>\n</code></pre></div>\n<p>For example in my case I'm using a model of counting beta reductions in untyped lambda calculus and I can get a complexity for it without having to write(or cost) nat.mul myself in lambda calculus.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">example</span><span class=\"o\">:</span>  <span class=\"o\">(</span><span class=\"n\">complexity</span> <span class=\"n\">distance_model</span> <span class=\"n\">nat.mul</span><span class=\"o\">)</span> <span class=\"bp\">≤</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"n\">y</span><span class=\"o\">:</span> <span class=\"n\">ℕ</span><span class=\"o\">),</span> <span class=\"o\">((</span><span class=\"mi\">30</span><span class=\"bp\">*</span><span class=\"n\">y</span> <span class=\"bp\">+</span> <span class=\"mi\">43</span><span class=\"o\">):</span><span class=\"n\">ℕ</span><span class=\"o\">))</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"n\">intros</span> <span class=\"n\">n</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n  <span class=\"n\">simp</span> <span class=\"n\">only</span> <span class=\"o\">[</span><span class=\"n\">complexity</span><span class=\"o\">,</span> <span class=\"n\">complexity.cost_function.less_than_or_equal</span><span class=\"o\">,</span> <span class=\"n\">has_complexity.value</span><span class=\"o\">,</span> <span class=\"n\">fork</span><span class=\"o\">,</span> <span class=\"n\">const</span><span class=\"o\">,</span> <span class=\"n\">uncurry</span><span class=\"o\">],</span>\n  <span class=\"n\">ring_nf</span><span class=\"o\">,</span>\n  <span class=\"n\">norm_num</span><span class=\"o\">,</span>\n  <span class=\"n\">induction</span> <span class=\"n\">m</span><span class=\"o\">,</span>\n  <span class=\"o\">{</span> <span class=\"n\">simp</span> <span class=\"n\">only</span> <span class=\"o\">[</span><span class=\"n\">iteration_complexity.cost_zero</span><span class=\"o\">],</span>\n    <span class=\"n\">ring_nf</span> <span class=\"o\">},</span>\n  <span class=\"n\">simp</span> <span class=\"n\">only</span> <span class=\"o\">[</span><span class=\"n\">iteration_complexity.cost_succ'</span><span class=\"o\">,</span> <span class=\"n\">function.iterate_succ'</span><span class=\"o\">,</span> <span class=\"n\">nat.mul_succ</span><span class=\"o\">,</span> <span class=\"n\">add_mul</span><span class=\"o\">,</span> <span class=\"n\">nat.add_assoc</span><span class=\"o\">],</span>\n  <span class=\"n\">nlinarith</span><span class=\"o\">,</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>One cute thing with this approach is that you don't even need a model, you could say something basically akin to \"for all models with properties X Y Z\" my function will have complexity better than C.</p>\n<p>One thing that's not clear to me is how to interact with general recursion using the instance strategy (or even if the instance strategy is a good idea). Additionally functions need to be written in a point-free style, so pointers on how I could get generic pattern matching against lambdas would also be of use I think.</p>",
        "id": 321423357,
        "sender_full_name": "Andrew Carter",
        "timestamp": 1673742891
    },
    {
        "content": "<p>[not relevant anymore]</p>",
        "id": 321581523,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673856398
    },
    {
        "content": "<p>We have discussed that we would like to meet probably twice a month.<br>\nLet's vote for a regular meeting — it will happen every other week, starting with the week that we vote about!<br>\n<a href=\"https://doodle.com/meeting/participate/id/e791Lj8b\">https://doodle.com/meeting/participate/id/e791Lj8b</a></p>",
        "id": 321691751,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1673891205
    },
    {
        "content": "<p>Please vote. I would like to announce the time of our meetings tomorrow.</p>",
        "id": 322997385,
        "sender_full_name": "Martin Dvořák",
        "timestamp": 1674473713
    }
]