[
    {
        "content": "<p>I became curious how sizes of proof terms affect speed, so I ran a quick experiment: </p>\n<div class=\"codehilite\"><pre><span></span>def foo : nat → Prop\n| 0 := true\n| (n+1) := (foo n) ∧ (foo n)\n\nmeta def mk_foo_expr : nat → expr\n| 0 := `(trivial)\n| (n+1) :=\n  expr.app\n    (expr.app\n      (reflected.to_expr `(@and.intro (foo n) (foo n)))\n      (mk_foo_expr n))\n    (mk_foo_expr n)\n\nopen tactic\n\nmeta def show_foo : tactic unit :=\ndo `(foo %%nx) ← target,\n   n ← eval_expr nat nx,\n   exact (mk_foo_expr n)\n\nset_option profiler true\n\nlemma foo_200 : foo 200 :=\nby show_foo\n\n#print foo_200\n</pre></div>\n\n\n<p>To my surprise Lean handles it with ease, in roughly 50ms. I don' t think Lean is constructing and checking a gigantic proof term that has 2^200 occurrences of <code>trivial</code>. What's going on here?</p>",
        "id": 154864164,
        "sender_full_name": "Seul Baek",
        "timestamp": 1547148098
    },
    {
        "content": "<p>To a certain degree, Lean can handle terms with dag-like sharing (your term is very small when viewed as a dag).  This happens e.g. because type inference and whnf and unifiability are cached, i.e., only evaluated once for every subterm (there are only few subterms).</p>",
        "id": 154868623,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1547151975
    },
    {
        "content": "<p>See also <a href=\"https://github.com/leanprover/tc/issues/8\" target=\"_blank\" title=\"https://github.com/leanprover/tc/issues/8\">https://github.com/leanprover/tc/issues/8</a></p>",
        "id": 154868637,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1547151994
    },
    {
        "content": "<p>I see. Thanks!</p>",
        "id": 154876865,
        "sender_full_name": "Seul Baek",
        "timestamp": 1547158687
    }
]