[
    {
        "content": "<p>There is a disagreement in <a href=\"https://github.com/leanprover-community/mathlib/pull/17553\">#17553</a> between <span class=\"user-mention\" data-user-id=\"214703\">@Yury G. Kudryashov</span>  and <span class=\"user-mention\" data-user-id=\"310045\">@Eric Wieser</span>, where Yury wants to replace <code>fintype</code> instances by <code>finite</code> instances (thereby making things less <code>computable</code>), while Eric would like to make things more computable.  I think this is something we should sort out on Zulip. Personally, I would be really happy to get rid of all computability in mathlib, and even of the <code>[decidable]</code> typeclass, since I don't think this is a good mechanism for computation (algorithms implemented in CAS-like functions like norm_num seem to me much better suited for the job), I've seen it creating defeq issues a lot of times, and I haven't seen once a place where this really turned out to be useful. But this is maybe an extreme position :-). I'd be interested in hearing why some people think it is important/useful to get things computable, especially with concrete examples!</p>",
        "id": 311795086,
        "sender_full_name": "Sebastien Gouezel",
        "timestamp": 1669199545
    },
    {
        "content": "<p>It's worth remembering that one of the ways that norm_num works is by transferring the information from a noncomputable type (such as <code>real</code>) into a computable type (such as <code>rat</code>), and then doing computation in the VM to get a normal form.</p>\n<p>If we follow that pattern, it's reasonable to assume that it will be easier to build norm_num extensions for types if we have computable versions of them.</p>",
        "id": 311798347,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669200726
    },
    {
        "content": "<p>Thanks for raising this on Zulip; If someone is blocked by the change in <a href=\"https://github.com/leanprover-community/mathlib/pull/17553\">#17553</a> then I'm fine with the noncomputable version going in in the medium term.</p>",
        "id": 311798752,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669200850
    },
    {
        "content": "<p>In the long term, I strongly feel that we're making things hard on ourselves by making <code>finsupp</code> noncomputable (whether polynomials are noncomputable is a related but independent concern). Changing that is already a herculean effort, and every change to make corners of the API more noncomputable makes that even harder</p>",
        "id": 311798862,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669200898
    },
    {
        "content": "<p>In this case, I guess a question is whether something like <code>norm_num</code> would ever want to compute with a <code>finsupp</code>, or if it would try to transfer to a much better representation (like a list of monomials).</p>",
        "id": 311798952,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669200923
    },
    {
        "content": "<p>Perhaps that's an argument in favor of <code>finsupp</code> being represented internally with a list of monomials instead</p>",
        "id": 311799030,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669200952
    },
    {
        "content": "<p>Perhaps it's not a particularly good argument, but; if <code>finsupp</code> and <code>polynomial</code> were computable than <code>(X + C 2 : polynomial rat).degree = 1</code> would be true by <code>refl</code> (because all the decidability instances would reduce without getting stuck on a <code>classical.dec_eq</code>)</p>",
        "id": 311799237,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669201006
    },
    {
        "content": "<p><a href=\"https://leanprover-community.github.io/mathlib_docs/find/finsupp\">docs#finsupp</a> is a closure (<code>to_fun</code>) along with a <code>finset</code> that is a support. The problem with closures is that they are not memoized by default, so you can easily have very inefficient <code>finsupp</code> terms.</p>",
        "id": 311799362,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669201049
    },
    {
        "content": "<p>Do you have a good reference to learn how closures are represented in the VM?</p>",
        "id": 311799504,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669201090
    },
    {
        "content": "<p>From a complexity point of view, things will depend a lot on the shape of the underlying space. For instance, if you're working with polynomials, then you want to use the order structure to compute efficiently, while if you're working with the group algebra of the free group, say, then you would frame your computations very differently. I don't think there is a \"one size fits all\" algorithm here. That's something norm_num can do very well, using different algorithms depending on the shape of the input, while <code>refl</code> can't.</p>",
        "id": 311799595,
        "sender_full_name": "Sebastien Gouezel",
        "timestamp": 1669201126
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/stream/113488-general/topic/finite.20or.20fintype/near/311799237\">said</a>:</p>\n<blockquote>\n<p>(because all the decidability instances would reduce without getting stuck on a <code>classical.dec_eq</code>)</p>\n</blockquote>\n<p>I'm not sure that's a safe assumption, because they might depend on well-founded recursion, which can get stuck. You can still <code>#eval</code> of course.</p>\n<p>I've thought about how it might be nice to have a <code>super_refl</code> that does something like <code>refl</code> but by (essentially) unfolding definitions using <code>unfold</code> rather than <code>dunfold</code>. I.e., allow rewrites. This is a bit of a non-sequitur, but it'd solve this well-founded recursion problem.</p>",
        "id": 311799856,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669201214
    },
    {
        "content": "<p>Having <code>(X + C 2 : polynomial rat).degree</code> to compute by <code>refl</code> doesn't seem useful to me, because most often you're not working with rat but with a general abstract field, in which this won't compute.</p>",
        "id": 311799868,
        "sender_full_name": "Sebastien Gouezel",
        "timestamp": 1669201218
    },
    {
        "content": "<p>Even if there's not a \"one size fits all\" algorithm, having an inefficient algorithm available everywhere would remove a lot of pain from the simple cases without having to write tactic code.</p>",
        "id": 311800059,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669201284
    },
    {
        "content": "<blockquote>\n<p>because most often you're not working with rat but with a general abstract field, in which this won't compute.</p>\n</blockquote>\n<p>I agree; but <code>lift</code> or <code>norm_num</code> could presumably translate it into a polynomial over rat for the sake of computation, just like it does for numeric literals</p>",
        "id": 311800236,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669201332
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"310045\">Eric Wieser</span> <a href=\"#narrow/stream/113488-general/topic/finite.20or.20fintype/near/311799504\">said</a>:</p>\n<blockquote>\n<p>Do you have a good reference to learn how closures are represented in the VM?</p>\n</blockquote>\n<p>Not really, but the main point is that a closure is a bit of code that has to be re-evaluated every time, and there's nothing to cache the results. Inductive datatypes (like a <code>list</code>) are places where evaluated data can actually be stored and read again later. One way to deal with this problem with closures is to evaluate a table of all outputs for every input then create a new function that reads that (and then prove that the new function is equal to the old one).</p>",
        "id": 311800392,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669201398
    },
    {
        "content": "<p>I had something like your last sentence in mind too</p>",
        "id": 311801322,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669201784
    },
    {
        "content": "<p>I guess I'll mention one example of polynomials in Lean 4. I needed an efficient implementation, so <a href=\"https://github.com/kmill/arrow_poly/blob/master/ArrowPoly/Poly.lean\">this</a> is hand-tuned for handling multivariable Laurent polynomials (this ring in particular is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">Z</mi><mo stretchy=\"false\">[</mo><msubsup><mi>t</mi><mn>0</mn><mrow><mo>±</mo><mn>1</mn></mrow></msubsup><mo separator=\"true\">,</mo><msubsup><mi>t</mi><mn>1</mn><mrow><mo>±</mo><mn>1</mn></mrow></msubsup><mo separator=\"true\">,</mo><msubsup><mi>t</mi><mn>2</mn><mrow><mo>±</mo><mn>1</mn></mrow></msubsup><mo separator=\"true\">,</mo><mo>…</mo><mtext> </mtext><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbb{Z}[t_0^{\\pm 1}, t_1^{\\pm 1}, t_2^{\\pm 1},\\dots]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1205em;vertical-align:-0.2663em;\"></span><span class=\"mord mathbb\">Z</span><span class=\"mopen\">[</span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8542em;\"><span style=\"top:-2.4337em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span><span style=\"top:-3.1031em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">±</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2663em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8542em;\"><span style=\"top:-2.4337em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.1031em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">±</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2663em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8542em;\"><span style=\"top:-2.4337em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.1031em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">±</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2663em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mclose\">]</span></span></span></span>). It uses lists of monomials sorted by exponents, since that way when you add polynomials you can merge and deal with entries with duplicate exponents. This is much more efficient than what you could get with a more generic <code>finset</code> type of representation, and I suspect I couldn't have finished my calculation without this optimization. I'm sure this is what Sebastien meant about using order properties.</p>",
        "id": 311804539,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669202975
    },
    {
        "content": "<p>For now I have the impression that the discussion is pretty hypothetical, in the form: if  <code>finsupp</code> were computable, then maybe it could help for some <code>norm_num</code> algorithms. <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> , do you have some thoughts on that?</p>",
        "id": 311814574,
        "sender_full_name": "Sebastien Gouezel",
        "timestamp": 1669206598
    },
    {
        "content": "<p>Can someone remind us of the <code>polynomial</code> story? My memory was that they were initially computable and then they were switched to being noncomputable, and my memory was that this solved some problems but didn't break anything. </p>\n<p><span class=\"user-mention\" data-user-id=\"110032\">@Reid Barton</span> had a problem with <code>finsupp</code> IIRC, in the sense that I think he once told me that in some sense it was some kind of half-way house between being computable and noncomputable and hence no good for anyone.</p>",
        "id": 311816368,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1669207176
    },
    {
        "content": "<p>I think it's very good to have definitions that are optimized for reasoning as well as definitions that are optimized for computations. And they probably will overlap sometimes, but not very much.</p>",
        "id": 311817316,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1669207480
    },
    {
        "content": "<p>Arguably what matters for reasoning is usually the API, not the definition itself</p>",
        "id": 311817457,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669207543
    },
    {
        "content": "<p>Making the API is much harder (for me, at least) if the definition is not optimised for reasoning.</p>",
        "id": 311817541,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1669207564
    },
    {
        "content": "<p>Example: proving <code>add_assoc</code> for <code>num</code> vs proving it for <code>nat</code>.</p>",
        "id": 311817622,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1669207593
    },
    {
        "content": "<p>If the API mentions <code>decidable_eq</code> in the type of <code>union</code> or whatever, then it is the wrong API for reasoning about.</p>",
        "id": 311817683,
        "sender_full_name": "Reid Barton",
        "timestamp": 1669207616
    },
    {
        "content": "<p>Are you advocating removing <code>decidable_eq</code> from <a href=\"https://leanprover-community.github.io/mathlib_docs/find/finset.union\">docs#finset.union</a>?</p>",
        "id": 311817755,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669207640
    },
    {
        "content": "<p>The problem is no one seems to agree on what finset is for</p>",
        "id": 311817832,
        "sender_full_name": "Reid Barton",
        "timestamp": 1669207666
    },
    {
        "content": "<p>I'd advocate for defining <code>finite_set</code> as a <code>set</code> with a <code>set.finite</code> proof, then letting <code>finset</code> be there for generic computation.</p>",
        "id": 311817983,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669207709
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110032\">Reid Barton</span> <a href=\"#narrow/stream/113488-general/topic/finite.20or.20fintype/near/311817683\">said</a>:</p>\n<blockquote>\n<p>If the API mentions <code>decidable_eq</code> in the type of <code>union</code> or whatever, then it is the wrong API for reasoning about.</p>\n</blockquote>\n<p>Taking this to the extreme, this seems to also advocate having and prefering a classical version of <a href=\"https://leanprover-community.github.io/mathlib_docs/find/ite\">docs#ite</a> that avoids <code>decidable</code> arguments</p>",
        "id": 311818240,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669207801
    },
    {
        "content": "<p>I am just saying that the API(s) for implementation(s) of <code>finset</code> designed for computation cannot be identical to the API for the one for reasoning, because they will have extra <code>decidable_eq</code> or <code>linear_order</code> or whatever hypotheses.</p>",
        "id": 311818329,
        "sender_full_name": "Reid Barton",
        "timestamp": 1669207820
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"310045\">@Eric Wieser</span> There's been at least one discussion on zulip (with Reid) with that suggestion for a classical <code>ite</code></p>",
        "id": 311818436,
        "sender_full_name": "Kyle Miller",
        "timestamp": 1669207862
    },
    {
        "content": "<p>I don't fully see the argument for why a <code>decidable_eq</code> argument is a big deal for reasoning; most of the time you can just ignore it's there and Lean will handle it for you</p>",
        "id": 311818477,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669207879
    },
    {
        "content": "<p>Didn't you design a whole linter to make sure that <code>decidable_eq</code> arguments in definitions/lemmas are properly generalized over, and such? It wouldn't even need to exist in an ideal world.</p>",
        "id": 311819035,
        "sender_full_name": "Reid Barton",
        "timestamp": 1669208066
    },
    {
        "content": "<p>Arguably I only designed about 3/4 of it and never got back to the 1/4...</p>",
        "id": 311819814,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1669208327
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110050\">Sebastien Gouezel</span> <a href=\"#narrow/stream/113488-general/topic/finite.20or.20fintype/near/311814574\">said</a>:</p>\n<blockquote>\n<p>For now I have the impression that the discussion is pretty hypothetical, in the form: if  <code>finsupp</code> were computable, then maybe it could help for some <code>norm_num</code> algorithms. <span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> , do you have some thoughts on that?</p>\n</blockquote>\n<p>It's not used in <code>norm_num</code>, but one example of a tactic that does rely on kernel reduction in order to extract witnesses is <code>fin_cases</code>. This will use typeclass inference to infer a <code>fintype</code> instance on the target, then reduce it to get a list of elements to present as goals.</p>\n<p>Also, of course there is the <code>decide</code> tactic (previously <code>dec_trivial</code>), which should not be discounted. I find 657 uses of <code>dec_trivial</code> in mathlib, which puts it up there with some of the most used heavy tactics like linarith. If we were to completely remove decidability, then we would basically lose this tactic entirely and would have to reinvent an evaluation framework to which <code>norm_num</code> is only an approximation.</p>\n<p>However, I agree with the thrust of the argument. Kernel reduction has weird performance characteristics and generally requires custom implementations anyway, so I don't think we should assume that standardly defined mathlib notions will be optimal for this kind of computation. And in many cases, like <code>fin_cases</code>, I think it would be better served by a meta typeclass similar to norm_num extensions to handle synthesis of the elements of arbitrary types, written the way we want them to be (the one you get by kernel reduction is more or less forced to have the form <code>[zero, zero.succ, zero.succ.succ]</code> and so on).</p>",
        "id": 311923538,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1669247753
    },
    {
        "content": "<p>And note the <code>interval_cases</code> (which is a thin wrapper on <code>fin_cases</code>) actually has to call norm_num internally to rewrite those horrible \"numerals\" back into literals.</p>",
        "id": 311931304,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1669255370
    }
]