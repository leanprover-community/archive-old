[
    {
        "content": "<p>Hi general. I have been speaking with NLP researchers who wonder whether tools from semantic parsing, information extraction, and (assisted, not automatic) machine translation may be able to accelerate formalization. I see two high-level possibilities:</p>\n<ol>\n<li>interactive translation (i.e. fancy somewhat-semantically-aware autocomplete, for both statements and proofs)</li>\n<li>hint extraction (i.e. extract useful snippets from informal proofs to feed to hint-friendly solver)</li>\n</ol>\n<p>For (2), the info-extraction could be entirely untrusted so it need not be interactive. For example, a system could extract multiple best-effort guesses at which variables to induct on, intermediate steps, existential witnesses, lemmas referenced, etc. There are certainly some kinds of solvers that could be very effectively with even moderately-reliable hints of this form, though no such solver exists yet for Lean.</p>\n<p>High-level ideas aside, I don't have a good enough sense of the actual formalization process in mathlib to have an informed opinion about what is likely to be useful in practice. As mathlib grows, are most formalizations increasingly isomorphic to existing informal texts? In other words, is \"translation\" even a reasonable analogy for most projects? Roughly what percentage of the difficulty is in formalizing statements (and other non-machine-checkable components) versus proofs? Any other thoughts/concerns/doubts/suggestions? Thanks in advance.</p>",
        "id": 243867864,
        "sender_full_name": "Daniel Selsam",
        "timestamp": 1624586164
    },
    {
        "content": "<p>I find it really hard to tell whether such tools would be useful without any idea of how well they would work. If they are useful one every 100 proofs then we won't bother asking (this is my experience with gptf for instance, typing the tactic name and waiting for it to run is simply distracting since it works so rarely).  The example \"best-effort guesses at which variables to induct on\" and \"existential witnesses\" look completely useless to me. If we already have a paper proof then fetching this information takes up a negligible amount of time. The \"intermediate steps\" idea could be useful if the system manages to correctly state those steps in Lean. And \"lemmas referenced\" would be useful if the system actually finds the lemmas in mathlib or better, manages to find very related lemmas even if they are not exactly the actual lemma we need.</p>",
        "id": 243879967,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1624602509
    },
    {
        "content": "<blockquote>\n<p>As mathlib grows, are most formalizations increasingly isomorphic to existing informal texts? </p>\n</blockquote>\n<p>I would say no. We're not yet at this stage. There is simply too much missing ground work. And every new theory starts with dozens of lemmas and intermediate constructions that simply have no analogue in the real world. That's why I'm so skeptical of all those auto-formalization dreams.</p>",
        "id": 243880071,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1624602614
    },
    {
        "content": "<blockquote>\n<p>Roughly what percentage of the difficulty is in formalizing statements (and other non-machine-checkable components) versus proofs? </p>\n</blockquote>\n<p>I think formalizing statements is still taking up most of the time, except when computations are involved in the proof. Proofs by computation are still extremely painful, for lack of automation.</p>",
        "id": 243880107,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1624602701
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110031\">@Patrick Massot</span> Thanks for the thoughtful response. I share your general skepticism of auto-formalization, especially for statements. Maybe the better question would be: NLP researchers are excited about Mathlib and want to help -- can they? I am only trying to be a bridge for them in this thread.</p>\n<blockquote>\n<p>If they are useful one every 100 proofs then we won't bother asking</p>\n</blockquote>\n<p>Let's say in the thought experiment for (1), it is instantaneous and doesn't require asking each time, like visual studio's intellicode or recent translation-assistance tools. (I was imagining (2) would be done in batch-offline mode.) For (1), let's also say that it is connected to a good info-retrieval system so it can find lemmas for you based on both the current context and vague allusions in the informal text.</p>\n<blockquote>\n<p>I find it really hard to tell whether such tools would be useful without any idea of how well they would work.</p>\n</blockquote>\n<p>Hard to know at this stage. Auto-translate for natural language is not reliable enough for high-stakes contexts, but I have been told that in most contexts, human translators generally prefer fixing-up/interacting-with auto-guesses than translating from scratch. But I also guess that a lot of NL translation is literally typing-limited, and even the translation-like parts of formalizing probably have &gt;1 OOM more thought-per-keystroke in general. </p>\n<blockquote>\n<p>As mathlib grows, are most formalizations increasingly isomorphic to existing informal texts?</p>\n<blockquote>\n<p>I would say no. We're not yet at this stage. There is simply too much missing ground work. And every new theory starts with dozens of lemmas and intermediate constructions that simply have no analogue in the real world. <br>\n</p>\n</blockquote>\n</blockquote>\n<p>To what extent do you think this stage will come eventually? I remember that for Feit Thompson, 80% of the lines were preliminaries, but that there was eventually a \"translation-like\" phase converting 250 human pages into Coq with a 4:1 de Bruijn ratio.</p>\n<blockquote>\n<p>I think formalizing statements is still taking up most of the time</p>\n</blockquote>\n<p>Interesting.</p>\n<blockquote>\n<p>Proofs by computation are still extremely painful, for lack of automation.</p>\n</blockquote>\n<p>This sounds like an oxymoron on first read -- non-automated proofs by computation. Can you please point me to a Zulip thread explaining this issue? Thanks.</p>",
        "id": 243923182,
        "sender_full_name": "Daniel Selsam",
        "timestamp": 1624631315
    },
    {
        "content": "<p>A non-automated proof by computation is e.g. a proof that <code>x &lt; y</code> by showing that <code>x &lt; 65/100</code> and <code>65/100 &lt; y</code>. See, for example, <a href=\"https://github.com/leanprover-community/mathlib/blob/6d2e589f73961949e6a430f926ce64f1575eae72/src/number_theory/bertrand.lean#L539\">https://github.com/leanprover-community/mathlib/blob/6d2e589f73961949e6a430f926ce64f1575eae72/src/number_theory/bertrand.lean#L539</a> which is a rather awful proof I did recently that <code>2/3 ≤ log 2</code>. Absolutely trivial by decimal expansion, but you have to show that your expansions are sufficiently precise. (I'm quite sure it's possible to do better than how we ended up doing it, but that's the class of \"extremely painful\" we're talking about, I think.)</p>",
        "id": 243943310,
        "sender_full_name": "Patrick Stevens",
        "timestamp": 1624640648
    },
    {
        "content": "<p>Here are a couple of examples (taken from those I either wrote myself or read very closely) of computational proofs which are rather painful:</p>\n<ul>\n<li><a href=\"https://leanprover-community.github.io/100.html#9\">The area of a circle is pi</a> -- annoying calculus</li>\n<li><a href=\"https://leanprover-community.github.io/mathlib_docs/find/stereo_left_inv\">docs#stereo_left_inv</a>, the composition of two particular functions is the identity -- annoying algebra</li>\n<li><a href=\"https://leanprover-community.github.io/mathlib_docs/find/exists_lt_lower_semicontinuous_integral_gt_nnreal\">docs#exists_lt_lower_semicontinuous_integral_gt_nnreal</a> -- annoying switching among real, <code>nnreal</code> and <code>ennreal</code></li>\n</ul>",
        "id": 243945299,
        "sender_full_name": "Heather Macbeth",
        "timestamp": 1624641694
    },
    {
        "content": "<p>And, again taking a random example I wrote myself, I think <a href=\"https://github.com/leanprover-community/mathlib/issues/8012\">#8012</a> is a good example of the opposite phenomenon described by Patrick, where formalizing the statements takes up most of the time.  I think it would be quite difficult for an AI (or even, say, someone without enough \"mathematical maturity\") to come up with a Lean statement for the lemma I have docstring'd as,</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"sd\">/-- If a conjugation-invariant subalgebra of `C(X, ℂ)` separates points, then the real</span>\n<span class=\"sd\">subalgebra of its purely real-valued elements also separates points. -/</span>\n</code></pre></div>",
        "id": 243946225,
        "sender_full_name": "Heather Macbeth",
        "timestamp": 1624642205
    },
    {
        "content": "<p>I meant what Heather wrote.</p>",
        "id": 243950287,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1624644263
    },
    {
        "content": "<p>You can also have a look at the proof of <a href=\"https://github.com/leanprover-community/lean-liquid/blob/master/src/system_of_complexes/completion.lean#L44-L48\">this</a>.</p>",
        "id": 243950634,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1624644449
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"117987\">Patrick Stevens</span> <a href=\"#narrow/stream/113488-general/topic/nlp.20for.20lean.3F/near/243943310\">said</a>:</p>\n<blockquote>\n<p>A non-automated proof by computation is e.g. a proof that <code>x &lt; y</code> by showing that <code>x &lt; 65/100</code> and <code>65/100 &lt; y</code>. See, for example, <a href=\"https://github.com/leanprover-community/mathlib/blob/6d2e589f73961949e6a430f926ce64f1575eae72/src/number_theory/bertrand.lean#L539\">https://github.com/leanprover-community/mathlib/blob/6d2e589f73961949e6a430f926ce64f1575eae72/src/number_theory/bertrand.lean#L539</a> which is a rather awful proof I did recently that <code>2/3 ≤ log 2</code>. Absolutely trivial by decimal expansion, but you have to show that your expansions are sufficiently precise. (I'm quite sure it's possible to do better than how we ended up doing it, but that's the class of \"extremely painful\" we're talking about, I think.)</p>\n</blockquote>\n<p>For the record, you can prove this with</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">lemma</span> <span class=\"n\">two_div_three_lt_log_two</span> <span class=\"o\">:</span> <span class=\"o\">(</span><span class=\"mi\">2</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"bp\">/</span> <span class=\"mi\">3</span> <span class=\"bp\">≤</span> <span class=\"n\">log</span> <span class=\"mi\">2</span> <span class=\"o\">:=</span>\n<span class=\"n\">le_trans</span> <span class=\"o\">(</span><span class=\"kd\">by</span> <span class=\"n\">norm_num</span><span class=\"o\">)</span> <span class=\"n\">log_two_gt_d9.le</span>\n</code></pre></div>",
        "id": 243974183,
        "sender_full_name": "Bhavik Mehta",
        "timestamp": 1624657172
    },
    {
        "content": "<p>I think that it would be a bad idea to focus on \"computational proofs\" when looking at areas to improve. These are already well covered, just not in lean. The techniques are well known and this is a relatively easy task. (I'm referring to things like <code>2/3 &lt;= log 2</code> here, not Heather's examples, which are things that a mathematician would consider computation but involve more parts of the library and are not as well-defined algorithmic.)</p>",
        "id": 243994226,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1624681997
    },
    {
        "content": "<p>Another similar example is one of the exercises that I came across:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">data.real.basic</span>\n<span class=\"kn\">import</span> <span class=\"n\">data.real.sqrt</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">aopsbook_v2_c13_ex7</span>\n  <span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"n\">y</span> <span class=\"n\">z</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₀</span> <span class=\"o\">:</span> <span class=\"mi\">0</span> <span class=\"bp\">&lt;</span> <span class=\"n\">x</span> <span class=\"bp\">∧</span> <span class=\"mi\">0</span> <span class=\"bp\">&lt;</span> <span class=\"n\">y</span> <span class=\"bp\">∧</span> <span class=\"mi\">0</span> <span class=\"bp\">&lt;</span> <span class=\"n\">z</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₁</span> <span class=\"o\">:</span> <span class=\"n\">x</span> <span class=\"bp\">*</span> <span class=\"n\">y</span> <span class=\"bp\">=</span> <span class=\"mi\">12</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₂</span> <span class=\"o\">:</span> <span class=\"n\">y</span> <span class=\"bp\">*</span> <span class=\"n\">z</span> <span class=\"bp\">=</span> <span class=\"mi\">18</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₃</span> <span class=\"o\">:</span> <span class=\"n\">z</span> <span class=\"bp\">*</span> <span class=\"n\">x</span> <span class=\"bp\">=</span> <span class=\"mi\">24</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"n\">y</span><span class=\"o\">,</span> <span class=\"n\">z</span><span class=\"o\">)</span> <span class=\"bp\">=</span> <span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">6</span><span class=\"o\">)</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"n\">repeat</span> <span class=\"o\">{</span> <span class=\"n\">refine</span> <span class=\"n\">congr</span> <span class=\"o\">(</span><span class=\"n\">congr_arg</span> <span class=\"n\">prod.mk</span> <span class=\"n\">_</span><span class=\"o\">)</span> <span class=\"n\">_</span> <span class=\"o\">}</span><span class=\"bp\">;</span> <span class=\"n\">nlinarith</span><span class=\"o\">,</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>Here <code>nlinarith</code> can close the goal soundly, but if I add a factor of <code>real.sqrt 2</code>, that will make <code>nlinarith</code> loss its way.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">data.real.basic</span>\n<span class=\"kn\">import</span> <span class=\"n\">data.real.sqrt</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">aopsbook_v2_c13_ex7</span>\n  <span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"n\">y</span> <span class=\"n\">z</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₀</span> <span class=\"o\">:</span> <span class=\"mi\">0</span> <span class=\"bp\">&lt;</span> <span class=\"n\">x</span> <span class=\"bp\">∧</span> <span class=\"mi\">0</span> <span class=\"bp\">&lt;</span> <span class=\"n\">y</span> <span class=\"bp\">∧</span> <span class=\"mi\">0</span> <span class=\"bp\">&lt;</span> <span class=\"n\">z</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₁</span> <span class=\"o\">:</span> <span class=\"n\">x</span> <span class=\"bp\">*</span> <span class=\"n\">y</span> <span class=\"bp\">=</span> <span class=\"mi\">12</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₂</span> <span class=\"o\">:</span> <span class=\"n\">y</span> <span class=\"bp\">*</span> <span class=\"n\">z</span> <span class=\"bp\">=</span> <span class=\"mi\">18</span>  <span class=\"bp\">*</span> <span class=\"n\">real.sqrt</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₃</span> <span class=\"o\">:</span> <span class=\"n\">z</span> <span class=\"bp\">*</span> <span class=\"n\">x</span> <span class=\"bp\">=</span> <span class=\"mi\">24</span>  <span class=\"bp\">*</span> <span class=\"n\">real.sqrt</span> <span class=\"mi\">2</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"n\">y</span><span class=\"o\">,</span> <span class=\"n\">z</span><span class=\"o\">)</span> <span class=\"bp\">=</span> <span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">6</span> <span class=\"bp\">*</span> <span class=\"n\">real.sqrt</span> <span class=\"mi\">2</span><span class=\"o\">)</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"c1\">-- repeat { refine congr (congr_arg prod.mk _) _ }; nlinarith,</span>\n  <span class=\"gr\">sorry</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>I wonder if it's a good direction for GPT-f to target this kind of \"not-fully automated\" but somehow \"mathematically trivial\" proofs? If I make an analogy it will look like proving ( ; ; 1 2 5 gcd ; ; 2 0 0 ) = ; 2 5 in Metamath, for which we can generate many structured proofs for training and then the machine is super capable in it.</p>\n<p>I would like to know if GPT-f can mitigate this and guide people through the computation playing a good role of assistant, is this a need of the community? Happy to hear what you think about it.</p>",
        "id": 244265570,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1624966016
    },
    {
        "content": "<p>One thing that I worry about is that this sounds kind of like a temporally solution: since this \"not-fully automated\" may be well mitigated by improving these high level tactics to make them more powerful, someday in the future an update of mathlib may make this effort become useless.</p>",
        "id": 244266439,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1624966620
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"230999\">@Daniel Selsam</span> automation to fill in proofs and subproofs seems like a promising target for machine learning since there is 100 percent ground truth if the resulting proof is valid (like DeepMind playing Breakout), although theory and experience provide some basis to wonder how easily this task can be automated. Human intervention will need to come from people who intimately understand the thing they are trying to at least partially automate, so your NLP friends may want to gain some experience with formalizing. Translation of formal proofs from other systems into Lean should be far easier than translating informal proofs into Lean, but experience also indicates that is hard. Mario's MM0 can translate Metamath proofs into Lean, which is impressive, but then you need to match up Metamath and Lean concepts (automation could surely help), and even then, a 500K line proof of Dirichlet's theorem cannot be added to mathlib (better tools/automation for refactoring again might help).</p>",
        "id": 244840981,
        "sender_full_name": "Hunter Monroe",
        "timestamp": 1625374576
    }
]