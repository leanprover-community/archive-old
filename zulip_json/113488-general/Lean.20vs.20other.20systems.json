[
    {
        "content": "<p>There are many systems that allow you to write and check proofs written in a formal language. Lean, Metamath, Isabelle HOL, Coq, ACL2, Agda, Albatross, F*, HOL Light, HOL4, LEGO, Mizar, NuPRL, PVS, Twelf</p>\n<p>I'm just wondering if we are better or worse off having all these different systems. Should we all just stick to one and build a good library in it, or is there some fundamental reasons why one is better than another. Yes, Lean has a computational component that Metamath doesn't, but nothing is stopping you from writing a model of a real computer in Metamath. Yes, Lean has proof automation that Metamath doesn't have, but aren't big general  theorems exactly the same as proof automation? I'd personally be especially interested in hearing <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> s thoughts on this. Pondering this I'm reminded of  a song called \"One Vision\" by Queen..</p>",
        "id": 179166943,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572175471
    },
    {
        "content": "<p>I am largely agnostic as to foundations. But I do think that we should make every effort to connect all the proof systems together, because how else can we make proofs \"stick\"? It would be quite the nightmare if we can never get off the ground because we reinvent natural number addition every five years in a new system</p>",
        "id": 179167011,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572175628
    },
    {
        "content": "<p>Actually, I prefer simple foundations to complex ones because the complex foundations fool you into thinking that you have something that is easier than it is; as soon as you stray from what the axioms give you things become hard again</p>",
        "id": 179167069,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572175744
    },
    {
        "content": "<p>Are things easier in Lean though, than in Metamath, after having actually tried to do complicated things in Metamath?</p>",
        "id": 179167129,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572175850
    },
    {
        "content": "<p>The most important feature a foundation has is <em>expressivity</em>, the ability to define the things people care about, with proportional overhead (not exponential overhead!). If you can't write X in the system, then you are doomed, although you can delay the inevitable for a while</p>",
        "id": 179167145,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572175890
    },
    {
        "content": "<p>The problem with questions like that is it presupposes that Metamath and Lean are trying to solve the same problems, and they aren't</p>",
        "id": 179167190,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572175928
    },
    {
        "content": "<p>Metamath is a logical foundation and a verification system. It has a \"build your own\" user experience</p>",
        "id": 179167198,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572175969
    },
    {
        "content": "<p>so asking if Metamath has tactics is beside the point; it's not supposed to have tactics, it's a foundational system</p>",
        "id": 179167208,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176004
    },
    {
        "content": "<p>Of course you want tactics and a nice user experience, but I think metamath is one of the few systems that conciously separates this from the verification aspect</p>",
        "id": 179167268,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176047
    },
    {
        "content": "<p>I'm just wondering whether there is any fundamental reason why writing general theorems in metamath in order to help proving things is any different from writing tactics in Lean?</p>",
        "id": 179167276,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176101
    },
    {
        "content": "<p>Theorems and tactics are different. A tactic is more like a metatheorem, a family of related theorems that are all provable</p>",
        "id": 179167290,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176145
    },
    {
        "content": "<p>btw. there are efforts to connect theorem provers, e.g. <a href=\"http://logipedia.inria.fr/about/about.php\" target=\"_blank\" title=\"http://logipedia.inria.fr/about/about.php\">Logipedia/Dedukti</a></p>",
        "id": 179167340,
        "sender_full_name": "Kevin Kappelmann",
        "timestamp": 1572176168
    },
    {
        "content": "<p>the downside of a tactic is that you have to run it every time you want a new proof, whereas you can check a theorem once and use it as many times as you like</p>",
        "id": 179167341,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176180
    },
    {
        "content": "<p>I'm not an expert on this so please correct me if I'm completely wrong, but doesn't expressive foundations allow you to write category theory kinds of general theorems that are basically the same thing as tactics? I'm just guessing.</p>",
        "id": 179167349,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176218
    },
    {
        "content": "<p>An example of something that can't be proven by a single theorem is <code>norm_num</code></p>",
        "id": 179167367,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176249
    },
    {
        "content": "<p>that is, evaluating arbitrary arithmetic expressions</p>",
        "id": 179167373,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176267
    },
    {
        "content": "<p>So the problem is that you can't make the metamath proof checker do complex calculations? You have to spell out the calculations in the proof? and then you need to have a metasystem that writes those long calculations?</p>",
        "id": 179167424,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176320
    },
    {
        "content": "<p>yes</p>",
        "id": 179167425,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176326
    },
    {
        "content": "<p>But is this actually true?</p>",
        "id": 179167427,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176338
    },
    {
        "content": "<p>This is a benefit</p>",
        "id": 179167433,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176365
    },
    {
        "content": "<p>By an appropriate sequence of lemmas, you can make the metamath proof checker perform any computation in NP</p>",
        "id": 179167437,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176393
    },
    {
        "content": "<p>the same holds true for lean btw</p>",
        "id": 179167480,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176423
    },
    {
        "content": "<p>So there is no way to make metamath proof checker perform an arbitrary calculation by writing down the definition of the calculation as a turing machine and proving that the calculation terminates? Because if you could, couldn't you somehow transfer the calculation work on to the proof checker side? that is make a tactic that's written inside the system?</p>",
        "id": 179167496,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176518
    },
    {
        "content": "<p>As in you can't say, this theorem holds because this tactic proof searching algorithm is proven to terminate, and the proof is simply to run the program?</p>",
        "id": 179167538,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176576
    },
    {
        "content": "<p>You have to provide a proof that the TM evaluates to some particular value, and that means roughly one proof step per TM step</p>",
        "id": 179167539,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176578
    },
    {
        "content": "<p>so the longer the program runs, the longer the proof gets</p>",
        "id": 179167546,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176597
    },
    {
        "content": "<p>metamath doesn't support extending the kernel by a verified computation. I hope to add this capability to MM0 after the bootstrap</p>",
        "id": 179167551,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176636
    },
    {
        "content": "<p>It's still pretty darn fast without it</p>",
        "id": 179167593,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176650
    },
    {
        "content": "<p>So, the point is that tactics may not terminate, and you can't make a proof checker run something that may not terminate. and metamath is only a proof checker.</p>",
        "id": 179167611,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572176732
    },
    {
        "content": "<blockquote>\n<p>this theorem holds because this tactic proof searching algorithm is proven to terminate</p>\n</blockquote>\n<p>It would be a very strange theorem that would not be provable outright if you had this property</p>",
        "id": 179167654,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176767
    },
    {
        "content": "<p>the only reason it doesn't work for things like <code>norm_num</code> is because the theorems are not uniformly describable</p>",
        "id": 179167660,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176804
    },
    {
        "content": "<p>termination or lack thereof is not a problem. Metamath simply doesn't know how computers work so it can't extend itself with a computer program given by the user</p>",
        "id": 179167708,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176884
    },
    {
        "content": "<p>If we're running the program, then there is no problem with termination - we continue verification only after it returns, so if it never returns then we never finish the proof</p>",
        "id": 179167721,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176938
    },
    {
        "content": "<p>If we are not running the program, and are simply using the existence of the program to prove the theorem, then we may as well chuck the program and prove the theorem directly</p>",
        "id": 179167731,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572176972
    },
    {
        "content": "<p>I'm just guessing again but isn't the point of all these new type theory systems to kind of make a unification algorithm that can be easily used as a general purpose computer?</p>",
        "id": 179167780,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177030
    },
    {
        "content": "<p>that's the theory...</p>",
        "id": 179167782,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177045
    },
    {
        "content": "<p>turns out computers are better at being computers than unification algorithms are at being computers</p>",
        "id": 179167791,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177073
    },
    {
        "content": "<p>theory and practice are the same in theory but not in practice</p>",
        "id": 179167798,
        "sender_full_name": "Kenny Lau",
        "timestamp": 1572177115
    },
    {
        "content": "<p>I like how metamath is such a simple system and then you just have a list of axioms. Could we do kind of a similar thing where we could extend the unification algorithm by just adding an extra axiom?</p>",
        "id": 179167863,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177193
    },
    {
        "content": "<p>Lean is actually extensible in this way, but it's a \"big boy\" feature and was not entrusted to us. Currently the only extensions to the normalization algorithm are quotients and inductive types</p>",
        "id": 179167910,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177249
    },
    {
        "content": "<p>If you add your own rules it becomes very easy to loop or lose some metatheoretic property</p>",
        "id": 179167920,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177285
    },
    {
        "content": "<p>and you will almost certainly make defeq undecidable</p>",
        "id": 179167928,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177308
    },
    {
        "content": "<p>Basically, in DTT defeq is \"second class\" equality - it's never strong enough, but you can't just ignore it. I like that in metamath there is only one equality</p>",
        "id": 179167978,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177365
    },
    {
        "content": "<p>But couldn't you just add a feature that allows you to prove things by saying that the proof exist because a computation terminates, and then you can just add a \"proof by termination\" and the checker runs the computation. Of course you may not actually have the proof.</p>",
        "id": 179167983,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177375
    },
    {
        "content": "<p>But thats basically what a tactic is.</p>",
        "id": 179167986,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177410
    },
    {
        "content": "<p>right, that's what I mean by extending the kernel with a verified computation</p>",
        "id": 179167988,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177417
    },
    {
        "content": "<p>that's not the same as a tactic, which is a <em>proof-producing program</em></p>",
        "id": 179167994,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177431
    },
    {
        "content": "<p>metamath prefers to deal with tactics in that sense, because the checker doesn't have to know anything about the tactic for the system to work</p>",
        "id": 179167999,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177476
    },
    {
        "content": "<p>Well, basically the same, as the program may not actually terminate when you write \"proof by termination\" and run the checker?</p>",
        "id": 179168000,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177477
    },
    {
        "content": "<p>The checker is given the output of the tactic as its input. It is never given the opportunity to run the tactic itself</p>",
        "id": 179168042,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177521
    },
    {
        "content": "<p>so when you start the checker you already have the proof of termination in hand</p>",
        "id": 179168057,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177546
    },
    {
        "content": "<p>Ah, right. A tactic would be adding that the program terminates as an axiom.</p>",
        "id": 179168058,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177550
    },
    {
        "content": "<p>And seeing what happens.</p>",
        "id": 179168060,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177557
    },
    {
        "content": "<p>Lean doesn't have trusted computations either btw</p>",
        "id": 179168064,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177580
    },
    {
        "content": "<p>It very nearly does, but you can't use <code>#eval</code> to prove a reflexivity</p>",
        "id": 179168096,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177603
    },
    {
        "content": "<p>I want to build my own proof system. Maybe we should collaborate on extending metamath? Personally, I'd like to build a system where you can solve real world problems by checking proofs in order of length.</p>",
        "id": 179168142,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177716
    },
    {
        "content": "<p>that will never scale unfortunately</p>",
        "id": 179168186,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177741
    },
    {
        "content": "<p>the number of proofs grows exponentially</p>",
        "id": 179168188,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177754
    },
    {
        "content": "<p>Well, it of course depends on the language. There's always some weird language where it works, of course. Because you can always put the computation on the checker side.</p>",
        "id": 179168203,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177795
    },
    {
        "content": "<p>There actually was a competition of sorts to find the shortest proofs of a bunch of basic propositional logic problems, and many of them were found by exhaustive search, but you hit a wall at about 18 steps</p>",
        "id": 179168206,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177819
    },
    {
        "content": "<p>I think a big problem in computer systems is fragility, that is, systems work great, until they don't.</p>",
        "id": 179168260,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177912
    },
    {
        "content": "<p>Like, some tactic might work great, until it doesn't. And then the next best thing is basically to go through proofs in order of length.</p>",
        "id": 179168307,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177968
    },
    {
        "content": "<p>Whenever you are solving an exponential problem it always looks like that. You might push the wall back a few steps with a different formalism, but you can't eliminate it unless your language is trivial</p>",
        "id": 179168308,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572177975
    },
    {
        "content": "<p>Having reasonable solutions there in between is where the improvements in systems happen.</p>",
        "id": 179168311,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572177996
    },
    {
        "content": "<p>machine learning is all about trying to solve these problems by finding structure in the search (something better than \"shorter is better\")</p>",
        "id": 179168330,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572178035
    },
    {
        "content": "<p>I mean reasonable solutions for practical problems, you obviously can't have reasonable solutions for all problems.</p>",
        "id": 179168331,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572178038
    },
    {
        "content": "<p>I think trying to build something between the two ends of search proofs by length and a few tactics that solve simple problems would be to work on trying to \"dig a tunnel\" from both ends, trying to make searching proofs in order of length better while making tactics better.</p>",
        "id": 179168417,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572178170
    },
    {
        "content": "<p>that at most doubles the distance to the wall</p>",
        "id": 179168476,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572178259
    },
    {
        "content": "<p>I mean I just think it could provide ideas to look at things from many angles.</p>",
        "id": 179168532,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572178341
    },
    {
        "content": "<blockquote>\n<p>I'm just wondering if we are better or worse off having all these different systems. Should we all just stick to one and build a good library in it</p>\n</blockquote>\n<p>Of course the problem with this is that everyone thinks that everybody else should be using their system. It also all depends on what your goals are.</p>\n<p>The work of Rob Lewis and his collaborators, and the perfectoid project, are attempts to prove that Lean is capable of doing modern mathematics which \"proper mathematicians\" are interested in (this is a tongue-in-cheek phrase referring to people doing stuff like number theory/algebra/analysis/geometry etc in maths departments, as opposed to all the category theory / type theory people working on foundations). Sebastian Gouezel has produced work in Isabelle/HOL which is also mainstream mathematics done in a theorem prover. But we are in the minority here -- most work done in these systems is not an attempt to do modern mathematics on a computer. For example my impression is that Agda is a fascinating experiment to see what kinds of fancy inductive-recursive-inductive types one can get away with whilst preserving soundness. That's great, but guess what? The perfectoid project <em>does not contain one single instance of the <code>inductive</code> command</em>. Us \"proper mathematicians\" just need structures. </p>\n<blockquote>\n<p>or is there some fundamental reasons why one is better than another.</p>\n</blockquote>\n<p>People can certainly argue about this point all day. I think that dependent type theory is currently our best option for \"all of pure mathematics\" and I think that Lean is currently our best option for dependent type theory, but plenty of people have plenty of other opinions. The reason I like my opinion best is that I am one of few \"proper mathematicians\" who is even in a position to have an opinion, and when I hear other people talking about what they think mathematics is and how it fits best into their systems, I often find that the underlying disagreement is actually the question of what mathematics <em>is</em>. I am attempting to represent the generic pure mathematician who works in a generic mathematics department. </p>\n<p>As is the case for programming languages in general, the best tool for the job depends very heavily on what the job is. An analogue of your question might be \"I'm just wondering if we are better of worse off having all these different programming languages -- C, Rust, Java, Python, x86 assembly. Should we all just stick to one?\"</p>",
        "id": 179168537,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1572178379
    },
    {
        "content": "<p>To me, as someone who isn't that knowledgeable of foundational systems, it seems like a pretty practical way to describe a foundational system would be a list of axtioms that state that a computer program terminates. The lengths of various proofs and relative (between theorems) performance of the checker would be determined by which axioms you add. You could make a checker that both runs a computation to check for the proof and at the same time searches for proofs that the computation terminates so as to not have to do the computation. And I guess you can also describe the axiomatic content this way as well.</p>",
        "id": 179168864,
        "sender_full_name": "Juho Kupiainen",
        "timestamp": 1572179023
    },
    {
        "content": "<blockquote>\n<p>To me, as someone who isn't that knowledgeable of foundational systems, it seems like a pretty practical way to describe a foundational system would be a list of axtioms that state that a computer program terminates. The lengths of various proofs and relative (between theorems) performance of the checker would be determined by which axioms you add. And of course you can describe the axiomatic content this way as well.</p>\n</blockquote>\n<p>I guess this list of axioms are more or less the rules for recursors of inductive types.</p>",
        "id": 179168990,
        "sender_full_name": "Chris Hughes",
        "timestamp": 1572179238
    },
    {
        "content": "<p>I have a related question about foundations and how they play with different proof systems.</p>\n<p>I've been reading about topos theory lately, which I understand in some ways is an alternative foundations for mathematics, where there's a clearer distinction between the types of foundational assumptions we make.</p>\n<p>This may sound hilarious to the mathematicians, but as a programmer I think of it as being a foundation with built-in axiomatic/foundational polymorphism, in the sense that I can define mathematical objects like the real numbers that are independent of the topos I work in, so by delaying the choice of topos that restricts the type of things I can say, I get a nicer organization of theorems based on the types of assumptions that have been made.</p>\n<p>So to my naive understanding, it feels like doing math in topos theory is analogous to following good software engineering practices in maintaining a large code base. Is this a valid comparison?</p>\n<p>If so, does the choice of foundations between something like Metamath/STT/DTT make a difference in working towards a goal like this?</p>",
        "id": 179170648,
        "sender_full_name": "Olli",
        "timestamp": 1572182063
    },
    {
        "content": "<blockquote>\n<p>relative (between theorems) performance of the checker would be determined by which axioms you add.</p>\n</blockquote>\n<p>If you need to add axioms to make a proof check faster (by more than a constant additive factor), then the prover is not expressive enough</p>",
        "id": 179170880,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572182418
    },
    {
        "content": "<blockquote>\n<p>If so, does the choice of foundations between something like Metamath/STT/DTT make a difference in working towards a goal like this?</p>\n</blockquote>\n<p>Note that Metamath does not belong on that list - it is not a mathematical foundation, it is an environment that lets you define your mathematical foundation. Isabelle has a similar distinction, if you are familiar with the difference between Isabelle/Pure vs Isabelle/HOL.</p>\n<p>Most formal systems let you pick your axioms, although lean doesn't really let you turn off a great deal of axioms that you might not want (like universes or proof irrelevance). To me, your topos theory example is similar to proving theorems with attention paid to the axioms you use. In metamath this is easy because the axioms are quite fine grained - there are 30 or so of them, leading to 2^30 different subsystems of ZFC you could study; or just forget ZFC and add your own axioms if you prefer something else. Lean has only 3 axioms, and the only really useful one to avoid is <code>choice</code>, so reverse mathematics is a non-starter.</p>",
        "id": 179171075,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572182815
    },
    {
        "content": "<p>Could you define CoIC inside Metamath and then work using that?</p>",
        "id": 179171226,
        "sender_full_name": "Olli",
        "timestamp": 1572183092
    },
    {
        "content": "<p>Yes</p>",
        "id": 179171359,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183316
    },
    {
        "content": "<p>I got most of the way to doing that with lean, but that was before my masters thesis so I wasn't really sure what the axioms for inductives were</p>",
        "id": 179171375,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183360
    },
    {
        "content": "<p>MLTT is not difficult</p>",
        "id": 179171415,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183371
    },
    {
        "content": "<p>There is a HOL library for metamath available at <a href=\"http://us.metamath.org/holuni/mmhol.html\" target=\"_blank\" title=\"http://us.metamath.org/holuni/mmhol.html\">http://us.metamath.org/holuni/mmhol.html</a></p>",
        "id": 179171429,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183446
    },
    {
        "content": "<p>the real bottleneck is usually getting an exact statement of the axiomatic system</p>",
        "id": 179171478,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183528
    },
    {
        "content": "<p>Ok, yeah I can see the analogy with it being the assembly language for proofs.</p>",
        "id": 179171494,
        "sender_full_name": "Olli",
        "timestamp": 1572183586
    },
    {
        "content": "<p>For my example with topos theory, let's say I make a definition that works in any elementary topos, and then I define a new topos, I can do this in independent order, and as soon as I'm done I should get a bunch of free theorems. If one was working in Metamath, then it feels like you'd have to be working one level higher, where the free theorems get automatically generated, does this sound correct?</p>",
        "id": 179171625,
        "sender_full_name": "Olli",
        "timestamp": 1572183810
    },
    {
        "content": "<p>If you prove a theorem using axioms A, B and apply it in a context where you are also assuming C, you don't need to do anything special</p>",
        "id": 179171682,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183876
    },
    {
        "content": "<p>What you don't get with this kind of embedding is the ability to reinterpret symbols. For example, if I prove a fact like <code>foo = foo</code> where <code>foo</code> is a \"fixed constant\" about which I know nothing, I cannot apply it with <code>foo := 2</code>. I can add an axiom saying <code>foo = 2</code>, and then <code>foo</code> acts like a definition, but I can't also add a definition <code>foo = 3</code> because that would be inconsistent</p>",
        "id": 179171755,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572183995
    },
    {
        "content": "<p>In order to really do the topos theory approach justice, you have to deep embed it, getting something more like the category theory library</p>",
        "id": 179171769,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572184073
    },
    {
        "content": "<p>this comes with some notational overhead unless you design the tool to deal with terms of this form</p>",
        "id": 179171818,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572184118
    },
    {
        "content": "<p>right, the reason we have higher level programming languages is not just to avoid typing less, but because our intended meaning of the program is better captured at a higher level, because the lower level aspects are deemed implementation details.</p>",
        "id": 179171823,
        "sender_full_name": "Olli",
        "timestamp": 1572184132
    },
    {
        "content": "<p>MM1 is higher level than MM0 is higher level than Metamath if I've understood things correctly</p>",
        "id": 179171837,
        "sender_full_name": "Olli",
        "timestamp": 1572184194
    },
    {
        "content": "<p>MM0 is more complex than metamath but they are equally expressive. MM1 is sugar over MM0</p>",
        "id": 179171900,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572184268
    },
    {
        "content": "<p>ah right</p>",
        "id": 179171902,
        "sender_full_name": "Olli",
        "timestamp": 1572184281
    },
    {
        "content": "<p>\"higher level\" is perhaps not the right characterization. All of them are capable of speaking at quite high levels mathematically</p>",
        "id": 179171910,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572184310
    },
    {
        "content": "<p>the thing that is low level is the actual mechanics of proving theorems. It's not like with computer languages where the low level languages are concerned with completely different things, different levels of abstraction</p>",
        "id": 179172023,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572184409
    },
    {
        "content": "<p>Hmm yeah it is possible that I fail to notice my own bias with thinking about software/programming languages.</p>\n<p>When I say higher level proof language, I mean a language that generates proofs that have a lot of repetition. The way Metamath allows you to inspect proofs down to very low level of detail is very nice, but the issue I have in mind is that if I write a definition in this higher level language, then the best way to understand its meaning is by reading it at this level, because we want to hide the repetition to increase the signal/noise ratio. Do you see this not being an issue?</p>",
        "id": 179172307,
        "sender_full_name": "Olli",
        "timestamp": 1572184876
    },
    {
        "content": "<p>I can give an example I have in mind: Take Lawvere's Fixpoint Theorem, that generalizes many other fixpoint theorems. I would like to see not just the proof of the LFT itself, but also how other theorems are instances of it. To me it feels like you would need to work on a higher semantic level than MM0 in order to do this, but maybe I'm mistaken?</p>",
        "id": 179172633,
        "sender_full_name": "Olli",
        "timestamp": 1572185408
    },
    {
        "content": "<blockquote>\n<p>When I say higher level proof language, I mean a language that generates proofs that have a lot of repetition. The way Metamath allows you to inspect proofs down to very low level of detail is very nice, but the issue I have in mind is that if I write a definition in this higher level language, then the best way to understand its meaning is by reading it at this level, because we want to hide the repetition to increase the signal/noise ratio. Do you see this not being an issue?</p>\n</blockquote>\n<p>Metamath is actually very non-repetitive, MM0 even more so. It would be nice to have some theorems about this, but they are pretty close to optimal. The repetition you see in the web page presentation is because each line is independent of the others, and common subterms are expanded during printing. In the actual proof it's totally deduplicated</p>",
        "id": 179173747,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572187493
    },
    {
        "content": "<blockquote>\n<p>I can give an example I have in mind: Take Lawvere's Fixpoint Theorem, that generalizes many other fixpoint theorems. I would like to see not just the proof of the LFT itself, but also how other theorems are instances of it. To me it feels like you would need to work on a higher semantic level than MM0 in order to do this, but maybe I'm mistaken?</p>\n</blockquote>\n<p>I don't know what Lawvere's Fixpoint Theorem is, but I'm 100% confident you can state it as a single theorem of MM/MM0. If you want to apply it in a particular circumstance, you prove that your category instantiates the assumptions, unfold the meaning of the symbols (i.e. maybe you want to replace \"generic morphism\" with \"function\") in the statement, and that's it.</p>",
        "id": 179173825,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572187675
    },
    {
        "content": "<p>the considerations are roughly the same if you wanted to do the same thing in lean. Declare the general concept of a category, do your work there, then instantiate the theory in the concrete case</p>",
        "id": 179173890,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572187766
    },
    {
        "content": "<p>Lean is actually far more repetitive than Metamath. This is the result of things like tactics that don't clean up after themselves in the output proof, <code>congr</code> generating congr lemmas from scratch every time it's called, typeclass inference not saving and reusing typeclass lookups, and kernel <code>rfl</code> proofs, which have to unfold huge equation compiler terms</p>",
        "id": 179174088,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572188069
    },
    {
        "content": "<p>Metamath looks more repetitive because of its print settings, but exactly because of that people have a much greater incentive to minimize repetition. In other theorem provers all the junk is hidden under the rug</p>",
        "id": 179174155,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572188187
    },
    {
        "content": "<p>to clarify, Lean is far more repetitive than Metamath during compilation/tactic execution; this is the cost of automation which allows the user to work at a higher level when writing Lean source code</p>",
        "id": 179175045,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1572189917
    },
    {
        "content": "<p>of course, your Lean server recompiles every time more than 200ms elapses between keystrokes, and while ideally you don't notice this because of caching, it's not hard for users to get to a place where they experience significant lag waiting on elaboration, typeclass resolution, and tactic execution</p>",
        "id": 179175293,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1572190365
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"126113\">@Olli</span> note that Lean's implementation of DTT naturally has semantics in locally cartesian closed categories, so it's easy to shallowly replay the proof of the LFPT in <code>Type</code>:</p>\n<div class=\"codehilite\"><pre><span></span><span class=\"kn\">import</span> <span class=\"n\">tactic</span>\n\n<span class=\"c1\">-- https://ncatlab.org/nlab/show/Lawvere%27s+fixed+point+theorem</span>\n\n<span class=\"n\">def</span> <span class=\"n\">δ</span> <span class=\"o\">{</span><span class=\"n\">X</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"o\">}</span> <span class=\"o\">:</span> <span class=\"n\">X</span> <span class=\"bp\">→</span> <span class=\"n\">X</span> <span class=\"bp\">×</span> <span class=\"n\">X</span> <span class=\"o\">:=</span> <span class=\"bp\">λ</span> <span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"bp\">⟨</span><span class=\"n\">x</span><span class=\"o\">,</span><span class=\"n\">x</span><span class=\"bp\">⟩</span>\n\n<span class=\"n\">def</span> <span class=\"kn\">eval</span> <span class=\"o\">{</span><span class=\"n\">A</span> <span class=\"n\">B</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"bp\">*</span><span class=\"o\">}</span> <span class=\"o\">:</span> <span class=\"o\">(</span><span class=\"n\">A</span> <span class=\"bp\">→</span> <span class=\"n\">B</span><span class=\"o\">)</span> <span class=\"bp\">×</span> <span class=\"n\">A</span> <span class=\"bp\">→</span> <span class=\"n\">B</span> <span class=\"o\">:=</span> <span class=\"bp\">λ</span> <span class=\"n\">pr</span><span class=\"o\">,</span> <span class=\"n\">pr</span><span class=\"bp\">.</span><span class=\"mi\">1</span> <span class=\"n\">pr</span><span class=\"bp\">.</span><span class=\"mi\">2</span>\n\n<span class=\"kn\">lemma</span> <span class=\"n\">lfpt</span> <span class=\"o\">{</span><span class=\"n\">A</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">B</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"o\">}</span> <span class=\"o\">(</span><span class=\"n\">ϕ</span> <span class=\"o\">:</span> <span class=\"n\">A</span> <span class=\"bp\">→</span> <span class=\"n\">A</span> <span class=\"bp\">→</span> <span class=\"n\">B</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">Hf</span> <span class=\"o\">:</span> <span class=\"n\">function</span><span class=\"bp\">.</span><span class=\"n\">surjective</span> <span class=\"n\">ϕ</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"bp\">∀</span> <span class=\"n\">f</span> <span class=\"o\">:</span> <span class=\"n\">B</span> <span class=\"bp\">→</span> <span class=\"n\">B</span><span class=\"o\">,</span> <span class=\"bp\">∃</span> <span class=\"n\">s</span> <span class=\"o\">:</span> <span class=\"n\">B</span><span class=\"o\">,</span> <span class=\"n\">f</span> <span class=\"n\">s</span> <span class=\"bp\">=</span> <span class=\"n\">s</span> <span class=\"o\">:=</span>\n<span class=\"k\">begin</span>\n  <span class=\"n\">intro</span> <span class=\"n\">f</span><span class=\"o\">,</span> <span class=\"n\">cases</span> <span class=\"n\">Hf</span> <span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"err\">∘</span> <span class=\"kn\">eval</span> <span class=\"err\">∘</span> <span class=\"o\">(</span><span class=\"n\">prod</span><span class=\"bp\">.</span><span class=\"n\">map</span> <span class=\"n\">ϕ</span> <span class=\"n\">id</span><span class=\"o\">)</span> <span class=\"err\">∘</span> <span class=\"n\">δ</span><span class=\"o\">)</span> <span class=\"k\">with</span> <span class=\"n\">p</span> <span class=\"n\">Hp</span><span class=\"o\">,</span> <span class=\"n\">use</span> <span class=\"n\">ϕ</span> <span class=\"n\">p</span> <span class=\"n\">p</span><span class=\"o\">,</span>\n  <span class=\"n\">conv_rhs</span> <span class=\"o\">{</span> <span class=\"n\">rw</span> <span class=\"n\">Hp</span> <span class=\"o\">},</span> <span class=\"n\">refl</span>\n<span class=\"kn\">end</span>\n\n<span class=\"kn\">lemma</span> <span class=\"n\">cantor</span> <span class=\"o\">{</span><span class=\"n\">X</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span><span class=\"o\">}</span> <span class=\"o\">(</span><span class=\"n\">ϕ</span> <span class=\"o\">:</span> <span class=\"n\">X</span> <span class=\"bp\">→</span> <span class=\"o\">(</span><span class=\"n\">set</span> <span class=\"n\">X</span><span class=\"o\">))</span> <span class=\"o\">:</span> <span class=\"bp\">¬</span> <span class=\"n\">function</span><span class=\"bp\">.</span><span class=\"n\">surjective</span> <span class=\"n\">ϕ</span> <span class=\"o\">:=</span>\n<span class=\"k\">begin</span>\n  <span class=\"n\">intro</span> <span class=\"n\">H</span><span class=\"o\">,</span> <span class=\"n\">cases</span> <span class=\"n\">lfpt</span> <span class=\"n\">ϕ</span> <span class=\"n\">H</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"n\">p</span><span class=\"o\">,</span> <span class=\"bp\">¬</span> <span class=\"n\">p</span><span class=\"o\">)</span> <span class=\"k\">with</span> <span class=\"n\">p</span> <span class=\"n\">Hp</span><span class=\"o\">,</span> <span class=\"n\">ifinish</span>\n<span class=\"kn\">end</span>\n</pre></div>",
        "id": 179177310,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1572193714
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> thanks, I was going to attempt this on my own but it's good to have something to look at if I get stuck :)</p>",
        "id": 179177389,
        "sender_full_name": "Olli",
        "timestamp": 1572193863
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> if I were to prove LFPT in Metamath and then show that Cantor's Theorem follows from it, would I be assuming <code>lfpt</code> and deriving <code>canth</code>, or would I have to make a new statement different from <code>canth</code> that I know means the same thing?</p>",
        "id": 179177441,
        "sender_full_name": "Olli",
        "timestamp": 1572193944
    },
    {
        "content": "<p>I think the larger issue I am trying to understand is if there is a way for me to see what are some examples of theorems/definitions that have very high generality (such as LFPT) simply by looking at the shape of the set of proof derivations in the database of theorems I'm working in (so probably <code>set.mm</code> in Metamath)</p>",
        "id": 179177506,
        "sender_full_name": "Olli",
        "timestamp": 1572194055
    },
    {
        "content": "<p>You would apply <code>lfpt</code> to get something that is not exactly <code>canth</code> but is equivalent to it (defeq in lean terminology)</p>",
        "id": 179177511,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194101
    },
    {
        "content": "<p>you would have to postprocess the statement to get rid of the category theory notations</p>",
        "id": 179177522,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194122
    },
    {
        "content": "<p>There isn't that much category theory, but here's the yoneda lemma: <a href=\"http://us.metamath.org/mpeuni/yoneda.html\" target=\"_blank\" title=\"http://us.metamath.org/mpeuni/yoneda.html\">http://us.metamath.org/mpeuni/yoneda.html</a></p>",
        "id": 179177563,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194214
    },
    {
        "content": "<p>To continue with the analogy of maintaining software at large, I am wondering if we have the same conflict between adding new features (theorems) vs. abstracting/decoupling them from each other, and how much does the foundation we choose have to do with the ease of organizing things nicely</p>",
        "id": 179177674,
        "sender_full_name": "Olli",
        "timestamp": 1572194415
    },
    {
        "content": "<p>I think that the cost of abstraction is higher in (formal) math than it is in software, because you have to filter everything through an extra definitional layer</p>",
        "id": 179177802,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194686
    },
    {
        "content": "<p>the foundation doesn't matter at all. The only thing a foundation can do for you is make certain things impossible and other things difficult</p>",
        "id": 179177820,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194737
    },
    {
        "content": "<p>the job of making things easy is left to the user interface</p>",
        "id": 179177865,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194774
    },
    {
        "content": "<p>it's interesting to think about especially when it comes to the prospects of having some kind of an AI helping us do mathematics. The first thing that comes to mind is a superhuman tactic to solve a goal, but wouldn't it be even more valuable having suggestions on how to organize the material we already have in a way that makes expanding it easier?</p>",
        "id": 179177881,
        "sender_full_name": "Olli",
        "timestamp": 1572194831
    },
    {
        "content": "<p>yes? What does that have to do with foundations?</p>",
        "id": 179177936,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572194916
    },
    {
        "content": "<p>To give a simple example of why foundations don't matter, suppose I rewrote the lean kernel to produce typechecking proofs that it fed to metamath / ZFC as an oracle, and reports success if the backend prover accepts the proof. The front end wouldn't change at all, mathlib would still compile, you wouldn't even notice the difference - yet now the foundation is fundamentally different</p>",
        "id": 179178023,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572195074
    },
    {
        "content": "<p>this is why I see a type system as a front end feature</p>",
        "id": 179178069,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572195129
    },
    {
        "content": "<p>if we think of the set of theorems we have as a data structure, then I think a highly general theorem such as LFPT should be a densely connected node, or there should be some metric for valuing these types of theorems over ones that are less general. I mean I am not a mathematician so this might be very naive, but if we were to throw process cycles at a database of theorems in the hopes of making breakthroughs that connect areas of mathematics we had not realized were related, then this type of activity would be essential</p>",
        "id": 179178088,
        "sender_full_name": "Olli",
        "timestamp": 1572195190
    },
    {
        "content": "<p>densely connected node is probably an incorrect description, but I hope you understand what I mean by valuing generality</p>",
        "id": 179178142,
        "sender_full_name": "Olli",
        "timestamp": 1572195263
    },
    {
        "content": "<p>Honestly, given Jesse's demonstration of the proof, I don't value it very highly. The proof is apparently only two lines, which means that if you try to apply this theorem in a particular category the unpacking overhead will be more than the proof itself</p>",
        "id": 179178159,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572195316
    },
    {
        "content": "<p>I always tend to get this impression from category theory proofs</p>",
        "id": 179178167,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572195348
    },
    {
        "content": "<p>the savings from extracting a theorem is (number of uses - 1) * (size of proof) - definitions needed to state the theorem - (number of uses) * (unpacking overhead)</p>",
        "id": 179178271,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572195505
    },
    {
        "content": "<p>if unpacking overhead is small and no new definitions are needed, then it's a pretty low bar to meet, but definitions and unpacking overhead often go together, and category theory is full of small abstract theorems</p>",
        "id": 179178343,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572195604
    },
    {
        "content": "<p>the unpacking overhead is high because it is applicable in a wider set of contexts, right? i.e. we are talking about breadth vs. depth</p>",
        "id": 179178532,
        "sender_full_name": "Olli",
        "timestamp": 1572195898
    },
    {
        "content": "<p>Unpacking overhead depends on a few things. Roughly, it depends on the difference between the putative generalization and the concrete instance that it's supposed to be generalizing. Oftentimes it's a literal generalization, as in you have just universally quantified some variable that was already there. Rather than prove 2 + 3 = 3 + 2 we prove a + b = b + a. This kind of generalization has almost no overhead.</p>\n<p>A bigger overhead comes from unpacking structures; for example we want that to follow from an abelian group axiom, so we have some <code>&lt;G, +&gt; \\in AbGroup</code> and have a theorem <code>&lt;G, +&gt; \\in AbGroup -&gt; a \\in G -&gt; b \\in G -&gt; a + b = b + a</code>. This is still not so bad, because I've unfolded the structure. If it shows up bundled, it might instead be <code>S \\in AbGroup -&gt; a \\in carrier G -&gt; b \\in carrier G -&gt; add G a b = add G b a</code> and now we have to instantiate this with <code>&lt;G, +&gt;</code>, and then replace <code>add &lt;G, +&gt;</code> with <code>+</code> and <code>carrier &lt;G, +&gt;</code> with <code>G</code> everywhere they appear</p>",
        "id": 179178741,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572196304
    },
    {
        "content": "<p>If, as in lean, we didn't bother to remember that e.g. <code>int</code> is an abelian group, but instead have to traverse the algebraic hierarchy to find the proof, then the overhead is a bit more.</p>",
        "id": 179178795,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1572196387
    },
    {
        "content": "<p>right, so you are saying that you don't value this theorem as much because its abstractness makes it less useful in digging deeper faster, meaning it would not be a useful piece of a superhuman tactic, as it would slow it down.</p>\n<p>What I am getting at is that this type of theorem is useful not as a building block, but as a goal in itself, i.e. I care less about theorems that go very deep, and more about the ones that bridge a lot of gaps.</p>",
        "id": 179178869,
        "sender_full_name": "Olli",
        "timestamp": 1572196549
    },
    {
        "content": "<p>I concede that what I initially wrote is what your point refutes, but it made me realize that an actually useful AI for doing mathematics probably should wrestle with these two conflicting goals of digging deep into a search tree, and the conflicting objective of organizing theorems in such a way that makes them slow to use (but insightful to read)</p>",
        "id": 179179433,
        "sender_full_name": "Olli",
        "timestamp": 1572197610
    }
]