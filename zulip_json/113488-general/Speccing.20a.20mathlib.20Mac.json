[
    {
        "content": "<p>I'm thinking about buying a new Mac laptop to replace my struggling 2013 MacBook Pro.  If you were speccing out a machine that's primarily intended for mathlib work, what would you be looking for?  For example (and sorry for the very basic questions!): </p>\n<ul>\n<li>Should I be looking to get more memory or more CPU/GPU cores?</li>\n<li>Is there a threshold on memory or cores beyond which I won't see any noticeable benefit, or can VSCode use whatever I can throw at it?</li>\n<li>I saw that some people were having issues with getting things set up on M1 machines.  Are these issues generally resolved now?</li>\n<li>If you're using a Mac, what are you using?</li>\n<li>If you've upgraded from an old Mac to a new one have you seen appreciable improvements in performance when working on mathlib, or should I lower my expectations?</li>\n</ul>",
        "id": 285550611,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1654787907
    },
    {
        "content": "<p>If you only care about running one lean project at a time, 16~20 GB RAM should be fine. Maxing out on CPU will help a lot, there's a lot of stuff that can be parallelized when building Lean projects. GPU isn't helpful at the moment.</p>",
        "id": 285550842,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1654788015
    },
    {
        "content": "<p>I can confirm that 16GB is not enough anymore for compiling mathlib on my machine, so I am considering upgrading to 24 or 32 GB</p>",
        "id": 285550929,
        "sender_full_name": "Anne Baanen",
        "timestamp": 1654788060
    },
    {
        "content": "<p>I think <span class=\"user-mention\" data-user-id=\"306577\">@Matthew Ballard</span> has experience running lean on the newer MacBook</p>",
        "id": 285550991,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1654788092
    },
    {
        "content": "<p>I've been doing my mathlib work exclusively on gitpod for months now since my 8GB is nowhere near enough</p>",
        "id": 285551152,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1654788148
    },
    {
        "content": "<p>I'm assuming that even with a powerful new computer I would still be relying on pushing stuff up to GitHub and letting the remote machine do the heavy work, rather than recompiling mathlib locally?</p>",
        "id": 285551260,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1654788208
    },
    {
        "content": "<p>For completeness, I'm on an M1 MacBook Air with no upgrades == 8GB RAM, and have no real complaints honestly, but I do do a lot less work than others so it's possible I'd be more frustrated if I did. If I have a need to compile mathlib I do it on a beefier machine, or really I let GH do it and just pull the oleans, but I compile lean, lean4 and friends regularly.</p>",
        "id": 285551833,
        "sender_full_name": "Julian Berman",
        "timestamp": 1654788435
    },
    {
        "content": "<p>My data point. I had a M1 Macbook Air with 8 gb and building all mathlib was not possible. I now have a MacBook Pro with an M1 max with 64gb and it takes a little under an hour to build mathlib. </p>\n<p>My belief, from experience and looking at other benchmarking, is that the processor bump from M1 to M1 max would not be seriously noticeable for these tasks.</p>",
        "id": 285552152,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1654788552
    },
    {
        "content": "<p>FYI. The new M2 lifts the memory upper limit to 24gb.</p>",
        "id": 285552391,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1654788636
    },
    {
        "content": "<p>I'm using a <a href=\"https://everymac.com/systems/apple/macbook_pro/specs/macbook-pro-core-i7-2.6-15-dual-graphics-late-2013-retina-display-specs.html\">late 2013 MacBook Pro (version 11,3)</a> and a custom built desktop (Intel Core i5 8500 3.0 GHz; 16G RAM) running Arch Linux. I have the impression that the 6 cores/6 threads of the desktop are faster than the 4 cores/8 threads of the MacBook, so upgrading a CPU definitely makes sense to me.</p>",
        "id": 285552541,
        "sender_full_name": "Anne Baanen",
        "timestamp": 1654788694
    },
    {
        "content": "<p>My m1 pro 16GB macbook with lean compiled to ARM runs something like 2x faster than my desktop with a Ryzen 3600+32GB, and as of a month or two ago could compile mathlib.</p>",
        "id": 285554351,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1654789394
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"284160\">Eric Rodriguez</span> <a href=\"#narrow/stream/113488-general/topic/Speccing.20a.20mathlib.20Mac/near/285554351\">said</a>:</p>\n<blockquote>\n<p>My m1 pro 16GB macbook with lean compiled to ARM runs something like 2x faster than my desktop with a Ryzen 3600+32GB, and as of a month or two ago could compile mathlib.</p>\n</blockquote>\n<p>Did you notice a performance bump when moving to native from emulated lean?</p>",
        "id": 285557579,
        "sender_full_name": "Matthew Ballard",
        "timestamp": 1654790802
    },
    {
        "content": "<p>Can I use reported benchmarks to extrapolate anything about the day-to-day experience of doing mathlib work?  For example, I'm looking at a computer whose single-core Geekbench scores are 2-3 times better than my current machine, and whose multi-core scores are 6-8 times better.  So, for example, when I edit a file and I'm waiting for the orange bar to chug through, should I expect it to go about twice as fast, or 7 times faster — or is there nothing informative that can be said about this?</p>",
        "id": 285558199,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1654791079
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"422543\">@Stuart Presnell</span> Lean 3 has features to process multiple parts of a single file in parallel, so it might go above 2-3x, but I am guessing it depends on how independent parts of the file are, and it would not be by much. That is, is it a sequence of lemmas where each depends on the previous - not very parallelisable - or a bunch of more independent definitions - maybe it can go above 3x. Where multicore really helps is building whole projects such as mathlib, since then many files can be processed in parallel. Lean 4 processes single files sequentially, so as far as a single elaboration pass is concerned it could not go above 3x, although having more cores still helps the editing experience - we handle changes to the file and various requests such as printing the goal using all the available cores.</p>",
        "id": 285561074,
        "sender_full_name": "Wojciech Nawrocki",
        "timestamp": 1654792182
    },
    {
        "content": "<p>That's a point I hadn't considered: I guess the majority of the lifespan of this computer will be spent on Lean 4 rather than Lean 3, and I have no idea about how that changes things.</p>",
        "id": 285561523,
        "sender_full_name": "Stuart Presnell",
        "timestamp": 1654792371
    },
    {
        "content": "<p>I did some benchmarking when I set up the benchmarking bot and my experience was that the mathlib build does not scale super well beyond 4 cores. 6-8 still got faster, but not by as much as one would expect. Things might have changed since then, of course, but I would prioritise single-core performance if you have a choice.</p>",
        "id": 285561848,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1654792505
    },
    {
        "content": "<p>I think Lean 3 checks proofs of lemmas in parallel, after elaborating their statements (and definitions). A proof cannot depend in another proof in Lean (unfortunately it can in informal maths).</p>",
        "id": 285562912,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1654792950
    },
    {
        "content": "<p>Will lean 4 not be able to do that? That sounds unfortunate</p>",
        "id": 285563480,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1654793191
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"306577\">Matthew Ballard</span> <a href=\"#narrow/stream/113488-general/topic/Speccing.20a.20mathlib.20Mac/near/285557579\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"284160\">Eric Rodriguez</span> <a href=\"#narrow/stream/113488-general/topic/Speccing.20a.20mathlib.20Mac/near/285554351\">said</a>:</p>\n<blockquote>\n<p>My m1 pro 16GB macbook with lean compiled to ARM runs something like 2x faster than my desktop with a Ryzen 3600+32GB, and as of a month or two ago could compile mathlib.</p>\n</blockquote>\n<p>Did you notice a performance bump when moving to native from emulated lean?</p>\n</blockquote>\n<p>I never really tried it, it was 50% about the performance and 50% about the fun for me. I'll try next time there's a community version bump.</p>",
        "id": 285566466,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1654794514
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"307953\">Ruben Van de Velde</span> <a href=\"#narrow/stream/113488-general/topic/Speccing.20a.20mathlib.20Mac/near/285563480\">said</a>:</p>\n<blockquote>\n<p>Will lean 4 not be able to do that? That sounds unfortunate</p>\n</blockquote>\n<p>Parallel proof compilation could be added as a future optimization, but I believe it was decided not to add it for now because it introduces lots of engineering complexity and the gain is not as great as it might seem. In practice one cannot just elaborate the statement first and then move on with the proof, as the full statement might <em>depend</em> on the proof. Here is a silly contrived example of where we must descend into the proof term to find out which assumptions to pick up.</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">variable</span> <span class=\"o\">(</span><span class=\"n\">a</span> <span class=\"o\">:</span> <span class=\"n\">Nat</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">h3</span> <span class=\"o\">:</span> <span class=\"mi\">3</span> <span class=\"bp\">=</span> <span class=\"n\">a</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">h4</span> <span class=\"o\">:</span> <span class=\"mi\">4</span> <span class=\"bp\">=</span> <span class=\"n\">a</span><span class=\"o\">)</span>\n<span class=\"kd\">theorem</span> <span class=\"n\">le_a_3</span> <span class=\"o\">:</span> <span class=\"mi\">3</span> <span class=\"bp\">≤</span> <span class=\"n\">a</span> <span class=\"o\">:=</span> <span class=\"n\">Nat.le_of_eq</span> <span class=\"n\">h3</span>\n<span class=\"kd\">theorem</span> <span class=\"n\">le_a_3'</span> <span class=\"o\">:</span> <span class=\"mi\">3</span> <span class=\"bp\">≤</span> <span class=\"n\">a</span> <span class=\"o\">:=</span> <span class=\"n\">Nat.le_of_succ_le</span> <span class=\"o\">(</span><span class=\"n\">Nat.le_of_eq</span> <span class=\"n\">h4</span><span class=\"o\">)</span>\n<span class=\"c\">/-</span><span class=\"cm\"> le_a_3 : ∀ (a : Nat), 3 = a → 3 ≤ a -/</span>\n<span class=\"k\">#check</span> <span class=\"bp\">@</span><span class=\"n\">le_a_3</span>\n<span class=\"c\">/-</span><span class=\"cm\"> le_a_3' : ∀ (a : Nat), 4 = a → 3 ≤ a -/</span>\n<span class=\"k\">#check</span> <span class=\"bp\">@</span><span class=\"n\">le_a_3'</span>\n</code></pre></div>",
        "id": 285584026,
        "sender_full_name": "Wojciech Nawrocki",
        "timestamp": 1654802756
    },
    {
        "content": "<p>To be honest I would prefer Lean doesn't try to guess which variables to include by inspecting the proof term. And non trivial proofs use tactics anyway.</p>",
        "id": 285584964,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1654803268
    },
    {
        "content": "<p>yeah, it doesn't work with tactics and such which means it's just a mess 99% of the time</p>",
        "id": 285586075,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1654803859
    },
    {
        "content": "<p>and often it doesn't work even without that, I had an example with <code>is_cyclotomic_extension</code> a couple days ago that makes sense but is really annoying</p>",
        "id": 285586125,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1654803895
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110031\">Patrick Massot</span> <a href=\"#narrow/stream/113488-general/topic/Speccing.20a.20mathlib.20Mac/near/285584964\">said</a>:</p>\n<blockquote>\n<p>To be honest I would prefer Lean doesn't try to guess which variables to include by inspecting the proof term. And non trivial proofs use tactics anyway.</p>\n</blockquote>\n<p>No, inspecting the (final) proof term is what makes it <em>work</em> with arbitrary tactics, and is what Lean 4 does in contrast to Lean 3's lexical analysis of the surface syntax. Thus there is no need for an <code>include</code> statement in Lean 4.</p>",
        "id": 285646642,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1654850546
    },
    {
        "content": "<p>Does it mean you chose to avoid needing <code>include</code> rather than allowing parallel proof checking?</p>",
        "id": 285646731,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1654850615
    },
    {
        "content": "<p>Because I would clearly choose the other option (favoring parallel proof checking and using <code>include</code> from times to times).</p>",
        "id": 285646807,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1654850653
    },
    {
        "content": "<p>No, that was a distant secondary reason for removing parallel checking. The main reason was that it's complex to implement, and we wanted to do better than Lean 3's hacky implementation. But as Wojciech, I am somewhat optimistic that it could return in the future, preferably after we measure its potential impact on e.g. mathlib4.</p>",
        "id": 285647083,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1654850855
    },
    {
        "content": "<p>Ok, thanks. That seemed a very weird reason to drop parallel checking. And indeed I may be imagining only that parallel checking is crucial. We'll see when measurement will be available.</p>",
        "id": 285647283,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1654850987
    }
]