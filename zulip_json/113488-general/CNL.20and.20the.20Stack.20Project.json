[
    {
        "content": "<p>I just watched <span class=\"user-mention\" data-user-id=\"115334\">@Thales</span> talk from last week, on CNL and Formal Abstracts. I was wondering about the following question: would it be interesting to attempt to pre-process the Stacks Project into something that is readable by the CNL? Is it reasonable to think that this is possible? (cc people that might know more about this: <span class=\"user-mention\" data-user-id=\"221640\">@Peter Koepke</span> <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span>)</p>",
        "id": 185892636,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579244498
    },
    {
        "content": "<p>One reason for suggesting the Stacks Project is that it is written in a very strict subset of LaTeX</p>",
        "id": 185892943,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579244914
    },
    {
        "content": "<p>See <a href=\"https://github.com/stacks/stacks-project/blob/master/documentation/rules\" target=\"_blank\" title=\"https://github.com/stacks/stacks-project/blob/master/documentation/rules\">https://github.com/stacks/stacks-project/blob/master/documentation/rules</a> from which I will quote the 4th line:</p>\n<blockquote>\n<p>Avoid reading this document at all cost!</p>\n</blockquote>",
        "id": 185893085,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579245131
    },
    {
        "content": "<p>it would be interesting, and definitely easier than preprocessing a less uniform corpus like all math pages from e.g. Wikipedia</p>\n<p>besides working in a small subset of LaTeX, the CNL parser only accepts certain canned phrases in a certain order, so some NLP will be required to align the informal text with CNL equivalents</p>",
        "id": 185924147,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1579274181
    },
    {
        "content": "<p>Yup, I guess it won't be trivial. But for an informal text, the Stacks Project is written rather on the formal side of the spectrum.</p>",
        "id": 185924558,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579274435
    },
    {
        "content": "<p>That's another reason why I think it might be an interesting target.</p>",
        "id": 185924572,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579274447
    },
    {
        "content": "<p>A long time ago, I thought about a baby version of this problem. Together with <span class=\"user-mention\" data-user-id=\"223577\">@Josef Urban</span> we tried to \"linkify\" the Stacks Project: whenever a concept was defined in SP, link back to it when it is used later. We used some very basic NLP, but nothing fancy. As far as I remember, the results were cute, but not impressive. Also: I had to write a PhD thesis, so I stopped working on it.</p>",
        "id": 185926196,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579275469
    },
    {
        "content": "<p>Unfortunately, I don't think I have any code left from that side project.</p>",
        "id": 185926561,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579275651
    },
    {
        "content": "<p>Stacks tags the term being defined in each definition too, which is helpful.</p>",
        "id": 185926611,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1579275691
    },
    {
        "content": "<p>You mean with the pattern <code>{\\it some-word}</code>?</p>",
        "id": 185926809,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579275812
    },
    {
        "content": "<p>Right, its unfortunate they don't use a more descriptive macro, but I guess that's counter to the whole philosophy of a strict subset of latex</p>",
        "id": 185926946,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1579275896
    },
    {
        "content": "<p>I don't expect that you'll find many false positive <code>{\\it ...}</code> patterns inside definition environments.</p>",
        "id": 185927052,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579275960
    },
    {
        "content": "<p>It is true that one definition environment might contain multiple definitions.</p>",
        "id": 185927113,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579275977
    },
    {
        "content": "<p>I have an even babier version of what you are talking about set up for my own notes (nothing fancy at all, just matching exact strings + pluralization) but it works passably, the main difficulties are stacked adjectives (complete separated noetherian scheme) and mathematics overloading (even stacks has normal subgroups and normal schemes) I'd imagine your NLP approach would deal with the latter quite well?</p>",
        "id": 185927167,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1579276016
    },
    {
        "content": "<p>No, our NLP wasn't very NLP at all. IIRC we used something that Josef called \"bag-of-words\" and was some sort of statistical matching algorithm.</p>",
        "id": 185927267,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579276076
    },
    {
        "content": "<p>We didn't pair it with any typing information. So if you had <code>Let $X$ be a scheme. ... blabla ... Hence $X$ is normal.</code> Then it wouldn't figure out that <code>normal</code> meant <code>scheme.normal</code> (if there was no occurence of the word <code>scheme</code> within the closest 10 words).</p>",
        "id": 185927414,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579276159
    },
    {
        "content": "<p>In fact, if <code>group</code> occured near by (for some reason), it would likely have a false match.</p>",
        "id": 185927457,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579276184
    },
    {
        "content": "<p>Ideally, we could get a version of the Ganesalingam–BL parser. But that seems socially impossible.</p>",
        "id": 185927592,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579276241
    },
    {
        "content": "<p>Yeah that would definitely be a boost, let me know if you ever try anything like this, this problem has been something I've wanted to try and do more on for a while.</p>",
        "id": 185928102,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1579276557
    },
    {
        "content": "<blockquote>\n<p>Ideally, we could get a version of the Ganesalingam–BL parser. But that seems socially impossible.</p>\n</blockquote>\n<p>I don't know anything about this parser. What does your second sentence mean?</p>",
        "id": 185951211,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1579291368
    },
    {
        "content": "<p>Mohan Ganesalingam and TBL (yes, that TBL) have a parser that eats LaTeX and spits out some weakly typed abstract syntax tree.</p>",
        "id": 185951360,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579291447
    },
    {
        "content": "<p>But Mohan doesn't want to share the code.</p>",
        "id": 185951373,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579291459
    },
    {
        "content": "<p>Ask Tim Gowers for details.</p>",
        "id": 185951388,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579291470
    },
    {
        "content": "<p>Hi, </p>\n<p>I got an e-mail ping from Johan's post.</p>\n<p>The short 2015 paper about our poor-man's Stacks hyper-linker is in<br>\n<a href=\"https://cicm-conference.org/2015/CICM2015-wip.pdf\" target=\"_blank\" title=\"https://cicm-conference.org/2015/CICM2015-wip.pdf\">https://cicm-conference.org/2015/CICM2015-wip.pdf</a> (p. 19 - 23).</p>\n<p>As for combining statistical parsing and typing, we have so far done<br>\na couple of things in the inf2formal project :</p>\n<ol>\n<li>\n<p>Learning probabilistic grammars from the corpora and adding native<br>\n   (HOL/Mizar) typing inside the probabilistic parsers (but also using<br>\n   the type checking and ATP ex post). See:<br>\n<a href=\"https://doi.org/10.1007/978-3-319-66107-0_2\" target=\"_blank\" title=\"https://doi.org/10.1007/978-3-319-66107-0_2\">https://doi.org/10.1007/978-3-319-66107-0_2</a><br>\n<a href=\"https://doi.org/10.1109/SYNASC.2017.00036\" target=\"_blank\" title=\"https://doi.org/10.1109/SYNASC.2017.00036\">https://doi.org/10.1109/SYNASC.2017.00036</a> and the Flyspeck-trained<br>\n   demo at <a href=\"http://grid01.ciirc.cvut.cz/~mptp/demo.ogv\" target=\"_blank\" title=\"http://grid01.ciirc.cvut.cz/~mptp/demo.ogv\">http://grid01.ciirc.cvut.cz/~mptp/demo.ogv</a> .</p>\n</li>\n<li>\n<p>Trained supervised neural translation models on synthetic<br>\n   Latex-Mizar corpus: <a href=\"http://arxiv.org/abs/1805.06502\" target=\"_blank\" title=\"http://arxiv.org/abs/1805.06502\">http://arxiv.org/abs/1805.06502</a> .</p>\n</li>\n<li>\n<p>Recently we have added unsupervised neural translation (so an aligned<br>\n   corpus is not needed) - <a href=\"https://arxiv.org/pdf/1912.02636\" target=\"_blank\" title=\"https://arxiv.org/pdf/1912.02636\">https://arxiv.org/pdf/1912.02636</a> and<br>\n   combined the neural translation with type-checking. This is so far<br>\n   done by a simple data-augmentation loop between the neural net and<br>\n   the Mizar elaborator (Section 5.4 of the paper). The paper<br>\n   also shows that it is so far much harder to go from ProofWiki<br>\n   (informal) to Mizar. To do this for Stacks, it would be good to<br>\n   have at least some parts of Stacks formalized (in Lean or<br>\n   whatever). I believe this is happening (Kevin, Johan &amp; Co.), so UMT<br>\n   methods might probably be directly tried right now.</p>\n</li>\n<li>\n<p>We have also started to play with neural text summarization for<br>\n   e.g. conjecturing over Mizar and Stacks -<br>\n<a href=\"http://grid01.ciirc.cvut.cz/~mptp/AITP_2020_paper_21.pdf\" target=\"_blank\" title=\"http://grid01.ciirc.cvut.cz/~mptp/AITP_2020_paper_21.pdf\">http://grid01.ciirc.cvut.cz/~mptp/AITP_2020_paper_21.pdf</a> . Again<br>\n   (unsurprisingly), Stacks looks quite a bit harder (at least for the<br>\n   task we have chosen) than the nice and declarative Mizar ND proofs for<br>\n   such tasks.</p>\n</li>\n</ol>\n<p>Obviously, these are just the beginnings - the future of<br>\nautoformalization is bright and unavoidable (the naysayers can bet<br>\nme - see IHP Challenge 3 at <a href=\"http://ai4reason.org/aichallenges.html\" target=\"_blank\" title=\"http://ai4reason.org/aichallenges.html\">http://ai4reason.org/aichallenges.html</a> ;-).</p>\n<p>Josef<br>\n(I am an email-driven dinosaur, ping me again via @JosefUrban if needed) <br>\n(And come to <a href=\"http://aitp-conference.org/2020/\" target=\"_blank\" title=\"http://aitp-conference.org/2020/\">http://aitp-conference.org/2020/</a> if interested in the topic)</p>",
        "id": 185954693,
        "sender_full_name": "Josef Urban",
        "timestamp": 1579293707
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"223577\">@Josef Urban</span> Oooh, I had completely forgotten about that wip document <span aria-label=\"face palm\" class=\"emoji emoji-1f926\" role=\"img\" title=\"face palm\">:face_palm:</span> Thanks for reminding me (-;</p>",
        "id": 185955202,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1579294056
    }
]