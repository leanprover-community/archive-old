[
    {
        "content": "<p>I'm quoting a discussion from another topic because I think Gabriel raised an interesting point.<br>\n<span class=\"user-mention silent\" data-user-id=\"110043\">Gabriel Ebner</span> <a href=\"#narrow/stream/113488-general/topic/with_top.20irreducible/near/260782489\">said</a>:</p>\n<blockquote>\n<p>At the risk of completely derailing this discussion, it would also be cool to have eta for decidable instances.  That is, make it so that <code>example (a b : Decidable p) : a = b := rfl</code>.  We have quite a bit of ugly library design in mathlib to work around decidability diamonds (i.e., if you have a theorem about classical.decidable you can't use it prove something about a computable instance).  See e.g. the noncomputable polynomials issue.</p>\n</blockquote>\n<p>There are other examples of equalities that we would like to add to the kernel that are basically theorems and not true by definition or easily automatically provable. I made a list of a bunch of type class diamonds recently by searching Zulip for \"diamond\" and there are a few diamonds that arise because one piece of data in a type class is uniquely determined by the rest, e.g. sups are unique given a partial order, or inverses in a group are unique given the monoid structure. Adding custom rules like this would be cool. Also, making <code>module Z _</code> and <code>fintype _</code> a subsingleton definitionally would be cool. Another more ambitious but very useful set of rules to add would be things like <code>eval_X</code>, <code>eval_C</code> and the following eta rule for polynomials <code>evalâ‚‚ (f.comp C) (f X) = f</code>, which is kind of analogous to eta for products, making use of the UMP for polys. That way our current <code>ext; simp</code> method for proving mundane equalities of morphisms could be definitional.</p>\n<p>In my opinion it's a fundamental design flaw of both Lean and Coq that we rely on a method (defeq) to prove things that doesn't let us add any user proved theorems and we end up having to change the definition of a group to add stuff in the middle of the hierarchy.</p>\n<p>Will we ever be able to add stuff like that?</p>",
        "id": 261007746,
        "sender_full_name": "Chris Hughes",
        "timestamp": 1636558923
    },
    {
        "content": "<blockquote>\n<p>Also, making [...] <code>fintype _</code> a subsingleton definitionally would be cool.</p>\n</blockquote>\n<p>Do you remember some concrete examples where it would have helped, or that were ugly because of fintype-diamonds?</p>",
        "id": 261009967,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1636559814
    },
    {
        "content": "<blockquote>\n<p>we rely on a method (defeq) to prove things that doesn't let us add any user proved theorems</p>\n</blockquote>\n<p>For practical reasons we need some restrictions here, definitional equality needs to stay reasonably decidable.  We also want a form of conservativity, adding definitions to the environment should also not make earlier definitions type-incorrect.</p>",
        "id": 261010595,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1636560033
    },
    {
        "content": "<p>Another consideration for defeq reductions is that they should interact well with unification.  If <code>a</code> and <code>b</code> unify, then <code>whnf(a)</code> and <code>whnf(b)</code> should also unify.</p>",
        "id": 261011461,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1636560369
    },
    {
        "content": "<p>It's not exactly what you're proposing, but some keywords here are \"extensional type theory\" and \"equality reflection.\" If every provable equality becomes definitional, type checking is undecidable. There are proof assistants built around this (<a href=\"https://arxiv.org/pdf/1802.06217.pdf\">https://arxiv.org/pdf/1802.06217.pdf</a>, and I think PVS?) but they come with tradeoffs.</p>",
        "id": 261011822,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1636560505
    },
    {
        "content": "<p>I think <code>fintype _</code> being definitionally subsingleton would let you derive a equality reflection rule.</p>",
        "id": 261014110,
        "sender_full_name": "Reid Barton",
        "timestamp": 1636561434
    },
    {
        "content": "<p>Using also the computation rule for quotients.</p>",
        "id": 261014235,
        "sender_full_name": "Reid Barton",
        "timestamp": 1636561467
    },
    {
        "content": "<p>For example if <code>y z : X</code> and <code>h : y = z</code> then <code>fintype { x : X // x = z }</code> is inhabited by both <code>[[ [&lt;z, rfl&gt;] ]]</code> and <code>[[ [&lt;y, h&gt;] ]]</code> and we should be able to extract the elements <code>z</code> and <code>y</code> from these respectively.</p>",
        "id": 261014571,
        "sender_full_name": "Reid Barton",
        "timestamp": 1636561593
    },
    {
        "content": "<p>I think this is all idle speculation, but I would take the tradeoffs associated with extensional type theory any day. It really just means we need a better mechanism for being able to provide defeq proofs when needed; currently we have the worst of both worlds because type checking is undecidable anyway, and there isn't anything we can do when the kernel fails to deduce a certain defeq</p>",
        "id": 261021849,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1636564512
    },
    {
        "content": "<p>Jesper Cockx and collaborators are working on rewrite rules in Agda and Coq. These allow you to turn propositional equalities into definitional equalities, subject to a confluence check to keep type checking decidable. (You can also disable the checker for more fun/breakage.) Latest paper: <a href=\"https://hal.archives-ouvertes.fr/hal-02901011\">https://hal.archives-ouvertes.fr/hal-02901011</a></p>",
        "id": 261022449,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1636564736
    },
    {
        "content": "<p>In any case if we want to make <code>fintype</code> definitionally a subsingleton then we can just do that--the tradeoff is somewhere else.<br>\nAt present, I think the best compromise within Lean as it exists today is:</p>\n<ol>\n<li>Get rid of any classically irrelevant data such as <code>decidable_eq</code> or data-carrying <code>fintype</code> in the assumptions of the primary definitions, making them <code>noncomputable</code></li>\n<li>Add \"realizers\" that are secondary, computable versions of the definitions that take these computational assumptions, plus lemmas that say they are equal to the primary ones for any choice of the computational data.</li>\n</ol>\n<p>One way to do 1+2 is to give the computable definition first, and then make the main definition invoke it with instances provided by <code>classical</code>/<code>choice</code>.</p>",
        "id": 261022788,
        "sender_full_name": "Reid Barton",
        "timestamp": 1636564871
    },
    {
        "content": "<p>There are at least 3 kinds of reasons we like having definitional equalities:</p>\n<ol>\n<li>We want to prove a propositional equality <code>A = B</code> and we can prove it by <code>rfl</code>. This is the least important reason--we're just proving a proposition so the stakes are low (it doesn't matter how we do it), and we have a lot of other good tools for proving propositions (e.g. <code>simp</code>, <code>congr</code>).</li>\n<li>\n<p>We don't want to distinguish between two expressions <code>f x</code> and <code>f y</code> where <code>x</code> and <code>y</code> are something like decidability hypotheses (typically implicit arguments). This is sort of a special case of 1, but it is a bit different because we might not even realize that we are in a situation where both <code>f x</code> and <code>f y</code> are involved. Again we can get out of it using <code>congr</code>, but a simpler solution is to just not make definitions that depend on decidability hypotheses in the first place.<br>\nA variant is that <code>x</code> and <code>y</code> are something like <code>topological_space</code> instances that are propositionally but not definitionally equal--then we can't simply not take the <code>topological_space</code> instance, but I'm also not clear on what the proposed rules for adding definitional equalities would look like.</p>\n</li>\n<li>\n<p>We need the definitional equality for the conversion rule: <code>a : P x</code> so <code>a : P y</code>. In this case you're in a really bad spot if you don't have the definitional equality, since you need to use <code>eq.rec</code> and nobody wants to do that.</p>\n</li>\n</ol>\n<p>The motivation for <code>opposite</code> to be a structure with definitional eta in the other thread was to deal with 3 for the morphisms of the opposite category.</p>",
        "id": 261027010,
        "sender_full_name": "Reid Barton",
        "timestamp": 1636566464
    }
]