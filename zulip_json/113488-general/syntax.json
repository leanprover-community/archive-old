[
    {
        "content": "<p>Fun fact: <code>notation</code> applies even in <code>namespace</code> commands, so if you write <code>notation `foo` := bar</code> and then <code>namespace foo</code> it means <code>namespace bar</code></p>",
        "id": 127544480,
        "sender_full_name": "Reid Barton",
        "timestamp": 1528118569
    },
    {
        "content": "<p>Cool! :)</p>",
        "id": 127544555,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528118684
    },
    {
        "content": "<p>I don't know if that's a good thing but I was properly surprised when I realized what you meant</p>",
        "id": 127544571,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528118718
    },
    {
        "content": "<p>If you don't want this to happen (like I didn't) then you can write <code>namespace «foo»</code></p>",
        "id": 127544624,
        "sender_full_name": "Reid Barton",
        "timestamp": 1528118775
    },
    {
        "content": "<p>Curiously enough, it also works on binders: <code>∀ foo, f foo</code> actually means <code>∀ bar, f bar</code> which is inconvenient when the notation is a short hand for an expression, not an identifier</p>",
        "id": 127544649,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528118835
    },
    {
        "content": "<p>Thanks for the fix</p>",
        "id": 127544653,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528118845
    },
    {
        "content": "<p>o.O</p>",
        "id": 127544833,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1528119099
    },
    {
        "content": "<p>I take it that's unintended behavior?</p>",
        "id": 127544900,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528119177
    },
    {
        "content": "<p>(I was actually using <code>local notation</code>, in case that matters)</p>",
        "id": 127544969,
        "sender_full_name": "Reid Barton",
        "timestamp": 1528119244
    },
    {
        "content": "<p>I get \"identifier expected\" in both cases</p>\n<div class=\"codehilite\"><pre><span></span>constants foo bar : Type\nlocal notation `foo` := bar\n#check ∀ foo, foo\nnamespace foo\n</pre></div>",
        "id": 127545111,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1528119476
    },
    {
        "content": "<p>Oh, I might have jumped to conclusions.<br>\nWhat actually happened was that I was inside <code>namespace foo</code> and then I defined <code>foo</code> as local notation for something, and then <code>end foo</code> stopped working.</p>",
        "id": 127545171,
        "sender_full_name": "Reid Barton",
        "timestamp": 1528119536
    },
    {
        "content": "<p>with some error like \"identifier expected\"</p>",
        "id": 127545183,
        "sender_full_name": "Reid Barton",
        "timestamp": 1528119556
    },
    {
        "content": "<p>Yes, you're right. That's what happens for me too. Sorry about the confusion</p>",
        "id": 127545332,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528119762
    },
    {
        "content": "<p>I see. This will probably be fixed in Lean 4 automatically since we will be using a parser combinator without a scanner, so we can skip notations where we only want to parse identifiers.</p>",
        "id": 127545397,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1528119843
    },
    {
        "content": "<p>How is that going by the way?</p>",
        "id": 127545431,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528119941
    },
    {
        "content": "<p>Let's... say that we're transitioning from the planning phase to the implementation phase (parser, compiler, and other refactorings)</p>",
        "id": 127545680,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1528120289
    },
    {
        "content": "<p>Exciting :D I can't wait to see where that will end up!</p>",
        "id": 127545899,
        "sender_full_name": "Simon Hudon",
        "timestamp": 1528120580
    },
    {
        "content": "<p>Is there an EBNF description of the Lean syntax?</p>",
        "id": 166864691,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559170456
    },
    {
        "content": "<p>Maybe this: <a href=\"https://leanprover.github.io/reference/lexical_structure.html\" target=\"_blank\" title=\"https://leanprover.github.io/reference/lexical_structure.html\">https://leanprover.github.io/reference/lexical_structure.html</a> ? I'm not sure if it's actually EBNF</p>",
        "id": 166864806,
        "sender_full_name": "Wojciech Nawrocki",
        "timestamp": 1559170576
    },
    {
        "content": "<p>That doesn't describe, for example, the syntax of optional types, etc. It is more like a tokenizer syntax.</p>",
        "id": 166864945,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559170775
    },
    {
        "content": "<p>Oh, that's true</p>",
        "id": 166865244,
        "sender_full_name": "Wojciech Nawrocki",
        "timestamp": 1559171028
    },
    {
        "content": "<p>I'd like to accept Lean syntax as part of a system I'm building, making it easier to export proofs to Lean</p>",
        "id": 166865321,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559171095
    },
    {
        "content": "<p>Perhaps <a href=\"https://bitbucket.org/gebner/pygments-main/src/d5e82e6ed58cfedf985cb1d5d55d056e477aa667/pygments/lexers/theorem.py?at=default#lines-377\" target=\"_blank\" title=\"https://bitbucket.org/gebner/pygments-main/src/d5e82e6ed58cfedf985cb1d5d55d056e477aa667/pygments/lexers/theorem.py?at=default#lines-377\">the Pygments syntax description</a> might help?</p>",
        "id": 166865501,
        "sender_full_name": "Wojciech Nawrocki",
        "timestamp": 1559171324
    },
    {
        "content": "<p>Wow,  the person who wrote that code is multilingual. Still it is only a tokenizer. I was hoping that I didn't have to create the EBNF from Leonardo's source code.</p>",
        "id": 166865935,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559171653
    },
    {
        "content": "<p>I think <span class=\"user-mention\" data-user-id=\"115334\">@Thales</span> wrote a Lean grammar for Mehnir, which is probably the closest approximation yet. Part of the problem is that lean isn't actually context free, so you have to make a bunch of approximations and it's not actually sure to work (or even likely to work without a lot of hard-coding) on a real lean file</p>",
        "id": 166887712,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559202262
    },
    {
        "content": "<p>Well, I read the sources looking for EBNF. Unfortunately(?) Lean uses Pratt parsing. You can change syntax on the fly.</p>",
        "id": 167059113,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559372107
    },
    {
        "content": "<p>right, that's the <code>notation</code> command</p>",
        "id": 167061418,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559376212
    },
    {
        "content": "<p>You can still do decently if you know what syntaxes are defined, and parse up to the next command keyword</p>",
        "id": 167061486,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559376262
    },
    {
        "content": "<p>This causes a portability problem. I'd like to have a parser / exporter that can communicate freely with Lean. But if the notation command is used the associations can change dynamically making previously accepted communications fail. The obvious workaround would be to export the led-nud tables but I don't see anything in the source code that provides that. Another alternative would be to communicate using only \"unsugared syntax\". I will read more about that level of detail soon.</p>",
        "id": 167064470,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559381223
    },
    {
        "content": "<p>I am trying to combine proofs with code. This is a \"30 year horizon\" effort so it is important that the proofs are repeatable and stable (think latex, common lisp, and pdfs). The ability to modify syntax is one problem. The ability to modify tactics is another. Proofs are difficult, time consuming, and require a lot of effort. The hope is that today's proof will be accepted by the system years from now with the same result.</p>",
        "id": 167065160,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559382430
    },
    {
        "content": "<p>At the moment the only path seems to be to extract the Lean kernel and, for every proof, have the system dump a \"kernel version\" that used no syntax or tactics, just kernel-level I/O. That would enable proofs to be developed at the higher levels but stored and replayed as kernel code. The assumption is that the kernel is small and \"30 year horizion\" stable.</p>",
        "id": 167065472,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559383072
    },
    {
        "content": "<p>The API is likely to change drastically regardless when Lean v4 arrives later this year. There will be many breaking changes. At a minimum mutually inductive types are going to move into the kernel for performance reasons, and there is no reason to believe other things may not move into the kernel if necessary for usability.</p>",
        "id": 167065647,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1559383363
    },
    {
        "content": "<p>My code uses theorems from the 1890s so long-term proof stability is a vital issue.</p>",
        "id": 167065649,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559383377
    },
    {
        "content": "<p>I can't think of a single proof language with that kind of stability, unfortunately.</p>",
        "id": 167065809,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1559383664
    },
    {
        "content": "<p>How can I build up chains of Definitions and Lemmas proving code correctness if the whole thing fails a week later? I don't know how to do computational mathematics on that basis. I have common lisp source code for my computer algebra system that still compiles and executes with the same results today that it had when it was written in 1980. I have latex documents from then. It seems perfectly reasonable to expect that a \"proof\" by machine code would continue to be checkable in 2049.</p>",
        "id": 167066235,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559384357
    },
    {
        "content": "<p>If you're willing to never upgrade or import results from the proof community at large, you could fix a version of &lt;your favorite proof language&gt;. Hardly appealing, I know.</p>",
        "id": 167066617,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1559385040
    },
    {
        "content": "<p>It might be useful to consider a \"standards effort\" in proof technology. There used to be many competing lisp dialects and they eventually agreed on common lisp. I used lisp 1.5, maclisp, lispvm, psl, and many others and was good at \"porting code\". Now the common lisp standard makes that pointless. Perhaps there needs to be a Common Proof effort that provides a stable basis for (at least primitive syntax) of proofs  that can be universally accepted. Computational mathematics IS mathematics so it seems that long-term \"30 year horizon\" stability ought to be a major goal.</p>",
        "id": 167066810,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559385453
    },
    {
        "content": "<p>Competing on \"features\" is for game programmers, not mathematicians.</p>",
        "id": 167067071,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559385876
    },
    {
        "content": "<p>I think you don't quite understand where we stand now. Freezing stuff at such an early stage would probably be very counter-productive. See <a href=\"https://deducteam.github.io/\" target=\"_blank\" title=\"https://deducteam.github.io/\">https://deducteam.github.io/</a> however (but keep in mind that this is also very much work in progress)</p>",
        "id": 167067132,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1559385974
    },
    {
        "content": "<p>At the moment we are still before the beginning of history. Theorem provers are not yet useful for mathematics.</p>",
        "id": 167067381,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1559386472
    },
    {
        "content": "<p>(That is not to say they are not worth investing significant time in.)</p>",
        "id": 167067386,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1559386488
    },
    {
        "content": "<p>But any serious plans for backwards compatibility should wait (implementation at least, not discussion) until we have something.</p>",
        "id": 167067397,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1559386522
    },
    {
        "content": "<p>Systems that use CIC, for instance, ought to have a compatible \"raw\" language for accepting each other's proofs. At minimum, a system ought to accept its own \"raw\" logic no matter what else changes. The logic is stable. The issue isn't \"backward compatiblity\" but \"forward compatibilty\". I can still compile and run my C code from my PDP-11/03 (1976). It seems obvious that a proof system should always accept a \"proven\" proof forever.</p>",
        "id": 167067836,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559387297
    },
    {
        "content": "<p>In fact, the underlying logic isn't stable. For example, Coq only recently got universe polymorphism in version 8.5, and the underlying implementation is still subject to change.</p>",
        "id": 167067971,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1559387540
    },
    {
        "content": "<p>So Coq doesn't have a stable proven kernel of logic?</p>",
        "id": 167068043,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559387709
    },
    {
        "content": "<p>No. Extensions to CIC are allowed (as in Lean) as long as the developers think it's a good idea.</p>",
        "id": 167068160,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1559387893
    },
    {
        "content": "<p>Do they invalidate prior proofs? (Will Flyspeck become junk?)</p>",
        "id": 167068175,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559387952
    },
    {
        "content": "<p>I'm just looking at Flyspeck's project history and the most recent source code change was in Jan 25, 2019. Despite the fact the project was finished five years ago.</p>",
        "id": 167068523,
        "sender_full_name": "Andrew Ashworth",
        "timestamp": 1559388485
    },
    {
        "content": "<p>According to Manuel Eberl, the Flyspeck project is compiled <em>every day</em> against current version of the relevant proof assistant.</p>",
        "id": 167068536,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1559388559
    },
    {
        "content": "<p>(automatically of course)</p>",
        "id": 167068537,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1559388568
    },
    {
        "content": "<p>It seems my understanding of \"the trusted kernel\" is bogus. I'll read the source code and try to get a better grounding. Thanks.</p>",
        "id": 167068792,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559388988
    },
    {
        "content": "<blockquote>\n<p>At the moment the only path seems to be to extract the Lean kernel and, for every proof, have the system dump a \"kernel version\" that used no syntax or tactics, just kernel-level I/O. That would enable proofs to be developed at the higher levels but stored and replayed as kernel code. The assumption is that the kernel is small and \"30 year horizion\" stable.</p>\n</blockquote>\n<p><code>lean --export</code> is something like this</p>",
        "id": 167073089,
        "sender_full_name": "Reid Barton",
        "timestamp": 1559396476
    },
    {
        "content": "<p>There are external type checkers that can check the output of <code>lean --export</code>, for example <a href=\"https://github.com/dselsam/tc\" target=\"_blank\" title=\"https://github.com/dselsam/tc\">https://github.com/dselsam/tc</a></p>",
        "id": 167073209,
        "sender_full_name": "Reid Barton",
        "timestamp": 1559396702
    },
    {
        "content": "<p>So there is at least a very small amount of portability, namely, to outside the Lean kernel itself</p>",
        "id": 167073283,
        "sender_full_name": "Reid Barton",
        "timestamp": 1559396862
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"223495\">@Tim Daly</span>  I am sad I missed this discussion. Lean is not the place to look for stability, but there are proof languages that are designed for long term stability. Long future archiving and maintenance in perpetuity is among the selling points of Metamath. It has a standard, which was written down, in a published book from the 1990s and still remains valid (although additions have grown in the gaps, like HTML). There are as a result a whole ecosystem of independent verifiers, and in fact no single verifier \"dominates the market\", which is not something I can say about any other proof language I know. The axiom system is not completely stable but has changed only a few times in its 20 year history, and is based on an old mathematical standard, ZFC.</p>\n<p>The downside to this kind of long term standardization is that it doesn't leave a lot of room for \"syntax sugar\" and building conveniences into the language, which is where Lean and Coq are coming from. I try to provide some assurance of \"monotonicity\" of proofs that are submitted to mathlib, but it's not easy. The axiom system is dependent on subtleties that make the proofs not portable to similar systems (such as Coq or other DTT systems), the syntax changes every few years, and we've literally thrown away the library and started afresh since the previous version, and while I don't intend to repeat that experience with lean 4 the language changes are just as traumatic. This is done in the quest for an easy to use language, where you can just sit down and write proofs and it's easy, and I respect that goal; but I think that this need not conflict with standardization.</p>\n<p>Consider HTML and Javascript. This is a standard from the early 90s that we still use today. It has changed, but slowly and with a clearly defined standards body behind it. It has not hampered our ability to build new and glitzy things on top, but the key is that no one actually writes HTML/JS directly anymore. Instead the glitzy front end is written in some framework or language that didn't exist a year ago, and is hooked to a compiler that produces standards conformant HTML. I think this approach is very powerful, and I wish people would think more about having a separate back end with a well understood logic, and a core verifier that anyone can write.</p>",
        "id": 167075291,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559400316
    },
    {
        "content": "<p>In principal I really like this idea... but it seems very non-trivial to build this. Quite a bit harder than HTML, I would think</p>",
        "id": 167075619,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559400922
    },
    {
        "content": "<p>And already with the HTML + JS example you see that every new year brings a new framework, but it's hard to make them interact</p>",
        "id": 167075674,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559400968
    },
    {
        "content": "<p>I still write html ;-)</p>",
        "id": 167075678,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1559400994
    },
    {
        "content": "<p>If proofs are written in some framework that still compiles in 50 (or 500) years, that is of course fabulous. But if those theorems can't be applied, because the framework they are written in is outdated, I don't see how this is any better than saying \"Ooh, Flyspeck (or whatever) compiles with $ANCIENT version of $THEOREM_PROVER\".</p>",
        "id": 167075704,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559401075
    },
    {
        "content": "<p>The theorems can be applied, but they may not be easily modified. For a mathematical theorem that's actually not that bad, and not all that different from conventional mathematical practice</p>",
        "id": 167076295,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559402013
    },
    {
        "content": "<p>The software analog is a library written in $OUTDATED that does something useful for you, so you link it to your project written in the new hotness</p>",
        "id": 167076393,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559402151
    },
    {
        "content": "<p>They can be applied \"in theory\". Just like wordpress and joomla can probably be made to work together \"in theory\". But you can't just load a decades old joomla plugin into wordpress.</p>",
        "id": 167076443,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559402191
    },
    {
        "content": "<p>Summary: I will have to see this work... until then, I remain somewhat skeptical. That \"maths stack\" is orders of magnitude more layered than any web-dev stack.</p>",
        "id": 167076463,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559402255
    },
    {
        "content": "<p>I'm working on it. I actually hope to have a demo soonish</p>",
        "id": 167077027,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1559403145
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> Cool. I'm looking forward to it.</p>",
        "id": 167079600,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559407086
    },
    {
        "content": "<p>Axiom has a \"30 Year Horizon\" project goal, looking to develop the<br>\ntechniques and technology for reliable, proven computational<br>\nmathematics.</p>\n<p>I'm trying to prove Axiom Sane (rational, judicious, sound, logical)<br>\nby constructing axioms and specifications for functions (actually,<br>\nonly the GCD functions since I'm not gonna live long enough to do<br>\nmuch more :-) )</p>\n<p>It is clearlly important to construct proofs that can be checked<br>\nat compile time, every time, to ensure that Axiom is still<br>\nmathematically sound despite changes.</p>\n<p>Apparently this kind of \"30 Year Horizon\" stability isn't a project<br>\ngoal for a lot of the systems.I find this suprising since Gentzen's<br>\nrules seem like a solid logic kernel you'd have to implement.</p>\n<p>Robinson's resolution paper was in 1965 so proof technology is as<br>\nold as computer algebra (SAINT/SIN). After  50+ years, it seems<br>\nreasonable to think about standards.</p>\n<p>Add all the features you want. Just give me a low-level, long-term<br>\nlanguage that still proof-checks 30 years from now.</p>",
        "id": 167086565,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559418028
    },
    {
        "content": "<blockquote>\n<p>Axiom has a \"30 Year Horizon\" project goal, looking to develop the<br>\ntechniques and technology for reliable, proven <em>computational</em><br>\nmathematics.</p>\n</blockquote>\n<p>(emphasis mine) <span class=\"user-mention\" data-user-id=\"223495\">@Tim Daly</span> The problem is that most mathematicians are in love with a the 95% of mathematics that is not computational. Lean is a new system (not perfect, but getting close <span aria-label=\"wink\" class=\"emoji emoji-1f609\" role=\"img\" title=\"wink\">:wink:</span>) that gives us powerful tools <em>and</em> explicitly doesn't give a thing if we use the axiom of choice are as noncomputable as we want. That is a new combo that other systems didn't give us.<br>\nIt's too early to claim victory. (In fact as others pointed out, we need systems that are even better than Lean.) But it cannot be denied that in its rather short life span Lean has made a rather impressive impact on the mathematical community compared to what other theorem provers have accomplished. (I didn't check any numbers, but I guess that the ratio <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mtext>mathematicians</mtext></mrow><mrow><mtext>computer scientists</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{\\text{mathematicians}}{\\text{computer scientists}}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"strut\" style=\"height:0.8801079999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.3612159999999998em;vertical-align:-0.481108em;\"></span><span class=\"base\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mathrm mtight\">computer scientists</span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mathrm mtight\">mathematicians</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.481108em;\"></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span> is higher in the Lean community than in the other theorem prover communities.)</p>\n<p>I understand your wish for stability. It is also on my wish list. But my wish list is rather long (-;</p>",
        "id": 167103172,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1559447681
    },
    {
        "content": "<p>About \"victory\"...</p>\n<p>There are well over 100 \"computer algebra systems\" (I made a CD for a conference containing them). There are only a few large, general purpose ones (Mathematica, Maple, Axiom, Maxima, Magma). All of them had one thing in common... they had a reliable source of money. Sage was growing rapidly but the money ran out. (Oh, and nobody gets tenure writing software).</p>\n<p>Look to see how reliable, long term, and deep-pocket-ed  the money source is. Winning systems need a constant funding of about 2 million dollars a year that will continue for 15+ years. The 2 million dollars will support 6-8 \"core researchers\". </p>\n<p>When the core researchers are gone the software dies. Nobody ever, EVER, bothers to document the code. How many people know about the reference counting macros used in the Lean data structures? It is clever but unless you study the code and understand reference counting garbage collection, it is quite opaque. Who can maintain that software? (I'm trying to reverse-engineer the syntax which is considered a \"trivial\" task, just so my software can use Lean.)</p>\n<p>On the mathematical side, what logic rules are actually implemented?  Can a non-mathematician even read the  papers that MIGHT show what SOME of the code does? Who can maintain that software?</p>\n<p>Will Lean be \"one of the large, general purpose systems\" in 15 years? To misquote Zhou Enlai (when asked if the French Revolution was a good thing) ... \"It is too soon to tell\". But based on my experience, you just need to follow the money.</p>",
        "id": 167113103,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559465887
    },
    {
        "content": "<p>Interesting paper: Asperti et al. \"Crafting a Proof Assistant\"  Proc. Types 2006: Conf of the Types Project\", 2006</p>",
        "id": 167113450,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559466579
    },
    {
        "content": "<p>Also: Barthe and Elbers \"Towards Lean Proof Checking\"</p>\n<p>\"Our implementation of Oracle types is very flexible and allows rewriting to be perfomred either inside Lego or by Reduce, an efficient symbolic computation system. In our view, the main novelty of our approach is to combine a sound theoretical foundation with an efficient implementation. Besides, our work provides the first attempt to combine symbolic computation systems with theorem provers such as Coq and Lego, which are based on intensional type theories.\"</p>",
        "id": 167115133,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559469394
    },
    {
        "content": "<blockquote>\n<p>But it cannot be denied that in its rather short life span Lean has made a rather impressive impact on the mathematical community compared to what other theorem provers have accomplished. (I didn't check any numbers, but I guess that the ratio <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mtext>mathematicians</mtext></mrow><mrow><mtext>computer scientists</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{\\text{mathematicians}}{\\text{computer scientists}}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"strut\" style=\"height:0.8801079999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.3612159999999998em;vertical-align:-0.481108em;\"></span><span class=\"base\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mathrm mtight\">computer scientists</span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mathrm mtight\">mathematicians</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.481108em;\"></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span> is higher in the Lean community than in the other theorem prover communities.)</p>\n</blockquote>\n<p>Johan, I appreciate your enthusiasm, but let's not forget we are discussing statistics on a negligible subset of mathematicians. For the sake of discussion, let's take the simplistic definition that we count people having a PhD in mathematics. Then, thinking about it for a couple of minutes, I can't get more than 10 mathematicians using Lean. Maybe I  forgot one or two (the list I have in mind is Bryan, Jan David, Johan, Kevin, Neil, Patrick, Sander, Scott, Sébastien, Tom, although I guess we could add Reid anyway). It seems there are at least 100 000 living mathematicians, so the ratio of Lean users among them is at most 1 out of  10 000. And taken all together, they have written 1 paper about formalizing maths in Lean (the cap set paper).</p>\n<p>That's comparing Lean mathematicians with all mathematicians. Of course comparing to other proof assistants is more impressive. But we shouldn't forget there are, if I understand correctly, a number of mathematicians (using that simplistic definition) that do homotopy type theory, and it seems they almost all use Coq. There may very well be more than ten people in this category, say around Steve Awodey. There may be also people around Carlos Simpson, but I'm not sure he is still doing this. So I'm not sure at all we have more mathematicians than Coq. We probably have a better derivative, but still a tiny one. And we still miss having one landmark formalization. Computer scientists are not impressed <em>at all</em> by the perfectoid spaces project for instance. We claim it's something completely new, but we completely failed at convincing anyone of this up to now.</p>\n<p>At the Edinburgh workshop, we really freaked out other proof assistant communities by bringing half a dozen mathematicians. The other gangs were not expecting this at all, and seemed thoroughly confused by this fact. But Jeremy was among people suggesting names to the main organizer. And we still don't understand why there were only 2 Coq users (Gonthier and Kerjean) at this workshop (other people were philosophers or Isabelle users or one person per \"exotic\" proof assistant like Naproche or PVS).</p>",
        "id": 167118096,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1559474438
    },
    {
        "content": "<p>I agree with your point. I'm quite happy with Lean. I'm spending time reading the source code so I'm investing time in it.</p>",
        "id": 167121188,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559479822
    },
    {
        "content": "<p>There have been attempts to use Oracle systems to speed up proofs that involve computation. The problem is that the Oracle systems (usually a computer algebra system) are not proven, which undermines the premise that the proof is sound. However, if the GCD algorithms in Axiom were proven correct using Lean, then Lean could use Axiom's GCD computation as a proven Oracle. That changes the whole game.</p>",
        "id": 167121440,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559480332
    },
    {
        "content": "<p>And if the code for GCD was extracted from Lean it is even more interesting. See this paper:</p>\n<p>Berger and Schwichtenberg \"The Greatest Common Divisor: A Case Study for Program Extraction from Classical Proofs\"  LNCS 1158 pp 36-46 1985</p>",
        "id": 167121577,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559480573
    },
    {
        "content": "<p>Efforts invested in a \"landmark formalization\" are interesting but are essentially a one-off effort. But efforts to tightly cooperate with a proven computer algebra system vastly increases the range of possible uses, both for the proof community and the computer algebra community.<br>\nWe really need these two towers of computational mathematics to cooperate.</p>",
        "id": 167121797,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559480912
    },
    {
        "content": "<p>I spoke to <span class=\"user-mention\" data-user-id=\"116034\">@William Stein</span>  (who wrote SAGE) about Lean last year and he was super-enthusiastic, even going to the extent of making CoCalc work with Lean. I've used Lean in a multi-user environment within CoCalc just becuase of William's enthusiasm. But my aims were far less lofty than yours -- I just wanted a controlled place where students could use Lean without the hassle of having to install it. I'm sure he'd be very open-minded about further integration.</p>",
        "id": 167121880,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1559481102
    },
    {
        "content": "<p>Axiom also works with Sage.</p>",
        "id": 167125227,
        "sender_full_name": "Tim Daly",
        "timestamp": 1559486793
    },
    {
        "content": "<p>Hi Tim -- I'm glad to hear that you're learning and using LEAN!</p>",
        "id": 167126487,
        "sender_full_name": "William Stein",
        "timestamp": 1559489185
    }
]