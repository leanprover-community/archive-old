[
    {
        "content": "<p>At Rob's request, I'm setting up a new build time benchmarking server that'll hopefully be a little more consistent than Scott's bot. You can see its measurements at <a href=\"http://mathlib-bench.limperg.de/\">mathlib-bench.limperg.de</a>.</p>\n<p>At the moment, the thing is very bare-bones. If you have any additional features you think would be useful, that's what this thread is for! In particular, Scott's bot generates a lot of profiling output; if you need that, I can probably set it up.</p>\n<p>One thing I'll certainly improve: the server doesn't benchmark every commit, so the time difference is not \"time difference caused by this commit\" but rather \"time difference caused by all commits since the last commit we benchmarked\". This needs to be clearer in the UI.</p>",
        "id": 215115739,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1604074139
    },
    {
        "content": "<p>This is awesome, thanks Jannis!</p>",
        "id": 215116043,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074283
    },
    {
        "content": "<p>Saving the profile output would be great. I think it's a useful feature of Scott's script.</p>",
        "id": 215116096,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074306
    },
    {
        "content": "<p>there are three of those lines that are colored red or green - does that indicate that you think the build was broken on the red ones, and fixed on the green ones?</p>",
        "id": 215116200,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604074338
    },
    {
        "content": "<p>We should also make it announce results to Zulip. (Maybe time to rename the <a class=\"stream\" data-stream-id=\"113538\" href=\"/#narrow/stream/113538-travis\">#travis</a> stream, heh.)</p>",
        "id": 215116208,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074340
    },
    {
        "content": "<p>or is +/- 8% within typical expected noise</p>",
        "id": 215116274,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604074368
    },
    {
        "content": "<p>When you link to GitHub diffs, using three <code>...</code> instead of two <code>..</code> gives a nicer output for this kind of thing. I discovered this when I was setting up CI scripts. Compare <a href=\"https://github.com/leanprover-community/mathlib/compare/581b2af3eb36e4eb460cce3e3d7842e32e7a3349...58f8817e6aff0abdf7bc294ef72c3e441db151ab\">https://github.com/leanprover-community/mathlib/compare/581b2af3eb36e4eb460cce3e3d7842e32e7a3349...58f8817e6aff0abdf7bc294ef72c3e441db151ab</a> (three) and <a href=\"https://github.com/leanprover-community/mathlib/compare/581b2af3eb36e4eb460cce3e3d7842e32e7a3349..58f8817e6aff0abdf7bc294ef72c3e441db151ab\">https://github.com/leanprover-community/mathlib/compare/581b2af3eb36e4eb460cce3e3d7842e32e7a3349..58f8817e6aff0abdf7bc294ef72c3e441db151ab</a> (two)</p>",
        "id": 215116380,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074416
    },
    {
        "content": "<p>It would be nice to display the date/time the build started, possibly the date/time of the commit.</p>",
        "id": 215116806,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074601
    },
    {
        "content": "<p>Are these builds running single threaded? Those are pretty fast times. What kind of VM are you using?</p>",
        "id": 215117056,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074708
    },
    {
        "content": "<p>I guess single threaded isn't really important as long as the times are consistent.</p>",
        "id": 215117126,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074738
    },
    {
        "content": "<p>Your experimenting has cost $65 of our Azure credits. A lot in comparison to the $100 we've spent so far on olean serving, but a small fraction of the bucket we need to use up before June :p</p>",
        "id": 215117457,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604074878
    },
    {
        "content": "<p>An advanced feature: it would be really great if we could trigger a benchmark build from a PR with a comment on GitHub. The GitHub side is easy enough to do with an action. It would just need a hook on the server side and a way to display the result differently from a master build in the UI.</p>",
        "id": 215118328,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604075280
    },
    {
        "content": "<p>a couple minor suggestions. first, i suggest measuring the time change not from the previous build, but from the average of the previous 10 builds. right now the chart shows three \"outlier\"-colored listings, when really what happened was that a couple consecutive runs were a bit off in different directions. none of those are really time outliers</p>",
        "id": 215118698,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604075441
    },
    {
        "content": "<p>second, i suggest putting a time on each row for when the snapshot of the repo was taken - it's easier to reason about \"oh the problem started yesterday morning, hmm\" rather than \"oh the problem started at diff 62b3\"</p>",
        "id": 215118963,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604075556
    },
    {
        "content": "<p>The HTML report is gorgeous!<br>\nI'd really like to see more detailed profiling output, that is, runtime per file. (I believe Scott had that.)  Ideally there'd be a nice table where you could see how the runtime of each file changes between mathlib commits.</p>",
        "id": 215119314,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1604075741
    },
    {
        "content": "<p>Something like this: <a href=\"#narrow/stream/113538-travis/topic/build.20time.20bot/near/203363514\">https://leanprover.zulipchat.com/#narrow/stream/113538-travis/topic/build.20time.20bot/near/203363514</a></p>",
        "id": 215119436,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1604075790
    },
    {
        "content": "<p>This is the script Bryan cooked up (requires precompiled oleans): <a href=\"#narrow/stream/113538-travis/topic/build.20time.20bot/near/199713332\">https://leanprover.zulipchat.com/#narrow/stream/113538-travis/topic/build.20time.20bot/near/199713332</a></p>",
        "id": 215119671,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1604075909
    },
    {
        "content": "<p>If you add <code>--profile</code> to the Lean command-line, we could also get some reports on tactic runtime: <a href=\"#narrow/stream/113538-travis/topic/Analyzing.20build.20logs/near/197523478\">https://leanprover.zulipchat.com/#narrow/stream/113538-travis/topic/Analyzing.20build.20logs/near/197523478</a></p>",
        "id": 215119989,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1604076060
    },
    {
        "content": "<p>Lots of good suggestions, thanks!</p>\n<blockquote>\n<p>When you link to GitHub diffs, using three ... instead of two .. gives a nicer output for this kind of thing.</p>\n</blockquote>\n<p>Heh, wouldn't have guessed that. Thanks!</p>\n<blockquote>\n<p>It would be nice to display the date/time the build started, possibly the date/time of the commit.</p>\n</blockquote>\n<p>This is also easy.</p>\n<blockquote>\n<p>Are these builds running single threaded? Those are pretty fast times. What kind of VM are you using?</p>\n</blockquote>\n<p>These are four threads on E4as_v4 VMs. In my tests, these were the VMs that introduced the least variance. Four vs one thread didn't seem to make a huge difference in terms of relative variance, but that experiment is still running so maybe I'll have to go back to one thread when the results are in.</p>\n<blockquote>\n<p>An advanced feature: it would be really great if we could trigger a benchmark build from a PR with a comment on GitHub. The GitHub side is easy enough to do with an action. It would just need a hook on the server side and a way to display the result differently from a master build in the UI.</p>\n</blockquote>\n<p>Can do, but this requires some architectural changes, so it'll take a bit.</p>\n<blockquote>\n<p>first, i suggest measuring the time change not from the previous build, but from the average of the previous 10 builds. right now the chart shows three \"outlier\"-colored listings, when really what happened was that a couple consecutive runs were a bit off in different directions. none of those are really time outliers</p>\n</blockquote>\n<p>Good idea, will do.</p>\n<blockquote>\n<p>I'd really like to see more detailed profiling output, that is, runtime per file. (I believe Scott had that.) Ideally there'd be a nice table where you could see how the runtime of each file changes between mathlib commits.</p>\n</blockquote>\n<p>Wil do; it seems like this feature is in high demand.</p>\n<blockquote>\n<p>or is +/- 8% within typical expected noise</p>\n</blockquote>\n<p>Currently changes of more than +/- 3% (actually 2.65% for reasons) are marked red (worse) or green (better). It looks like the VM/Lean/OS introduces about this amount of error.</p>\n<blockquote>\n<p>We should also make it announce results to Zulip. (Maybe time to rename the #travis stream, heh.)</p>\n</blockquote>\n<p>If people want this, I can certainly do it. Do you actually look at #travis though? I always found it too spammy to be subscribed. Maybe just announce outliers?</p>",
        "id": 215121872,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1604076993
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"256311\">Jannis Limperg</span> <a href=\"#narrow/stream/113488-general/topic/Preview.20of.20new.20build.20time.20benchmark/near/215121872\">said</a>:</p>\n<blockquote>\n<p>If people want this, I can certainly do it. Do you actually look at #travis though? I always found it too spammy to be subscribed. Maybe just announce outliers?</p>\n</blockquote>\n<p>I do, but just announcing outliers is probably plenty, since we know where to look for the full output.</p>",
        "id": 215124504,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604078225
    },
    {
        "content": "<p>Ooh, I love the #travis spam.<br>\nWe should really rename that stream though</p>",
        "id": 215125997,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1604078886
    },
    {
        "content": "<p>Not sure it's helpful (you may all know these things better than we/I do), but just on the general topic of benchmarks and reducing error/noise in timing -- the top three articles here are nice things from Python-land which will be of at least some relevance: <a href=\"https://vstinner.readthedocs.io/benchmark.html\">https://vstinner.readthedocs.io/benchmark.html</a></p>",
        "id": 215126899,
        "sender_full_name": "Julian Berman",
        "timestamp": 1604079314
    },
    {
        "content": "<p>I think the main issue with consistency is just using a VM for benchmarking. there will always be some contention with whatever else is using the machine. but, it's way better than nothing! i just wouldn't worry about much optimization work on benchmarking consistency as long as it's on a VM, I would not be surprised by random 10-20% fluctuations that are entirely out of our control</p>",
        "id": 215129260,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604080553
    },
    {
        "content": "<p>Yeah, agreed.</p>",
        "id": 215137573,
        "sender_full_name": "Julian Berman",
        "timestamp": 1604084658
    },
    {
        "content": "<p>Julian, thank you for the benchmarking notes. I'll go through them to see which parts are applicable.</p>\n<p>Wrt noise, the VM is certainly not great and any micro-optimisations to reduce noise will likely be a drop in the bucket. But it's not horrible either. I did an experiment where I built the same commit 30 times and the biggest deviation from the mean was 3.4%. Standard deviation 49s, which is 1.5% of the mean. So the bot can't detect small optimisations/regressions, but bigger fish will show up.</p>",
        "id": 215143039,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1604087661
    },
    {
        "content": "<p>I guess we can also track an \"average of the last 5 builds\" parameter.</p>",
        "id": 215143508,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1604087935
    },
    {
        "content": "<p>And for now, the VMs are free, reasonably fast, and more reliable than Scott's server pushing to <a class=\"stream\" data-stream-id=\"113538\" href=\"/#narrow/stream/113538-travis\">#travis</a>.</p>",
        "id": 215143526,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604087946
    },
    {
        "content": "<p>so, the way VMs are typically inconsistent is based on how other people are scheduled around you. if your VM is co-scheduled with a bunch of light users, performance will look pretty stable, but at any time the cloud provider may reallocate resources, you find your job is coexisting with a heavier user, and performance is lower. it is possible to have a situation like, performance seems totally consistent for a week, and then is 20% lower the next week, while you make no changes. measuring a low standard deviation won't really capture that. all this is not to say this is a bad idea, but to point out for future consideration that an apparent large performance regression on a VM is a \"necessary but not sufficient\" condition for an actual performance regression <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 215144471,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604088456
    },
    {
        "content": "<p>That's fair. We'll have to see what the numbers look like over longer periods of time.</p>",
        "id": 215144651,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1604088542
    },
    {
        "content": "<p>Kevin, since you seem to know something about this: we have ~$10k credits on MS Azure and are projecting to use ~$300 for olean serving, which is why we went with this setup for benchmarking. Do you know if there's a way to do more reliable benchmarking using Azure?</p>",
        "id": 215144858,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604088637
    },
    {
        "content": "<p>We don't need perfect reliability and Jannis' setup is a clear improvement over what we had before.</p>",
        "id": 215145004,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604088716
    },
    {
        "content": "<p>But it's quite possible there's a better approach using the same resources and we're just not aware.</p>",
        "id": 215145085,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1604088741
    },
    {
        "content": "<p>I think it's a good idea to keep this running. Just don't be surprised if half the time, when it looks like things got slow, actually it was just VM flakiness</p>",
        "id": 215145291,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1604088865
    },
    {
        "content": "<p>I just found out that Azure also offers dedicated servers. :) They cost between 2000 and 87000 Euro/month. :(</p>",
        "id": 215145726,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1604089114
    },
    {
        "content": "<p>Have you tried profiling instructions instead of wall-clock time, using e.g. <code>perf stat</code>? It doesn't measure the speedup from parallelism, though.</p>",
        "id": 215187981,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1604145064
    },
    {
        "content": "<p>No; in fact I've never used <code>perf stat</code> before. Are these numbers useful? If they are, I can report them as well.</p>",
        "id": 215189842,
        "sender_full_name": "Jannis Limperg",
        "timestamp": 1604148000
    },
    {
        "content": "<p>They should be pretty much proportional to task-clock time while being less noisy, so they could be a strictly better regression indicator; I've never benchmarked on a VM, however.</p>",
        "id": 215190215,
        "sender_full_name": "Sebastian Ullrich",
        "timestamp": 1604148498
    }
]