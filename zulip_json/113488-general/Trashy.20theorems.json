[
    {
        "content": "<p>Wasn't sure what to call this one lol. Of the items in mathlib, <code>finset.sum_range_sub_of_monotone</code> and <code>composition_as_set_equiv._proof_5</code> produce the largest number of garbage/intermediate items while passing through the kernel (8x and 5x the runners up respectively). I don't think it's a problem or anything since in absolute terms it's not a huge amount of memory being consumed, I'm just curious if anyone with an understanding of these parts of mathlib had any insight into what about these items might be the underlying cause. Guesses/conjecture are also welcome.<br>\nThanks!</p>",
        "id": 206591025,
        "sender_full_name": "Chris B",
        "timestamp": 1597160438
    },
    {
        "content": "<p>How are you measuring this?</p>",
        "id": 206642316,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1597187426
    },
    {
        "content": "<p><a href=\"#narrow/stream/236449-Program-verification/topic/Arena.20allocation.20for.20ITPs/near/206617336\">https://leanprover.zulipchat.com/#narrow/stream/236449-Program-verification/topic/Arena.20allocation.20for.20ITPs/near/206617336</a></p>",
        "id": 206642761,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1597187722
    },
    {
        "content": "<p>and you're in the clear -- it's all polynomials, and computing pi to 6 decimal places</p>",
        "id": 206643027,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1597187891
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/113488-general/topic/Trashy.20theorems/near/206642316\">said</a>:</p>\n<blockquote>\n<p>How are you measuring this?</p>\n</blockquote>\n<p>He beat me to it, but it's an independent type checker. In terms of inference and equality checks it follows Lean's kernel pretty closely, so I'm  fairly confident these numbers are close to what you would actually get in Lean 3. I'm using Lean 4's strategy for inductive and quotient reduction, but not the nat stuff yet.</p>",
        "id": 206645004,
        "sender_full_name": "Chris B",
        "timestamp": 1597189689
    },
    {
        "content": "<p>Also it seems like suspicions in the other thread are converging on the <code>omega</code> tactic.</p>",
        "id": 206645481,
        "sender_full_name": "Chris B",
        "timestamp": 1597190074
    },
    {
        "content": "<p>from my experience writing the <code>next_coeff</code> proof, the <code>omega</code>s were the part I was saddest about. I tried extracting specialized arithmetic statements, stuff that really seemed to me like they shouldn't be named lemmas, and I had difficulty proving them concisely without <code>omega</code>.</p>",
        "id": 207047297,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597539041
    },
    {
        "content": "<p>i think a slightly better \"control flow\" in the argument could cut down on duplicated <code>omega</code> work, among other things. The whole proof feels like it's screaming out for more automation. I think i recently saw a pair of Coq tactics for \"pull the nth summation symbol to be the innermost / outermost sum\". I guess that really doesn't apply to the thing we're doing with the double sum here.</p>",
        "id": 207048320,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597541007
    }
]