[
    {
        "content": "<p>One claim that is often made when it comes to AIs that will conjecture and prove their own theorems is that the mathematical spaces is infinite-dimensional and the AI will just wander off into some silly boring corner while it happily proves silly boring theorems.</p>\n<p>I don't know much about AI. But have people trained AIs to judge music, or other forms of art? And score it along axes like</p>\n<ul>\n<li>this will be popular among the public (likes on youtube etc)</li>\n<li>this will be considered \"original\"</li>\n<li>this is technically difficult, but executed perfectly</li>\n<li>etc...</li>\n</ul>",
        "id": 225647623,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612848531
    },
    {
        "content": "<p>It would be extremely interesting if an AI could automatically recognize theorems in mathlib as \"helper lemmas\" or \"main results\".</p>",
        "id": 225647655,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612848592
    },
    {
        "content": "<p>[Of course some might say that if you give a bunch of computer scientists a theorem prover then they may well also just wander off into some silly boring corner and happily prove silly boring theorems...]</p>",
        "id": 225653655,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1612856326
    },
    {
        "content": "<p>I think partly the reason why humans don't wander off into silly boring corners is that we are essentially monkeys and so we have monkey intuition about several objects (including natural numbers and the 3-dimensional Euclidean space).</p>\n<p>The vast majority of college students worldwide who take calculus probably don't know ZFC or any other formal system in which reals can be defined yet they manage to take derivatives. They just visually imagine a straight line and that's usually good enough.</p>\n<p>Giving AI monkey intuition might be a way to prevent it from going to a silly boring corner. But developing monkey intuition took us millions of years so I am not sure how easy would it be.</p>",
        "id": 225655830,
        "sender_full_name": "Roman Bars",
        "timestamp": 1612858390
    },
    {
        "content": "<p>On the other hand in the early 1980s when I was an active chess player it was sometimes argued that these crappy chess computers which played at sub-club level would never amount to anything, because humans played chess with a refined intuition which a machine would never emulate.</p>",
        "id": 225656489,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1612858846
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> we have techniques for that; called GANs (generative adversarial networks). They're not shining as much on text as they do on image; but they have already been applied successfully there. The training objective is exactly what you're describing. One model conjectures statements; the other judge them and attempt to discriminate between synthetic and \"ground-truth\" statements (say the ones in mathlib). The equilibrium is reached when the judge can't discriminate the fake statements anymore.</p>\n<p>There are many variations around that idea that could be explored and are hugely exciting. But it all starts with having a good \"prover\" so that once we have generated these potentially interesting statements (or dare I say, definitions) we can submit them to a system that is not trivially bad at demonstrating/working with them (cc <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> who I know is prolly excited by these kind of projects). From there we can hope for continuous self-improvement and therefore potentially interesting knowledge creation.</p>\n<p>We're right now focusing on the prover; because without a good prover there's not much useful we can do with these generations (without intensive human labour, which is just simply not available on planet earth for this kind of tasks at the scale we would need :p)</p>",
        "id": 225661588,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862222
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> No, I'm not asking about proving things, or recognizing which conjectures are true.</p>\n<p>I'm talking about a metrics <code>beauty, important : expr -&gt; real</code>. And I wonder if an AI can get close to what humans can do there intuitively.</p>",
        "id": 225661814,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862367
    },
    {
        "content": "<p>Will it recognise <code>quadratic_reciprocity</code> not just as a \"true\" expression, but as one that is important, elegant, surprising, fundamental.</p>",
        "id": 225661846,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862396
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> in the setup above that would be exactly the signature of the discriminator <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 225661862,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862403
    },
    {
        "content": "<p>Ooh, ok. Then I misunderstood what you were describing <span aria-label=\"see no evil\" class=\"emoji emoji-1f648\" role=\"img\" title=\"see no evil\">:see_no_evil:</span></p>",
        "id": 225661931,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862430
    },
    {
        "content": "<p>The exact \"pseudo-signature\" would be: <code>likeness_to_mathlib : expr -&gt; real</code> but that's a not so bad proxy to what you're looking for in the space of all possible statements.</p>",
        "id": 225662002,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862483
    },
    {
        "content": "<p>I'm still confused why you call it <code>likeness_to_mathlib</code>...</p>",
        "id": 225662300,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862690
    },
    {
        "content": "<p>I wonder how you would generate negative training examples, i.e. theorem statements that are \"unlike mathlib\".  There are still many theorems which are not in mathlib, but are important and beautiful.</p>",
        "id": 225662309,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1612862699
    },
    {
        "content": "<p>I think that 95% of mathlib is ugly, boring, stupid, simple helper lemmas, rfl-lemmas, simp-lemmas, api building stuff.</p>",
        "id": 225662351,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862729
    },
    {
        "content": "<p>An AI shouldn't train on a random subset of mathlib to get a sense of what it should aim for to prove a beautiful, elegant, surprising theorem</p>",
        "id": 225662435,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862767
    },
    {
        "content": "<p>Of course it can train on mathlib to get a good sense of how to build a strong and useful API around a definition</p>",
        "id": 225662458,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862787
    },
    {
        "content": "<p>But to guide an AI towards interesting results, it should probably train on Fabstracts...</p>",
        "id": 225662492,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862808
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span>  So that's the main benefits of GANs, you don't need negative samples to start with. This setup, when it works, converges towards the distribution you want to mimic: here the distribution of mathlib statements. The negative examples are generated by the network that attempts to fake them; at the beginning they are quite bad and the discriminator can discriminate them. And is trained along with the generator on exactly that (discrimination of the mathlib statements vs the statements generated by generator).</p>",
        "id": 225662609,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862882
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> you can replace the mathlib distribution by another one you feel is more important; such as \"important statements from mathlib only\"</p>",
        "id": 225662704,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862927
    },
    {
        "content": "<p>or if you want to mimic your tastes \"important statements from mathlib that are beautiful to johan\" :)</p>",
        "id": 225662736,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862950
    },
    {
        "content": "<p>Right... just filter by <code>git blame</code> <span aria-label=\"rofl\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rofl\">:rofl:</span> <span aria-label=\"laughing\" class=\"emoji emoji-1f606\" role=\"img\" title=\"laughing\">:laughing:</span></p>",
        "id": 225662766,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612862973
    },
    {
        "content": "<p>Good training signal! :)</p>",
        "id": 225662791,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612862989
    },
    {
        "content": "<p>But like I said, I think there is only a very small subset of mathlib that qualifies as \"landmark\" results.</p>",
        "id": 225662857,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612863008
    },
    {
        "content": "<p>agreed! and data scarcity is obviously an issue here</p>",
        "id": 225662880,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1612863027
    },
    {
        "content": "<p>It would be a subset of what is mentioned in <code>overview.yaml</code></p>",
        "id": 225662885,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612863028
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span> We have been working on something related to this, namely, extracting theorems from a proof that could look interesting/useful to humans, by imitating human theorem extraction process. We applied the learned agent to MetaMath, and discovered quite a few theorems that are used frequently in the MetaMath library. It's a recent submission to ICML (a machine learning conference). If you'd like, I can share the submission with you.</p>",
        "id": 225751436,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1612902121
    },
    {
        "content": "<p>Sounds good! I'm interested in seeing the paper</p>",
        "id": 225751655,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1612902203
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/AI.20and.20artistic.20taste/near/225647623\">said</a>:</p>\n<blockquote>\n<p>One claim that is often made when it comes to AIs that will conjecture and prove their own theorems is that the mathematical spaces is infinite-dimensional and the AI will just wander off into some silly boring corner while it happily proves silly boring theorems.</p>\n<p>I don't know much about AI. But have people trained AIs to judge music, or other forms of art? And score it along axes like</p>\n<ul>\n<li>this will be popular among the public (likes on youtube etc)</li>\n<li>this will be considered \"original\"</li>\n<li>this is technically difficult, but executed perfectly</li>\n<li>etc...</li>\n</ul>\n</blockquote>\n<p>I think this is generally an important issue, but maybe an AI will wander into a corner that we currently consider silly and boring, but is actually beautiful beyond human comprehension? One somewhat naïve way to go about this is that because I (and probably a lot of other mathematicians) intuitively consider long proofs that do not introduce novel concepts to be silly and boring, so if we could penalize longness and somehow recognize lack-of-novelty to penalize that as well, I think that would be a significant improvement over the purely stupid approach of training just a theorem prover.</p>\n<p>EDIT: Also there's been work on training AIs to learn human preferences. For example, <a href=\"https://www.gwern.net/GPT-2-preference-learning\">gwern trained GPT-2 to learn how to make good music</a>. Also there's <a href=\"https://scholar.google.com/scholar?q=human+preference+learning+artificial+intelligence\">this Google Scholar search I just did</a> which turns up a lot of more results that are kind-of-like-this. (I'm pretty sure there's more work/research in this area but I'm too lazy to find out.)</p>",
        "id": 228182937,
        "sender_full_name": "duck_master",
        "timestamp": 1614575441
    }
]