[
    {
        "content": "<p>Two new papers dropped (submissions to ICLR 2023) on the topic of auto-formalization of statements and proofs in ITPs using off-the-shelf pre-trained models (like Codex and Minerva).</p>",
        "id": 301531452,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664485393
    },
    {
        "content": "<ul>\n<li><a href=\"https://openreview.net/forum?id=pKu077C57fH\">Towards a Mathematics Formalisation Assistant using Large Language Models</a><ul>\n<li>Lean 3 and 4</li>\n<li>Tried translating 120 informal theorem statements to Lean 4</li>\n<li>Tried translating 13 natural language proofs to Lean 3</li>\n<li>Some novel features include<ul>\n<li>input dependent prompts (similar to retrieval augmented language models)</li>\n<li>using the Lean 3 to Lean 4 syntax translator (from mathport?) and the Lean 4 elaborator to filter candidate theorem translations</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>",
        "id": 301532728,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664485908
    },
    {
        "content": "<ul>\n<li><a href=\"https://openreview.net/forum?id=SMa9EAovKMC\">Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs</a><ul>\n<li>Isabelle</li>\n<li>Focus on miniF2F problems</li>\n<li>Gives an end-to-end solution which takes in informal statement and formal statement and then generates a proof (by first generating an informal proof and a formal Isabelle proof sketch, and finally applying hammer-like tactics to fill in the missing steps in the sketch).</li>\n</ul>\n</li>\n</ul>",
        "id": 301532735,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664485911
    },
    {
        "content": "<p>First, I want to say that I think both papers are very nice.  They really show the potential of out-of-the-box large pre-trained language models to write proofs in ITPs.  And like the recent <a href=\"https://arxiv.org/abs/2205.12615\">auto-formalization paper,</a> these techniques could be immediately <a href=\"https://github.com/zhangir-azerbayev/lean-chat\">coded up</a> and given to Lean, Isabelle, Coq users to play with.</p>",
        "id": 301886403,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678738
    },
    {
        "content": "<p>But I'm also a bit concerned by a particular element in the second paper (\"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs\").  This paper shows that one can solve many more proofs using LLMs to first make an informal proof, then convert that to a formal proof sketch, and finally fill in the holes with hammer-like tactics.  Since this paper uses large pre-trained language models, I think it should at least have a small discussion about leakage.  This is even more important since many problems in the miniF2F benchmark are famous and have been discussed on the internet repeatedly.  In particular, there is a lot of discussion on the 1959 IMO p1 which the model got correct.  (This is both the first IMO problem ever, and the one which some consider the easiest ever).</p>",
        "id": 301886408,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678755
    },
    {
        "content": "<p>Of course, some types of leakage are worse than others.  For example, if Codex had the miniF2F repo in its training data, then any problems with an Isabelle proof in that repo would be much easier for the model.  (Now Isabelle proofs were just added recently to miniF2f, and the Codex code-davinci-002 data is from Jun 2021, so I don't think there is this type of leakage, but this should be discussed in the paper.)  Also, some of these proofs could also occur in other repos or in other ITPs.  Finally, it is guaranteed that at least some of these theorems are in Minerva's and Codex's training data in their natural language form.</p>",
        "id": 301886409,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678761
    },
    {
        "content": "<p>Now this problem isn't unique to this paper.  Most of the benchmarks on miniF2F use pre-trained language models specifically pre-trained on math-heavy data.  This is very much desirable as it is an easy way to instill the model with \"math and code awareness\", but it also means that the miniF2F benchmark with it's heavy use of competition problems, many years old, should be thought of as not purely a reasoning benchmark, but an auto-formalization benchmark.  This means that models like sledgehammer which don't even have the ability to memorize human-written proofs are at a disadvantage, especially for famous problems which have made their way into the training data (in natural language form).  A cynical view (to play the devil's advocate) would be that this disadvantage might be reduced heavily if evaluated on problems which are guaranteed to be novel.</p>",
        "id": 301886451,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678767
    },
    {
        "content": "<p>I do think this is a tricky matter.  The majority of formalization in mathematics is formalizing stuff which has already been published, some if it decades old and ubiquitous on the Internet.  It is quite advantageous to have a model like this one, which given a theorem, automatically retrieves and reconstructs a number of informal proofs of that theorem, and uses those to make a number of formal proof sketches which are tested to see if they can be automatically filled in.</p>",
        "id": 301886456,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678773
    },
    {
        "content": "<p>By the way, the first paper (Towards a Mathematics Formalisation Assistant using Large Language Models) goes very much out of their way to test on problems unlikely to be in the training data of Codex.</p>",
        "id": 301886463,
        "sender_full_name": "Jason Rute",
        "timestamp": 1664678776
    },
    {
        "content": "<p>I agree with Jason's assessment that the leakage problem should be addressed so that the readers are aware of this possibility. But to qualify it: the leakage, if any, is more of a Minerva problem (informal prover) than a Codex (autoformaliser) problem since the miniF2F/Isabelle partition contains not many proofs. For the DSP paper in particular, an ablation study shows that if one simply lets Codex write down the full formal proof, the performance is pretty bad. This to me suggests that pure memoisation does not work out-of-the-box.</p>",
        "id": 301935561,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1664717435
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I think the problem you are mentioning is indeed more specific to Minerva, where it's true that the majority of the test problems can be found on Google. But here, the main point of the paper was more to show that if an informal proof is available (whether it's written by a human or a by model), then you can use it to guide a formal prover. So the first application here is not necessarily to prove new problems, but more to facilitate the formalization / translation in Lean when informal proofs exist (which is the case for all theorems on mathlib I imagine).</p>",
        "id": 302018823,
        "sender_full_name": "Guillaume",
        "timestamp": 1664785511
    },
    {
        "content": "<p>After some research and analysis, I conclude that it is not likely that the abilities of large language models in the DSP paper can be attributed to memorisation, although I agree entirely with <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> that this is a very tricky matter and should be treated with care. My reasons for the conclusion are:</p>\n<ol>\n<li>\n<p>For coming up with informal proofs, Minerva and Codex are used. In section 5 of the <a href=\"https://arxiv.org/abs/2206.14858\">Minerva paper</a>, the authors analysed and discussed the possibility of memorisation by rephrasing the problems or changing the numbers/variables in the problems, plus a few more text-similarity examinations. They did not find convincing evidence that Minerva benefits from memorisation on their evaluation dataset. Codex is trained on code data and is less likely to remember mathematical proof data (it also performs inferiorly to Minerva models).</p>\n</li>\n<li>\n<p>For the autoformalisation of proof sketches, the specific Codex version (code-davinci-002) was trained on data up to June 2021, at which point the miniF2F benchmark was not publicly released yet! Also there is very little formalisation on the internet in general on these high-school maths competition problems. So it seems highly unlikely to me that Codex should remember anything about miniF2F.</p>\n</li>\n</ol>\n<p>I hope this is useful for the discussion of memorisation/data contamination with respect to this paper.</p>",
        "id": 302448403,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1664973854
    },
    {
        "content": "<p>I want to mention we had done an analysis in the Minerva paper about memorization, that is worth reading if you are concerned about this. We showed Minerva did a lot more than just memorization. In addition, Minerva attained great scores on new math exams this year (2022 UK, Polish math exams).</p>",
        "id": 304017408,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1665749629
    },
    {
        "content": "<p>I just looked at Section 5 of the Minerva paper.  I feel like it is stating two things:</p>\n<ul>\n<li>The MATH dataset wasn't in the training data for MInerva so memorization is impossible.</li>\n<li>Minerva does about the same with small changes to a question (it has not seen before).</li>\n</ul>\n<p>But does this say anything about competition level problems which may be in the dataset for PaLM or Minerva?  Those are not small changes from other problems.  Those are very different questions from each other.  Also, I don't think you looked at what happens when a problem does end up in Minerva's training data. (IMHO section 5.2 should have taken problems which were found in Minerva's training data, not problems that Minerva happened to get correct.)  Maybe the accuracy shoots up really high to almost 100%.</p>\n<p>Here are a few follow up hypotheticals:</p>\n<ul>\n<li>Do you think that Minerva would have done better or the same on the 2010-2020 polish math exams as the 2022 polish math exam, especially if those exams and their solutions made it into the training data for PaLM or Minerva (in English)?</li>\n<li>Do you think that Minerva would do the same on 1959 IMO p1 if it was or was not in the Minerva training data?</li>\n<li>Do you think the results in this ICLR paper which use Minerva would come out the same if the mini-F2F problems where replaced with different but similar caliber problems which are nowhere on the internet?</li>\n</ul>\n<p>I would also love to see the techniques of this ICLR paper applied to the 2022 Polish math exam.  That would be a good follow up and a cleaner experiment.  (But I'm not asking the authors to do that.  I know I'm being a Reviewer <a href=\"https://github.com/leanprover-community/mathlib/pull/2\">#2</a> right now.  I think the autoformalization part of the paper stands on its own.)</p>",
        "id": 304027964,
        "sender_full_name": "Jason Rute",
        "timestamp": 1665752886
    },
    {
        "content": "<p>Here is the arXiv version of one of the papers mentioned above:<br>\n<a href=\"https://arxiv.org/abs/2210.12283\">https://arxiv.org/abs/2210.12283</a></p>",
        "id": 306120667,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1666731529
    },
    {
        "content": "<p>The first paper is now public as both a <a href=\"https://arxiv.org/abs/2211.07524\">preprint</a> and a <a href=\"https://mathai2022.github.io/papers/17.pdf\">note for MATH-AI</a>.  Moreover, the authors <span class=\"user-mention\" data-user-id=\"439607\">@Ayush Agrawal</span>, <span class=\"user-mention\" data-user-id=\"266304\">@Siddhartha Gadgil</span>, <span class=\"user-mention\" data-user-id=\"248811\">@Navin Goyal</span>, <span class=\"user-mention\" data-user-id=\"250372\">@Ashvni Narayanan</span>, and <span class=\"user-mention\" data-user-id=\"303675\">@Anand Rao</span>  just released their LeanAid tool which is like LeanChat, but seems to have more powerful integration with Lean including checking if a translation typechecks and using Lean's environment docstrings to do adaptive prompting.  They include a repo and a demo video.</p>\n<ul>\n<li><a class=\"stream-topic\" data-stream-id=\"270676\" href=\"/#narrow/stream/270676-lean4/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#lean4 &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#general &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.203\">#general &gt; LeanAide translation: Natural language to Lean 3</a></li>\n</ul>",
        "id": 312985378,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669783210
    },
    {
        "content": "<p>Here is also another MATH-AI paper along these lines:</p>\n<ul>\n<li><a href=\"https://mathai2022.github.io/papers/20.pdf\">ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems</a> by <span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span>, <span class=\"user-mention\" data-user-id=\"257492\">@Bartosz Piotrowski</span>, and <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span></li>\n</ul>",
        "id": 312985812,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669783557
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/More.20papers.20on.20autoformalization/near/312985378\">said</a>:</p>\n<blockquote>\n<p>The first paper is now public as both a <a href=\"https://arxiv.org/abs/2211.07524\">preprint</a> and a <a href=\"https://mathai2022.github.io/papers/17.pdf\">note for MATH-eAI</a>.  Moreover, the authors <span class=\"user-mention silent\" data-user-id=\"266304\">Siddhartha Gadgil</span> <span class=\"user-mention silent\" data-user-id=\"303675\">Anand Rao</span>  just released their LeanAid tool which is like LeanChat, but seems to have more powerful integration with Lean including checking if a translation typechecks and using Lean's environment docstrings to do adaptive prompting.  They include a repo and a demo video.</p>\n<ul>\n<li><a class=\"stream-topic\" data-stream-id=\"270676\" href=\"/#narrow/stream/270676-lean4/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#lean4 &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.204\">#general &gt; LeanAide translation: Natural language to Lean 4</a> </li>\n<li><a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanAide.20translation.3A.20Natural.20language.20to.20Lean.203\">#general &gt; LeanAide translation: Natural language to Lean 3</a></li>\n</ul>\n</blockquote>\n<p>One clarification: we use docstrings extracted from <code>mathlib</code>in addition to those from the environment (the latter is not currently used by default in Lean 4). This is a large set so allows good prompt engineering. These were extracted by running docgen and using the intermediate JSON file (with some cleanup).</p>",
        "id": 312986067,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1669783750
    },
    {
        "content": "<p>This is extremely cool!</p>",
        "id": 313016801,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1669800351
    }
]