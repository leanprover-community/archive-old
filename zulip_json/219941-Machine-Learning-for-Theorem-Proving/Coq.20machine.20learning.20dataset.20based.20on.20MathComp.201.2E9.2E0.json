[
    {
        "content": "<p>Dear Lean ML community,</p>\n<p>My colleagues and I have recently released a <a href=\"https://github.com/EngineeringSoftware/math-comp-corpus\">dataset</a> for use in machine learning, based on Coq 8.10.2 and <a href=\"http://math-comp.github.io\">Mathematical Components</a> version 1.9.0. It is based on 449 source files from 21 Coq projects, which are in total over 297k lines of code, with projects divided into tiers based on quality. Each source file has two machine-readable files which are lists of S-expressions: one file with Coq tokens and one file with Coq abstract syntax trees. Each token/AST contains detailed information on its source file provenance. These representations were obtained directly from Coq's implementation via (OCaml PPX) metaprogramming. We also provide raw and processed (\"chopped\") Coq kernel terms as S-expressions for each lemma <em>statement</em> (proof terms were not our focus in this work).</p>\n<p>So far, we have used the dataset for <a href=\"https://arxiv.org/abs/2004.07761\">deep generation of lemma names</a>, as discussed <a href=\"#narrow/stream/116395-maths/topic/auto-generating.20names.20for.20declarations\">here</a>, and for <a href=\"https://coq-workshop.gitlab.io/2020/#talk-1-2-3\">suggesting spacing</a>. We would be interested in if similar datasets/techniques can be done for Lean (3 or 4). </p>\n<p>For precision, our work uses ML for proof assistant user productivity and not for proof automation (although the dataset might be useful for proof automation by others).</p>",
        "id": 202256927,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593380625
    },
    {
        "content": "<p>I’ll be sure to take a look!</p>",
        "id": 202259615,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593385424
    },
    {
        "content": "<p>I have a superficial question.  This is at least the fourth dataset for machine learning for Coq (after GamePad, CoqGym, ProverBot, and TacticToe for Coq).  Do you all use the same tools to mine your data or are they all different?</p>",
        "id": 202259758,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593385664
    },
    {
        "content": "<p>to my knowledge, none of the other datasets have machine-readable source files. We did lots of engineering work together with a Coq developer to properly serialize all the tokens and the ASTs and the kernel terms. AST serialization has even been roundtrip tested - Coq can be made to process the serialized ASTs instead of source files.</p>",
        "id": 202259828,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593385767
    },
    {
        "content": "<p>I believe CoqGym and ProverBot also use a previous version of the library (<a href=\"https://github.com/ejgallego/coq-serapi\">SerAPI</a>) we used for serialization, but they use it to interact directly with Coq and produce their own intermediate data (e.g., capturing the proof context before and after executing a tactic). Based on my understanding of GamePad, they hacked Coq 8.6 and produced their own ad-hoc tactic tracer which is obsolete now. TacticToe for Coq is based on a homegrown plugin which hooks directly into Coq's internals. Our ML toolchain is completely independent of Coq for training/generation, since the dataset doesn't need Coq to be processed. The Coq (de)serialization pipeline is actively maintained and works, e.g., for Coq 8.11.</p>",
        "id": 202259974,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593386000
    },
    {
        "content": "<p>Also a note about data quality: this is a curated set of projects which we divided into tiers based on discussion with MathComp devs. ProverBot's dataset comes from a single project (CompCert), and CoqGym selected projects from Coq's OPAM index without any curation other than that they could be built easily with the Coq version they were using. TacticToe for Coq seemingly only uses Coq's stdlib, which is acknowledged by Coq devs as being of variable quality overall.</p>",
        "id": 202260642,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593386972
    },
    {
        "content": "<p>That is a very helpful answer!</p>",
        "id": 202261409,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593388174
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"198375\">Karl Palmskog</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Coq.20machine.20learning.20dataset.20based.20on.20MathComp.201.2E9.2E0/near/202259828\">said</a>:</p>\n<blockquote>\n<p>Coq source files are known to be near-impossible to lex/parse accurately by tools external to Coq.</p>\n</blockquote>\n<p>This is definitely the state we are in with Lean 3.  I've written some tools, and am writing more, to work with Lean programmatically, but definitely it is not possible to parse a Lean file by itself without recreating Lean.  (Lean has an export format, but it throws too much away for machine learning.  It's only good for checking proofs with external checkers.)  I know Lean 4 is supposed to have an intermediate AST layer.  I don't know if that will be easy to access or work with.  I've been told it is, but I haven't seen any examples proving it.</p>",
        "id": 202261572,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593388425
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Coq.20machine.20learning.20dataset.20based.20on.20MathComp.201.2E9.2E0/near/202261572\">said</a>:</p>\n<blockquote>\n<p>I know Lean 4 is supposed to have an intermediate AST layer. </p>\n</blockquote>\n<p>Right, I think something like this has to be available for viability of ML-for-productivity (which tends to use ASTs a lot). The \"trick\" used in SerAPI is to introspect on the OCaml data structures used to implement the Coq source AST. Coq does have abstract term-level definitions/structures in MetaCoq, but the interesting stuff usually happens long before a proof becomes a term.</p>",
        "id": 202261809,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593388757
    },
    {
        "content": "<p>Just so I am clear, what is ML-for-productivity?  Does \"productivity\" here mean cleaning up code by suggesting variable names, spacing etc?  And I assume the ML is because it is hard or impossible to formalize all this stuff into a perfect linter, so it helps to learn some of it automatically?  Is this the idea of the term and your work?</p>",
        "id": 202262029,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593389074
    },
    {
        "content": "<p>I would define it along those lines, yes, basically we work on tools/techniques to assist proof assistant users that are not directly related to proof automation. The general ideas come from software engineering and NLP, and they also consider other areas than just coding conventions: <a href=\"https://ml4code.github.io\">https://ml4code.github.io</a></p>",
        "id": 202262121,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593389249
    },
    {
        "content": "<p>Also, the question I have for every ML for TP project, is this a usable product?.  Most projects are research and information flows out of the theorem prover, but can't be \"put back in\" in a reasonable way.  In other words, a user of that theorem prover couldn't use it without a lot of work.</p>",
        "id": 202262189,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593389367
    },
    {
        "content": "<p>Sorry, my thoughts are bouncing around a lot.  Also, I'd be curious if Lean's extremely flexible custom notation and custom parsing would make it difficult to do something like ML-for-productivity.  Maybe not, since even though a user could go crazy with notation, they usually pick something reasonable, and especially the ML part would conform to how notation is usually created and used.  It wouldn't have to cover every corner case.</p>",
        "id": 202262309,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593389554
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Coq.20machine.20learning.20dataset.20based.20on.20MathComp.201.2E9.2E0/near/202262189\">said</a>:</p>\n<blockquote>\n<p>Most projects are research and information flows out of the theorem prover, but can't be \"put back in\" in a reasonable way. </p>\n</blockquote>\n<p>Currently, the output format in our <a href=\"https://github.com/EngineeringSoftware/roosterize\">name generation tool</a> is a simple list of current/suggested lemma name pairs. However, my ML co-author has been able to replace lemma names directly in source files, since we have exact location information via SerAPI. It will be based on community feedback if we implement this in a convenient interface. For example, a user may pinpoint files or whole projects and get lemma names (or spacing) instantly replaced with top-1 ML suggestions.</p>",
        "id": 202262368,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593389654
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Coq.20machine.20learning.20dataset.20based.20on.20MathComp.201.2E9.2E0/near/202262309\">said</a>:</p>\n<blockquote>\n<p>Also, I'd be curious if Lean's extremely flexible custom notation and custom parsing would make it difficult to do something like ML-for-productivity.  Maybe not, since even though a user could go crazy with notation, they usually pick something reasonable, and especially the ML part would conform to how notation is usually created and used.  It wouldn't have to cover every corner case.</p>\n</blockquote>\n<p>Coq also has flexible notations, and it was a major challenge to handle this (that's how we ended up extending the SerAPI library with a lot of functionality and three CLI tools). In our ablation studies on lemma naming, it's very clear that having elaborated terms as input helped a lot, since a lot of information used by Coq users for naming is hidden by notations.</p>",
        "id": 202262521,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593389952
    },
    {
        "content": "<p>As for useability, what you have actually seems fairly usable.  Maybe this is more of an issue with automatic theorem proving, but most such projects could not be used to help with proof creation in new development without basically rebuilding the whole project in OCaml.</p>\n<p>Also, many projects are tested and trained on data from the same file (and used a fixed vocabulary, which can't handle a new definition).  While that might be fine to show that a NN can learn theorem proving, it prompts the question: What would happen if you used this trained agent on a new project?  Would you have to retrain it?  If so, would that be practical or useful?  Actually, that is a good question for you:  Can your tool suggest new lemma names for new projects without retraining or does its performance significantly degenerate?</p>",
        "id": 202262729,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593390314
    },
    {
        "content": "<p>Sorry.  I used \"project\" in two different ways.  That last sentence is referring to new coq projects.</p>",
        "id": 202262782,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593390393
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Coq.20machine.20learning.20dataset.20based.20on.20MathComp.201.2E9.2E0/near/202262729\">said</a>:</p>\n<blockquote>\n<p>What would happen if you used this trained agent on a new project?  Would you have to retrain it?  If so, would that be practical or useful?  Actually, that is a good question for you:  Can your tool suggest new lemma names for new projects without retraining or doesn't its performance significantly degenerate?</p>\n</blockquote>\n<p>For our lemma name generation work, we did try our tool on a project that was <em>not</em> in our training/validation sets and investigated generalizability. Firstly, we did a manual evaluation where we sent a bunch of suggestions to the project maintainer and asked him to rate them <span aria-label=\"thumbs up\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"thumbs up\">:thumbs_up:</span> / <span aria-label=\"thumbs down\" class=\"emoji emoji-1f44e\" role=\"img\" title=\"thumbs down\">:thumbs_down:</span> . This is described starting on p. 13 in <a href=\"https://arxiv.org/pdf/2004.07761.pdf\">the paper</a>. Around 24% of (top-1) suggestions were rated as good/adequate. Secondly, we did a generalizability case study (p. 30) to automatically measure how well our tool works on a never-seen project and how performance increases as training data from that project is added.</p>\n<p>The tool is obviously only as good as the training/validation sets can make it, so as naming conventions evolve, one will have to retrain (for our best model trained on four core projects, this takes about a day of machine time). There may not always be a connection between a good score on a testing set and good names in practice.</p>",
        "id": 202263108,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593391053
    },
    {
        "content": "<p>I’ll have to read the paper!  It sounds well thought out.  (And I haven’t even asked about the ML ideas yet.)  I also hope more people start posting their papers here!</p>",
        "id": 202263277,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593391347
    },
    {
        "content": "<p>As far as I know (I mainly provided the Coq expertise for this work, my ML knowledge is limited), the models and evaluation approach are fairly standard in the context of NLP / machine translation, which seems distinct from ML proof automation. One of the senior co-authors does research in linguistics.</p>",
        "id": 202263347,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593391479
    },
    {
        "content": "<p>It seems in the last few months there have been a lot of papers showing that modern NLP tools (transformers) are capable of a lot of simple reasoning tasks.  I could imagine that we will see more overlap between ML for productivity and ML for theorem proving and ML for code generation.</p>",
        "id": 202263526,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593391787
    },
    {
        "content": "<p>Looks like there is some transformer stuff in this paper using Isabelle: <a href=\"https://arxiv.org/abs/2006.09265\">https://arxiv.org/abs/2006.09265</a></p>",
        "id": 202290241,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593424345
    },
    {
        "content": "<p>Yeah, I’ve been meaning to write about that one.</p>",
        "id": 202293978,
        "sender_full_name": "Jason Rute",
        "timestamp": 1593427453
    },
    {
        "content": "<p>My co-author presented the paper today at virtual IJCAR: <a href=\"https://youtu.be/Ysd-dcizw1A?list=PLl1dj5prwUJz6KV4Q1EPG9Gx0bjiKa_ey&amp;t=3462\">https://youtu.be/Ysd-dcizw1A?list=PLl1dj5prwUJz6KV4Q1EPG9Gx0bjiKa_ey&amp;t=3462</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"Ysd-dcizw1A\" href=\"https://youtu.be/Ysd-dcizw1A?list=PLl1dj5prwUJz6KV4Q1EPG9Gx0bjiKa_ey&amp;t=3462\"><img src=\"https://i.ytimg.com/vi/Ysd-dcizw1A/default.jpg\"></a></div>",
        "id": 202827192,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593799898
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"198375\">@Karl Palmskog</span> hello! I am really interested in  the \"tree chopping\" methods mentioned in your paper. However I know very little about the internal data structures of Coq, I wonder is there any documentation easy to read about this?<br>\nfor example, when I use coq_serapi to input :<br>\nforall xyz:nat, xyz&lt;3<br>\nI got output:<br>\n(Prod (Name (Id xyz)) (Ind (((Mutind (MPfile (DirPath ((Id Datatypes) (Id Init) (Id Coq)))) (DirPath ()) (Id nat)) 0) (Instance ()))) (App (Const ((Constant (MPfile (DirPath ((Id Peano) (Id Init) (Id Coq)))) (DirPath ()) (Id lt)) (Instance ()))) ((Rel 1) (App (Construct ((((Mutind (MPfile (DirPath ((Id Datatypes) (Id Init) (Id Coq)))) (DirPath ()) (Id nat)) 0) 2) (Instance ()))) ((App (Construct ((((Mutind (MPfile (DirPath ((Id Datatypes) (Id Init) (Id Coq)))) (DirPath ()) (Id nat)) 0) 2) (Instance ()))) ((App (Construct ((((Mutind (MPfile (DirPath ((Id Datatypes) (Id Init) (Id Coq)))) (DirPath ()) (Id nat)) 0) 2) (Instance ()))) ((Construct ((((Mutind (MPfile (DirPath ((Id Datatypes) (Id Init) (Id Coq)))) (DirPath ()) (Id nat)) 0) 1) (Instance ()))))))))))))<br>\nThis looks so terrifying! I really wonder about the meanings of Mutind, MPfile and Rel blablabla....<br>\nSo I can better understand what parts of the kernel tree is important for information extraction.<br>\nlooking forward to your reply, thanks a lot!</p>",
        "id": 219335204,
        "sender_full_name": "LEZHI HU",
        "timestamp": 1607520527
    },
    {
        "content": "<p>Just to let you know, there is a Coq Zulip as well, and it's also pretty active; you might be more likely to get a response there. <a href=\"https://coq.zulipchat.com\">https://coq.zulipchat.com</a></p>",
        "id": 219335736,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1607520824
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> although I knew that Zulip, thanks for your help!</p>",
        "id": 219340955,
        "sender_full_name": "LEZHI HU",
        "timestamp": 1607523421
    }
]