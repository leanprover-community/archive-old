[
    {
        "content": "<p>Autoformalization with Large Language Models</p>\n<ul>\n<li>Authors: <span class=\"user-mention\" data-user-id=\"240875\">@Yuhuai Tony Wu</span> , <span class=\"user-mention\" data-user-id=\"258175\">@Albert Jiang</span> , <span class=\"user-mention\" data-user-id=\"384425\">@Wenda Li</span> , <span class=\"user-mention\" data-user-id=\"217806\">@Markus Rabe</span> , Charles Staats,  Mateja Jamnik, <span class=\"user-mention\" data-user-id=\"239426\">@Christian Szegedy</span> </li>\n<li>Twitter announcement: <a href=\"https://twitter.com/Yuhu_ai_/status/1529887383629443072\">https://twitter.com/Yuhu_ai_/status/1529887383629443072</a></li>\n<li>arXiv: <a href=\"https://arxiv.org/abs/2205.12615\">https://arxiv.org/abs/2205.12615</a></li>\n</ul>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/Yuhu_ai_/status/1529887383629443072\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/2b2aea46b32aac6ac3e0b8b147d04b990bfe53bc/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313238313332303237383336333131393631372f64394e67673035545f6e6f726d616c2e6a7067\"></a><p>The next figure shows a perfect translation of a grade school math problem by PaLM. This is remarkable because such a statement is completely out-of-distribution â€“ no formal mathematicians are interested in formalizing grade school math problems ;)\n\n4/ <a href=\"https://t.co/By0QbcNQK0\">https://twitter.com/Yuhu_ai_/status/1529887383629443072/photo/1</a></p><span>- Yuhuai (Tony) Wu (@Yuhu_ai_)</span><div class=\"twitter-image\"><a href=\"https://t.co/By0QbcNQK0\"><img src=\"https://uploads.zulipusercontent.net/ec38f786b8da566fa012fceef765d4e057161aff/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4654744163334956734141436b63712e6a70673a736d616c6c\"></a></div></div></div>",
        "id": 284142154,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653662675
    },
    {
        "content": "<p>I know some here say that auto-formalization is near, but in reality it is a really hard task.  However, Google's N2Formal team and researchers at Cambridge seem to have made some progress on certain classes of self-contained problems.  The idea is that those pre-trained large language models one hears about on the news, like Codex and PaLM, already have some built-in ability to translate natural math to formal math, Isabelle in this case.  For those of us in AI, this isn't shocking but it is pleasantly surprising.  One basically has to ask the model in the right way (prompt engineering) and it will translate an informal math problem to a formal one.  I think they focused on about 4K math competition problems and formalized about <strong>25%</strong> of the statements correctly with the language model.  Further, then one can take those formalized statements and use them to improve existing neural theorem provers.  Note <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> et al already showed it is possible to improve a neural theorem prover on MiniF2F by manually building a curriculum of formal problems for the prover to train on.  But now, with a language model one can just auto-formalize a diverse list of problems to train on.  And indeed the authors show strong improvement on the Thor model (which also just came out) applied to the MiniF2F benchmark.</p>",
        "id": 284142187,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653662688
    },
    {
        "content": "<p>And for those interested in auto-formalization in general, there is a small discussion in Section 6 on auto-formalization of real math and of auto-informalization where we turn formal math into informal English math, which is an easier task for these models to do.</p>",
        "id": 284142199,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653662695
    },
    {
        "content": "<p>Also, nothing about this work seems specific to Isabelle.</p>",
        "id": 284142206,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653662697
    }
]