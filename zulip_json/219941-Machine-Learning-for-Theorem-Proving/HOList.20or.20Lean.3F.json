[
    {
        "content": "<p>Hello! I'm interested in getting started developing ML-based search strategies for ATP, but I'm having some trouble deciding between hacking on HOList vs Lean. Lean is appealing because it's going to be used for the IMO challenge and I'm more familiar with it as a prover, but HOList seems easier to get started with because it's got a \"gym environment\" with a fixed training set and a leaderboard of existing results. I'm also worried about developing for Lean 3 knowing that Lean 4 is coming.</p>\n<p>Does anyone here have advice on whether Lean is in a good state to begin integrating with ML-based search? In particular I'm curious as to whether there's work on a similar \"gym environment\" for Lean, and whether proofs &amp; methods developed for Lean 3 will be portable to Lean 4.</p>",
        "id": 216271126,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1605040823
    },
    {
        "content": "<p>I am interested in the same area. I don't think Lean 3 is in a great state to integrate with ML right now. The problem is that any sort of interfacing with external processes and doing some moderate amount of work in Lean is pretty slow. Gabriel Ebner has done the most work like this - see his talk in January - that was integrating with ATPs rather than with a machine-learning-driven system, but the challenges are similar. But he's waiting on Lean 4 for more work and I think his judgment is correct there</p>",
        "id": 216276103,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605043212
    },
    {
        "content": "<p>To me, HOList doesn't seem promising to work with either. There's a leaderboard but the only people who interact with it in any way are people at one particular group at Google. So it isn't so much a big public thing as it is a specific approach by that group. I just suspect they will abandon it at some point, it's not like there's a community using it for anything. Maybe it is a good fit for what you're doing, though, if you're interested in work that would map nicely onto what they are doing</p>",
        "id": 216276258,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605043286
    },
    {
        "content": "<p>I see, thanks. After looking into it more it does look like Lean 4 will have nicer utilities for integrating with external ATP.  Have you found other environments you see as more promising?</p>",
        "id": 216276574,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1605043429
    },
    {
        "content": "<p>I think it depends what you want to do - do you want to publish an AI paper, or do you want to publish an ATP paper, or do you want to make some tool that will help mathematicians, or something else entirely</p>",
        "id": 216276703,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605043469
    },
    {
        "content": "<p>I'm primarily from an ML background, and I'm interested in developing reinforcement-learning-based search strategies that will help big ATP efforts like the IMO challenge (the work would look much more like an ML paper than an ATP paper)</p>",
        "id": 216276806,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1605043526
    },
    {
        "content": "<ol>\n<li>if you have some theory for how to beat the results in the \"Learning to Reason in Large Theories without Imitation\" paper, then hacking your idea into holist would make sense</li>\n</ol>",
        "id": 216277308,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605043760
    },
    {
        "content": "<ol start=\"2\">\n<li>kind of \"lower in the stack\" is work like <a href=\"https://arxiv.org/abs/2005.02094\">https://arxiv.org/abs/2005.02094</a> on the automated theorem provers - those are aiming at simpler logical forms like first-order logic instead of all this meta-inductive-type stuff. there, the state of the art generally does not use machine learning. it seems possible for machine learning to help out there though. i dunno the best way to approach, maybe find one of the open source theorem provers and tweak it, depends on your idea</li>\n</ol>",
        "id": 216277593,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605043906
    },
    {
        "content": "<ol start=\"3\">\n<li>\"higher in the stack\" would be theorem proving that lives in the interface layer. afaict lean 3 is slower than isabelle so the coolest automated stuff happens in isabelle there, but neither one is really suitable for modern AI techniques unless you pipe all the data over to some other process that uses the gpu, or something else that is kind of mucking around in the internals</li>\n</ol>",
        "id": 216277739,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605043985
    },
    {
        "content": "<p>My work would probably be most appropriate \"higher in the stack\" -- at a high level I want to help turn interactive provers into automated ones with learned policies, possibly by imitating human proofs + RL, similar to the existing work on HOList. Mainly this is because proofs are shorter there.</p>\n<p>I think I read that Lean 4 will have utilities to help with \"symbolic simulation\" so you don't have to translate Lean terms into your library before working with them, am I interpreting that right?</p>",
        "id": 216278064,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1605044143
    },
    {
        "content": "<p>I don't know - I don't think the features of Lean 4 are quite all figured out yet</p>",
        "id": 216278399,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605044330
    },
    {
        "content": "<p>so I think that it will just not be easy to integrate Lean 3 with an external AI process. hopefully Lean 4 makes this a lot better, but don't hold your breath waiting for that to happen. with Isabelle it is certainly possible to integrate with external processes, sledgehammer does this, but I don't know enough more about the Isabelle system to firmly recommend for or against it.</p>",
        "id": 216278665,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605044496
    },
    {
        "content": "<p>This was very very helpful, thank you!</p>",
        "id": 216278936,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1605044643
    },
    {
        "content": "<p>My short answer is that ML for Lean would be bleeding edge.  There is no good interface or dataset yet, but I'm optimistic that there will be more infrastructure soon.  Having said that, I hope you keep an interest in Lean.  It has a fast growing database, and I think ML for theorem proving does not need to be exclusive to any single theorem prover.</p>",
        "id": 216288560,
        "sender_full_name": "Jason Rute",
        "timestamp": 1605050527
    },
    {
        "content": "<p>There is a lot of interest in ML for Lean, some from large research groups like yourself (I'm assuming you are at NVIDIA from your LinkedIn), and some from hobbyists, or those firmly in the theorem proving community.  <strong>I think one of the best things you and others can do now is clearly state what you would want if an interface for Lean 3 existed (or if a dataset existed).  There are a few of us interested in building such things, and we would be happy to discuss further.</strong></p>",
        "id": 216288563,
        "sender_full_name": "Jason Rute",
        "timestamp": 1605050535
    },
    {
        "content": "<p>As for Lean 4, it is unclear how long it will take.  I do think that there will be advantages in theory, but there won't be an ML framework when it is built that I am aware of, so I'm inclined to start building something for Lean 3.</p>",
        "id": 216288566,
        "sender_full_name": "Jason Rute",
        "timestamp": 1605050543
    },
    {
        "content": "<p>As for the Grand Challenge, I'm not convinced that ML and RL done on human written Lean proofs will get very far in proving Olympiad problems, but I do think it will go really far in improving the ability of Lean to do trivial and simple tasks.  Also, I see the appeal for at least some dataset in the same language as the Grand challenge.</p>",
        "id": 216288569,
        "sender_full_name": "Jason Rute",
        "timestamp": 1605050547
    },
    {
        "content": "<p>As for other ML fo ITP projects, it really depends what you want.  HOList and CoqGym have some of the best datasets and RL interfaces, but unfortunately no one besides the developers uses them yet.  Also look into ProverBot9001 for Coq.  There are more datasets (for Isabelle for example), but I don't think they have interactive RL environments.  Also, you can be like OpenAI and build your own Metamath environment since the logic is really simple to implement.</p>",
        "id": 216288876,
        "sender_full_name": "Jason Rute",
        "timestamp": 1605050775
    },
    {
        "content": "<p>I definitely agree that naive RL on individual proof steps won't get us all the way to IMO problems. One hope I have is that ML-guided \"mid-level provers\" will enable the kind of \"very-high-level\" tactics Daniel Selsam described in his IMO challenge talk by dispatching intermediate goals. Another is that hierarchical procedural abstraction would allow learning multiple levels of guidance at once.</p>\n<p>As for a Lean automation interface / dataset: I'd definitely be happy to discuss more. I have personal interest in Lean from a type theory and formal proving perspective, and I think big datasets with well-defined measures of progress are responsible for a lot of success in ML.</p>\n<p>In the immediate future I'm leaning towards HOList for this project, partly for performance reasons as Kevin brought up. After discussing with the HOList authors, time taken to run the provers seems to be one of the biggest bottlenecks; their latest work uses a lot of compute already, mostly for generating experience. Fortunately, I don't think any given ML algorithm is likely to be too specialized to the prover it was developed in.</p>",
        "id": 216295805,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1605056287
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/216288563\">said</a>:</p>\n<blockquote>\n<p><strong>I think one of the best things you and others can do now is clearly state what you would want if an interface for Lean 3 existed (or if a dataset existed).  There are a few of us interested in building such things, and we would be happy to discuss further.</strong></p>\n</blockquote>\n<p>I would like functionality similar to the Isabelle sledgehammer, where you could export a theorem statement to some external prover, and then have the Lean kernel verify that proof. On top of that, I would like a script that you could give your own theorem prover to it, ML-based or otherwise, and it would just check against all the existing theorems in mathlib to see which ones your theorem prover could handle. That combo would basically let you use \"all the theorems in mathlib\" as training data. I suspect over time that would become the largest &amp; best ML-for-math training dataset, based on the velocity of the mathlib community.</p>",
        "id": 216306968,
        "sender_full_name": "Kevin Lacker",
        "timestamp": 1605069206
    },
    {
        "content": "<p>Sorry for potentially hijacking the HOList vs Lean thread (though my question will probably be relevant)...but I was wondering, what is wrong with Coq vs HOList or Lean?</p>",
        "id": 239632337,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1621532900
    },
    {
        "content": "<p>Since this is resurrected and I had totally forgot about this conversation, <span class=\"user-mention\" data-user-id=\"349892\">@Aidan Swope</span>, let me mention that since this conversation, we have produced two Lean datasets, a theorem prover evaluation harness, a trained Transformer model, and a tactic for users to use that model to assist their proofs.  Here is the <a href=\"https://arxiv.org/abs/2102.06203\">paper</a> and a <a href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\">talk</a>.  We would be happy to assist anyone looking to use the dataset or evaluation harness for their own projects.</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"EXpmbAfBNnw\" href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\"><img src=\"https://uploads.zulipusercontent.net/bd2a9c6a819c867efb19ea439f4a113b891f5dae/68747470733a2f2f692e7974696d672e636f6d2f76692f4558706d624166424e6e772f64656661756c742e6a7067\"></a></div>",
        "id": 239666693,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621548498
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"246156\">Brando Miranda</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239632337\">said</a>:</p>\n<blockquote>\n<p>Sorry for potentially hijacking the HOList vs Lean thread (though my question will probably be relevant)...but I was wondering, what is wrong with Coq vs HOList or Lean?</p>\n</blockquote>\n<p>I don't think anyone here is knocking on Coq.  (I even mention it above as a possibility.)  I think Adrian expressed his choice as (1) HOList because it is the easiest to use right now, or (2) Lean because he knows it the best.</p>",
        "id": 239668514,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621549642
    },
    {
        "content": "<p>I think however this highlights a big issue with this line of research.  <strong>It is very hard to get started.</strong>  Let's say you are interested in developing a neural theorem prover for Coq (or Lean, Metamath, HOL Light, Isabelle, HOL4, or Mizar).  </p>\n<ul>\n<li>First you have to learn the system, downloading it and playing around with it enough to get familiar.  It is much easier if you already know the system.</li>\n<li>Then you have to look for datasets and evaluation frameworks or build your own.  Right away, you may run into a problem.  You could be too constrained to other researchers choices.  For example, CoqGym doesn't have premise selection if I understand correctly, meaning it does half the work for you by already giving you the premises of the proof.  Tactician has a very specific way that a machine learning agent plugs into the framework, meaning a Transformer model wouldn't be useful.  Neither support reinforcement learning.  The same issue also applies to any other project including ours.  Unless you want to be unduly constrained, you likely have to start hacking the theorem prover to make it do what you want.  (Metamath and FOL (Mizar) have a slight advantage here since it is fairly easy for reconstruct the logic from scratch without having to plug it into another system.)</li>\n<li>If you want to create a tool in the end which is helpful for users, that might be more work as well.</li>\n</ul>",
        "id": 239668563,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621549683
    },
    {
        "content": "<p>For a while I dreamed of a gym like system that one could hook their neural prover into and it would work for all the standard ITPs, but I've since become more pessimistic.  The best I hope for now is that a research institute like FAIR would put the effort into instrumenting all the theorem provers so they could try out their approach on all of them.  However, I doubt other researchers would be satisfied with their approach and willing to use their system without modifications.</p>",
        "id": 239668629,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621549706
    },
    {
        "content": "<p>Thanks for the reply Jason! I am curious, is there something particularly bad about Coq?</p>",
        "id": 239671242,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1621551156
    },
    {
        "content": "<p>I think there could be a project going in the other direction, where ML researchers document their requirements from the prover so that the prover can support these use cases. Currently theorem provers are basically universally designed for interfacing with humans, not ML systems, which means there are a lot of square-peg designs in these ML interfaces where you try to drive the theorem prover in a way it fundamentally wasn't designed to be used, and as a result you have significant overheads and low quality data to work with, meaning less learning and poorer results.</p>\n<p>As someone who has been designing a high performance theorem prover backend this is definitely on my mind, but I'm not an ML researcher and I don't want to make things harder for future ML systems that want to target MM0.</p>",
        "id": 239673178,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621552213
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239666693\">said</a>:</p>\n<blockquote>\n<p>Since this is resurrected and I had totally forgot about this conversation, <span class=\"user-mention silent\" data-user-id=\"349892\">Aidan Swope</span>, let me mention that since this conversation, we have produced two Lean datasets, a theorem prover evaluation harness, a trained Transformer model, and a tactic for users to use that model to assist their proofs.  Here is the <a href=\"https://arxiv.org/abs/2102.06203\">paper</a> and a <a href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\">talk</a>.  We would be happy to assist anyone looking to use the dataset or evaluation harness for their own projects.</p>\n</blockquote>\n<p>Thank you for the mention and info, Jason! Over the last few months my group has been working with HOL Light, but I'll bring up this new benchmark with them. Nothing in our code or method is too specific to HOL Light, so I could see us switching.</p>\n<p>I've noticed that Lean 4 seems to be nearing a stable release. I don't know very much about Lean's internals, so I'm wondering: when and how do you expect Lean 4 to change the ML + ATP landscape? I've heard that many of the proofs in Mathlib may be ported to Lean 4, but I don't know how long I'd expect to wait until a sizable dataset is available.</p>",
        "id": 239687336,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621563558
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"246156\">Brando Miranda</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239632337\">said</a>:</p>\n<blockquote>\n<p>Sorry for potentially hijacking the HOList vs Lean thread (though my question will probably be relevant)...but I was wondering, what is wrong with Coq vs HOList or Lean?</p>\n</blockquote>\n<p>I'll echo Jason's thoughts on it being hard to get started, with very little developed infrastructure for gathering data and interacting with an environment relative to, say, doing reinforcement learning on Atari games. Another big factor is that benchmarks face a sort of Schelling point problem -- it's hard to produce a convincing paper if you're not comparing to existing results, so most ML research tends to cluster around already-existing benchmarks (aside from the time and effort it takes to set up a new one).</p>",
        "id": 239687669,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621563860
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"349892\">@Aidan Swope</span> I think it will be a long while before mathlib is ported to Lean4, but I could be mistaken.  It will be much easier to interface with Lean, but at the same time someone is going to have to put in the effort to provide the infrastructure for proof data gathering, evaluation, and reinforcement learning.  Also the IMO grand challenge is in Lean 4 which attracts folks, but at the same time that is more of a showcase of Microsoft Research than a completion or benchmark.</p>",
        "id": 239693142,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621569600
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"349892\">@Aidan Swope</span> also to be fair to Brando,  Coq has right now the most benchmarks (at least by different groups).  There is ProverBot9001, Tactician, and CoqGym (ASTactic), as well as the CoqHammer benchmarks.</p>",
        "id": 239693642,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621570023
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239693142\">said</a>:</p>\n<blockquote>\n<p>Also the IMO grand challenge is in Lean 4 which attracts folks, but at the same time that is more of a showcase of Microsoft Research than a completion or benchmark.</p>\n</blockquote>\n<p>I can't speak for everybody involved, but I aim to develop the IMO-GC into the ImageNet of the 2020s, i.e. the premiere AI benchmark that helps shape the global AI research agenda.</p>",
        "id": 239694932,
        "sender_full_name": "Daniel Selsam",
        "timestamp": 1621571274
    },
    {
        "content": "<p>In case useful, I spent some time this week refreshing the PACT paper related projects (<span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  <a href=\"https://github.com/jasonrute/lean_proof_recording_public\">lean_proof_recording</a> and <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> <a href=\"https://github.com/jesse-michael-han/lean-step-public\">lean-step</a>). I was able to successfully apply both techniques to lean-liquid on top of updated recent mathlib. <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> I think these two projects are a good specification by example of what ML researchers would need to extract ground-truth human examples. I would look in particular at Jason's code modification to capture tactic applications and Jesse's approach to extracting proof terms. They are both implemented in pure lean, Jesse's approach does not need any modification to Lean but Jason's does. Maybe exposing an improved version of  what Jason is doing is a good idea for Lean4? I guess basically provide a way to hook some code at each tactic execution that changes the tactic state?</p>\n<p>As for having an environment to interact with Lean3, I plan to hack one this week based on <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> 's <a href=\"https://github.com/jesse-michael-han/lean-tpe-public\">lean-tpe</a>. It'll be slow-ish but also designed to be easy to parallelize; should give you again a good specification by example of what we're looking for. It'll be mostly a REPL on top of pure Lean metaprogramming, so again, Lean3 is already good enough here and speed will be the main feature request for Lean4 :)</p>\n<p>Hope this helps!</p>",
        "id": 239710606,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1621583675
    },
    {
        "content": "<p>I think the very best we could do, is optimize the generation of these datasets, converging toward a standard version we all like and automatize their generation as part of Lean's CI pipeline, to make them one download away from any ML researcher + having a simple versatile gym like environment. </p>\n<p>As for the environment, I think exposing an interactive tactic mode REPL (as is my plan) is a very solid MVP for this. We don't want to use the lean-server for that as we don't want to send diffs triggering reparse/rebuild as it makes backtracking in proofs a living hell. We instead want to have a stateful REPL instantiated on a declaration name which returns serialized tactic states along with persistent identifiers for them and accept applying tactic code to previously returned tactic state identifiers.</p>",
        "id": 239711461,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1621584141
    },
    {
        "content": "<p>Minor correction:  The lean proof recording repo is currently here: <a href=\"https://github.com/jasonrute/lean-proof-recording-public\">https://github.com/jasonrute/lean-proof-recording-public</a>.  (The above link is missing the dashes.)</p>",
        "id": 239749484,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621603537
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"249373\">Stanislas Polu</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239710606\">said</a>:</p>\n<blockquote>\n<p>Maybe exposing an improved version of  what Jason is doing is a good idea for Lean4? I guess basically provide a way to hook some code at each tactic execution that changes the tactic state?</p>\n</blockquote>\n<p>Actually, in Lean4 I've been told, there is a built in way to do this sort of stuff, so one doesn't need as much hacking.  See <a href=\"#narrow/stream/270676-lean4/topic/Creating.20Lean.20environment/near/231266819\">this message from Leo</a>.  I haven't had a chance to play around with it yet.</p>",
        "id": 239750013,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621603762
    },
    {
        "content": "<p>But I agree, we already have what we need in Lean 3.  As you said, the big advantages of Lean 4 are speed (and possibly more official support from MR).</p>",
        "id": 239751025,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621604153
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"249373\">Stanislas Polu</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239711461\">said</a>:</p>\n<blockquote>\n<p>I think the very best we could do, is optimize the generation of these datasets, converging toward a standard version we all like and automatize their generation as part of Lean's CI pipeline, to make them one download away from any ML researcher + having a simple versatile gym like environment. </p>\n</blockquote>\n<p>Do you think we can come up with a format that \"we all like\"?  That would be great, but I'm a bit skeptical.  The current solutions out there are so different from each other that it is hard to know if we can all agree on a single environment.  I however totally agree that we can come up with a good proof-of-concept/MVP.  I'd also love more collaboration between research labs on data formats and environments.  (Maybe that is already going on behind the scenes, but it doesn't seem like it to me.)</p>",
        "id": 239752061,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621604571
    },
    {
        "content": "<p>Oh, I missed the \"converging towards\".  So the \"we all like\" is in the limit.  That I can get behind.</p>",
        "id": 239756193,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621605983
    },
    {
        "content": "<p>Well all the existing datasets predate the uprise of the Transformer. I’m ready to bet that they’ll all look the same in the coming years ;-)</p>",
        "id": 239778300,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1621614614
    },
    {
        "content": "<p>I'm not sure the Transformer solves premise selection.  Metamath GPT-f uses a Transformer to dream up the statement of premises, and uses a separate library search to match that statement to an available premise.  That is a fairly specialized feature which is harder to implement in other provers like Lean.  Lean GPT-f memorizes all the premises, which works well, but doesn't exactly scale to new projects.  I think other future systems may well still use a query-key lookup like HOList (likely where a Transformer makes the keys), or similarly a cross attention like Perceiver.  But I guess adding in support for listing all the premises in the environment (in an efficient way where one can cache premise embeddings) is going to be a fairly standard request.  (I could also imagine going further and doing cross attention over all the proof data.  This is essentially the TacticToe and Tactician approach.  There, they search through all the proofs for the ones which best match the current situation.  [They use k-NN instead of attention.]  The analogy is looking up the relevant literature instead of just memorizing it all.  I could imagine this reducing the model size and making it more scalable and adaptable to changes in the library.  I could also imaging this sort of thing is already being worked on in regards to information retrieval from documents corpuses in NLP.)</p>\n<p>Also, I'm not convinced that pretty printed text representations are always the way to go for tree/graph structured data.  It is definitely easy to build a system using those, and it allows very powerful and flexible pre-training, but one is throwing away a lot of potentially useful data in the process.</p>\n<p>Anyway, I guess I don't know if we've all settled on the same approach, or if there are still a lot of new directions to explore.  But my bet is that we can't accurately predict what other labs are going to do and what data/representations they are going to use, especially if they are willing to hack the prover system to get the data they want.  Of course, that doesn't stop us from building something right now.</p>",
        "id": 239783453,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621617002
    },
    {
        "content": "<p>Agreed that listing all premises to pre-cache embeddings will be a feature request <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 239783896,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1621617229
    },
    {
        "content": "<p>But my bet remains that moving forward there wont'be lot of interest for S-expressions :p</p>",
        "id": 239784003,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1621617260
    },
    {
        "content": "<p>(but that's just a bet)</p>",
        "id": 239784024,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1621617274
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239693642\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"349892\">Aidan Swope</span> also to be fair to Brando,  Coq has right now the most benchmarks (at least by different groups).  There is ProverBot9001, Tactician, and CoqGym (ASTactic), as well as the CoqHammer benchmarks.</p>\n</blockquote>\n<p>Hi Jason! I didn't mean to sound like a criticism to Lean or HOList or that we should use Coq. I was genuinely curious how people were choosing the ITP language for ML. Though I did like Mario's train of thought - something deliberate perhaps for ML would be nice (or a perspective paper that argues the case for one would be nice for us non ITP experts).</p>",
        "id": 239821739,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1621636821
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> <span class=\"user-mention\" data-user-id=\"246156\">@Brando Miranda</span> I tried to <a href=\"https://gist.github.com/jasonrute/b124c06ab4f0092ded1cb163dc4417b1\">specify my thoughts on what such an interface would look like between an ITP and an ML agent</a>.  There are a lot of considerations and I hopefully made the issues more clear, even if not providing a specific template.  I'd also be curious on thoughts from other ML researchers, like <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span>, <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span>, <span class=\"user-mention\" data-user-id=\"240875\">@Yuhuai Tony Wu</span>, <span class=\"user-mention\" data-user-id=\"349892\">@Aidan Swope</span>.  (There are many more researchers which sometimes visit here, but I don't want to spam them.)  Also, I'm making the assumption that the ML agent is a separate system from the theorem prover, so it doesn't cover advanced tactics which are also a form of AI (and a big part of Daniel Selsam's work).</p>",
        "id": 239894749,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621711822
    },
    {
        "content": "<p>Looks great! I will take a look at it. The page says secret at the top so I assume I should not share it with anyone? (btw thanks for putting the effort to do that!)</p>",
        "id": 240072714,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1621872525
    },
    {
        "content": "<p>Feel free to share.  I usually leave my gists private, since I guess I don't feel a need to spam the world with my thoughts unless others think they are helpful.</p>",
        "id": 240077027,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621874473
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/239894749\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <span class=\"user-mention silent\" data-user-id=\"246156\">Brando Miranda</span> I tried to <a href=\"https://gist.github.com/jasonrute/b124c06ab4f0092ded1cb163dc4417b1\">specify my thoughts on what such an interface would look like between an ITP and an ML agent</a>.  There are a lot of considerations and I hopefully made the issues more clear, even if not providing a specific template.  I'd also be curious on thoughts from other ML researchers, like <span class=\"user-mention silent\" data-user-id=\"249373\">Stanislas Polu</span>, <span class=\"user-mention silent\" data-user-id=\"116045\">Jesse Michael Han</span>, <span class=\"user-mention silent\" data-user-id=\"240875\">Yuhuai Tony Wu</span>, <span class=\"user-mention silent\" data-user-id=\"349892\">Aidan Swope</span>.  (There are many more researchers which sometimes visit here, but I don't want to spam them.)  Also, I'm making the assumption that the ML agent is a separate system from the theorem prover, so it doesn't cover advanced tactics which are also a form of AI (and a big part of Daniel Selsam's work).</p>\n</blockquote>\n<p>This looks good! A couple of comments:</p>\n<p><strong>Communication architecture</strong><br>\nI would bias towards ease of parallelization as much as possible, as long as that doesn't unduly sacrifice efficiency. In my mind, the ideal system consists of a pool of (GPU-backed) stateless ML model workers, a pool of (CPU-backed) stateless environment / ITP workers, and one stateful environment + tree search worker per theorem.  I know that my group (and I suspect others) would be willing to throw a lot of parallelism at the problem.</p>\n<p>That scalability depends on being able to efficiently serialize proof state and treat the ITP as a stateless \"(proof state, action) --&gt; next proof state\" function. Is replaying the entire previous proof currently the only way to do this? Would it be possible to have the system \"blindly trust\" a given proof state, without needing to re-run the proof steps that constructed it?</p>\n<p><strong>Data format</strong><br>\nI think the way to go would be a structured data format with all the gory details available (say, protobuf with all namespaces fully-qualified), plus a library for efficiently pretty-printing samples. It would be a shame for the data format to rule out all approaches other than text-to-text transformers. Even those kinds of models have reasons to care about structured data -- in my work we parse the HOL Light S-expressions to provide a more compressed tokenization for the model than plain BPE  would allow. Additionally, <a href=\"https://arxiv.org/pdf/2105.02769.pdf\">some recent work</a> has cleverly processed structured data with transformers in a way that pretty-printed data wouldn't allow.</p>",
        "id": 240131495,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621904026
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"349892\">Aidan Swope</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/240131495\">said</a>:</p>\n<blockquote>\n<p>That scalability depends on being able to efficiently serialize proof state and treat the ITP as a stateless \"(proof state, action) --&gt; next proof state\" function. Is replaying the entire previous proof currently the only way to do this? Would it be possible to have the system \"blindly trust\" a given proof state, without needing to re-run the proof steps that constructed it?</p>\n</blockquote>\n<p>From the ITP side, this might be tricky to support, since it has to simultaneously be able to access and switch between several proof states. This could be accomplished if proof states are pure functional data structures which are allocated and given identifiers as part of the communication protocol, although with an approach like that it's not clear when it is safe to reclaim proof state objects.</p>\n<p>That is to say, the protocol would be something like this:</p>\n<ul>\n<li>The ITP maintains a map <code>proof_data(id: ProofStateID) -&gt; ProofStateData</code> that can be queried by the ML system to get information about proof states.</li>\n<li>You can call a function <code>apply(old_state: ProofStateID, a: Action) -&gt; ProofStateID</code> to apply a tactic or proof operation (of some description) to any allocated proof state to produce a new proof state; the ITP returns the ID of the proof state and you can use that ID to query more information about it or apply more operations from that starting point.</li>\n<li>Since the ITP doesn't know when the ML agent is done with a given proof state, it can't ever reclaim things, so perhaps we need a <code>discard(id: ProofStateID)</code> function to be able to tell the ITP that it can safely invalidate the proof state ID, and future uses of the <code>proof_data</code> or <code>apply</code> functions on that ID will fail. (It could reclaim the ID itself, but this is probably not necessary, and not reclaiming the ID means additional protection if the ML driver is sloppy.)</li>\n</ul>",
        "id": 240134935,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621908254
    },
    {
        "content": "<p>Note that with this version, the ITP doesn't have to blindly trust any proof states, because it is responsible for maintaining the proof data map and can ensure that only validly produced states have IDs associated to them.</p>",
        "id": 240135045,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621908371
    },
    {
        "content": "<p>As for treating the proof state as stateless (or not), as I emphasis in that document, there are two states to think about:</p>\n<ul>\n<li>The environment</li>\n<li>The immediate proof state</li>\n</ul>",
        "id": 240173379,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621942311
    },
    {
        "content": "<p>The latter will depend on the former.  In HOList, one can't use a definition in a proof state or apply a theorem (inside <code>SIMP_TAC</code> say) to a proof state unless it is already loaded in the environment.  In Lean, one can't use notation, definitions, theorems, tactics unless they are in the environment.  (And the behavior of some tactics like <code>simp</code> depending on the environment.)</p>",
        "id": 240173404,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621942323
    },
    {
        "content": "<p>Serializing the environment and reloading it from scratch every time is likely not feasible for most any prover.  However, there may be ways to preload the environment and identify the parts needed for each theorem.  In HOList they load all theorems of the environment, but another agent (not the ITP agent) is careful to not use theorems from the future to prove a theorem.  This is more doable in LFC style theorem proving such as the HOL family of provers.  In Lean, we have a hack where we can load the environment associated with a theorem name, but IIRC it has an initialization cost so you don't want to do it on every proof step.</p>",
        "id": 240173445,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621942330
    },
    {
        "content": "<p>Now, as for serializing the intermediate proof state, this depends a lot on the system.  In HOL-Light, they have observed that modulo the environment, a proof state is just a list of sequents <code>[HYP1, HYP2, ...] |- GOAL</code> and therefore easy to serialize and load.  Also, each sequent is independent of the others (unlike Lean) so you just load one sequent at a time.  In Lean it is more complicated.  Technically the proof state (the tactic state) is a complicated beast containing the environment (already discussed), sequents, extra metavariables, user settings, and a few other things.  We worked out that one can probably get by with just serializing the list of sequents.  There are a few caveats: This is a not small amount of information to serialize and pass around.  One has to replace all the metavariables with fresh metavariables when loading which is some additional work.  Any macros can't be serialized, so not all proof states can be serialized.  A few tactics change the environment.  We don't serialize the environment so those changes are lost.  We don't (and likely can't) deserialize the intermediate term proof generated so far.  This also means we lose strong guarantees of correctness.  We are carefully checking each step, but information might get corrupted subtly in each serialization/deserialization of the proof state.</p>",
        "id": 240173457,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621942337
    },
    {
        "content": "<p>What <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> proposes is easier for most ITPs including Lean and is likely a more practical model for other ITPs.  I believe it is what Stan is proposing for a Lean server.  Still <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>, one still has to deal with the environment unless certain environment check points can also be turned into an id as well which can be quickly loaded.  (As I said, in Lean, it can be loaded by declaration name, but I don't know how quickly.)</p>",
        "id": 240173465,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621942342
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"349892\">@Aidan Swope</span> Do you think you could make such a system using proof ids work and still get the parallelization you need/want?</p>",
        "id": 240173470,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621942344
    },
    {
        "content": "<p>As for data structure, if the information is coming straight from the theorem prover, like a goal state, it is fairly easy to give a fully elaborated s-expression, JSON, or protobuf as well as pretty printed version.  (Of course, the more information provided, the larger the data being passed will be, unless there are settings to turn some things on or off.  And with Lean at least, sometimes these terms can be huge.  Also, it isn't always obvious how to match the pretty printed expression to its raw form, e.g. <code>[0, 1, 2]</code> verse <code>@list.cons.{0} nat (@has_zero.zero.{0} nat nat.has_zero) (@list.cons.{0} nat (@has_one.one.{0} nat nat.has_one) (@list.cons.{0} nat (@bit0.{0} nat nat.has_add (@has_one.one.{0} nat nat.has_one))</code> so I don't know how useful this would be to a tokenizer.)   </p>\n<p>A more difficult thing to do is parse the training data when that data is coming from human written code, especially Lean tactics which each come with their own parsing syntax.  For example, we could theoretically parse lean tactics down to their components, and we have the code to do it, but they parse in a funny way which looks very different from how they are written.  The tactic <code>rw [nat.add_comm, ←add_assoc]</code> looks a lot different when fully parsed:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">tactic.interactive.rw</span>\n  <span class=\"o\">{</span><span class=\"n\">rules</span> <span class=\"o\">:=</span> <span class=\"o\">[{</span><span class=\"n\">pos</span> <span class=\"o\">:=</span> <span class=\"o\">{</span><span class=\"n\">line</span> <span class=\"o\">:=</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"n\">column</span> <span class=\"o\">:=</span> <span class=\"mi\">6</span><span class=\"o\">},</span> <span class=\"n\">symm</span> <span class=\"o\">:=</span> <span class=\"n\">ff</span><span class=\"o\">,</span> <span class=\"n\">rule</span> <span class=\"o\">:=</span> <span class=\"bp\">``</span><span class=\"o\">(</span><span class=\"n\">nat.add_comm</span><span class=\"o\">)},</span>\n                     <span class=\"o\">{</span><span class=\"n\">pos</span> <span class=\"o\">:=</span> <span class=\"o\">{</span><span class=\"n\">line</span> <span class=\"o\">:=</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"n\">column</span> <span class=\"o\">:=</span> <span class=\"mi\">20</span><span class=\"o\">},</span> <span class=\"n\">symm</span> <span class=\"o\">:=</span> <span class=\"n\">tt</span><span class=\"o\">,</span> <span class=\"n\">rule</span> <span class=\"o\">:=</span> <span class=\"bp\">``</span><span class=\"o\">(</span><span class=\"n\">add_assoc</span><span class=\"o\">)}],</span>\n   <span class=\"n\">end_pos</span> <span class=\"o\">:=</span> <span class=\"n\">some</span> <span class=\"o\">{</span><span class=\"n\">line</span> <span class=\"o\">:=</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"n\">column</span> <span class=\"o\">:=</span> <span class=\"mi\">30</span><span class=\"o\">}}</span>\n  <span class=\"o\">(</span><span class=\"n\">interactive.loc.ns</span> <span class=\"o\">[</span><span class=\"n\">none</span><span class=\"o\">])</span>\n</code></pre></div>",
        "id": 240175888,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621943875
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/240173465\">said</a>:</p>\n<blockquote>\n<p>What <span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> proposes is easier for most ITPs including Lean and is likely a more practical model for other ITPs.  I believe it is what Stan is proposing for a Lean server.  Still <span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span>, one still has to deal with the environment unless certain environment check points can also be turned into an id as well which can be quickly loaded.  (As I said, in Lean, it can be loaded by declaration name, but I don't know how quickly.)</p>\n</blockquote>\n<p>I didn't mention the environment in that protocol, and I was envisioning it to be entirely proof-local. I am less sure about how to perform environment changes, and they are much less likely to be parallelizable short of running multiple copies of the ITP, although some ITPs (like lean) might be able to handle multiple environments simultaneously. Here's a sketch:</p>\n<ul>\n<li><code>load_file(file: String)</code>: Initialize the environment to the end of a given source file</li>\n<li><code>load_file_upto(file: String, thm: String)</code>: Initialize the environment to just before a given theorem in a given source file</li>\n<li><code>add_theorem(name: String, id: ProofStateId)</code>: Assuming <code>id</code> is a finished proof state, commit it to the environment</li>\n<li><code>add_sorry_theorem(name: String, sig: ThmSignature)</code>: Commit a theorem with the given signature to the environment without a proof</li>\n<li><code>new_theorem(sig: ThmSignature) -&gt; ProofStateId</code>: start a new proof in the current environment, receiving the initial proof state</li>\n</ul>\n<p>All of these commands invalidate and clear the proof state map, and there are no IDs for navigation here because the ITP maintains the environment statefully. The only way to jump to a random other location is to use <code>load_file_upto</code> which might be expensive since it runs all dependent files (although the ITP might be able to share work here with previous calls). There might also be additional <code>add_</code> functions for other kinds of environment items.</p>",
        "id": 240215683,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621960721
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/240173470\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"349892\">Aidan Swope</span> Do you think you could make such a system using proof ids work and still get the parallelization you need/want?</p>\n</blockquote>\n<p>I could see a parallel ID-based system working with some strategy to keep multiple environment workers synchronized. For example, the ML model might try several steps in parallel with different workers, and then re-run any successful steps on all workers in the pool to keep the state updated. This likely captures a lot of the benefit of parallelism without needing to serialize proof state and reload the environment to a point at every step.</p>",
        "id": 240237171,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621970647
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/240175888\">said</a>:</p>\n<blockquote>\n<p>As for data structure, if the information is coming straight from the theorem prover, like a goal state, it is fairly easy to give a fully elaborated s-expression, JSON, or protobuf as well as pretty printed version.  (Of course, the more information provided, the larger the data being passed will be, unless there are settings to turn some things on or off.  And with Lean at least, sometimes these terms can be huge.  Also, it isn't always obvious how to match the pretty printed expression to its raw form, e.g. <code>[0, 1, 2]</code> verse <code>@list.cons.{0} nat (@has_zero.zero.{0} nat nat.has_zero) (@list.cons.{0} nat (@has_one.one.{0} nat nat.has_one) (@list.cons.{0} nat (@bit0.{0} nat nat.has_add (@has_one.one.{0} nat nat.has_one))</code> so I don't know how useful this would be to a tokenizer.)   </p>\n<p>A more difficult thing to do is parse the training data when that data is coming from human written code, especially Lean tactics which each come with their own parsing syntax.  For example, we could theoretically parse lean tactics down to their components, and we have the code to do it, but they parse in a funny way which looks very different from how they are written.  The tactic <code>rw [nat.add_comm, ←add_assoc]</code> looks a lot different when fully parsed:</p>\n<p><div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">tactic.interactive.rw</span>\n  <span class=\"o\">{</span><span class=\"n\">rules</span> <span class=\"o\">:=</span> <span class=\"o\">[{</span><span class=\"n\">pos</span> <span class=\"o\">:=</span> <span class=\"o\">{</span><span class=\"n\">line</span> <span class=\"o\">:=</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"n\">column</span> <span class=\"o\">:=</span> <span class=\"mi\">6</span><span class=\"o\">},</span> <span class=\"n\">symm</span> <span class=\"o\">:=</span> <span class=\"n\">ff</span><span class=\"o\">,</span> <span class=\"n\">rule</span> <span class=\"o\">:=</span> <span class=\"bp\">``</span><span class=\"o\">(</span><span class=\"n\">nat.add_comm</span><span class=\"o\">)},</span>\n                     <span class=\"o\">{</span><span class=\"n\">pos</span> <span class=\"o\">:=</span> <span class=\"o\">{</span><span class=\"n\">line</span> <span class=\"o\">:=</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"n\">column</span> <span class=\"o\">:=</span> <span class=\"mi\">20</span><span class=\"o\">},</span> <span class=\"n\">symm</span> <span class=\"o\">:=</span> <span class=\"n\">tt</span><span class=\"o\">,</span> <span class=\"n\">rule</span> <span class=\"o\">:=</span> <span class=\"bp\">``</span><span class=\"o\">(</span><span class=\"n\">add_assoc</span><span class=\"o\">)}],</span>\n   <span class=\"n\">end_pos</span> <span class=\"o\">:=</span> <span class=\"n\">some</span> <span class=\"o\">{</span><span class=\"n\">line</span> <span class=\"o\">:=</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"n\">column</span> <span class=\"o\">:=</span> <span class=\"mi\">30</span><span class=\"o\">}}</span>\n  <span class=\"o\">(</span><span class=\"n\">interactive.loc.ns</span> <span class=\"o\">[</span><span class=\"n\">none</span><span class=\"o\">])</span>\n</code></pre></div><br>\n</p>\n</blockquote>\n<p>It'd require more effort from the interface developer, but the nicest solution for the ML researcher would probably be a parameter (either set at the start or per-request) determining whether to return fully-elaborated responses or pretty-printed ones.</p>",
        "id": 240237647,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621970880
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"349892\">@Aidan Swope</span> How important is parallelism across multiple proof states for different theorems / different environments as opposed to multiple proof states for subgoals in the tree search for a single theorem?</p>",
        "id": 240237768,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621970923
    },
    {
        "content": "<p>I personally expect the IMO challenge (and similarly hard problems) will require massive parallelism on a single problem. As far as simpler benchmarks, within-problem parallelism may be just a nice-to-have. But, given that the tree search is likely very fast and the ML model is very parallel, it would seem to be a shame to introduce a slow, CPU-bound, sequential bottleneck on each problem.</p>",
        "id": 240238191,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621971119
    },
    {
        "content": "<p>Do you think that the workaround of having multiple ITP instances running is viable? It would probably require a lot more memory</p>",
        "id": 240238411,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621971222
    },
    {
        "content": "<p>The interface I gave is inherently serial wrt environment changes, but if you added <code>EnvId</code> similar to <code>ProofStateId</code> for environment changes the ITP could at least in principle support parallel environment access, and then a serial ITP would just have an expensive switching process if you use the wrong environment ID</p>",
        "id": 240238863,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621971420
    },
    {
        "content": "<p>Yes, it's probably viable if synchronizing state across multiple ITP instances isn't too hard. I don't think memory will be too much of a bottleneck if a CPU server farm is available. How important are environment changes within a single problem? I was assuming the environment was mostly fixed during a given proof.</p>",
        "id": 240239074,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621971530
    },
    {
        "content": "<p>As I'm conceptualizing it, \"within a proof\" is synonymous with \"fixed environment / theorem statement\", although as Jason points out this isn't quite true for lean, which is capable of running tactics that add definitions to the environment. (Note that since architecturally a lean proof can't actually change the environment, what really happens is that the modifications to the environment are discarded when the proof is complete, and any definitions that are created are inlined during proof post-processing.)</p>",
        "id": 240239671,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621971793
    },
    {
        "content": "<p>In that case (assuming it's safe to ignore environment-modifying tactics) an <code>EnvId</code> likely isn't necessary and the ITP can just load the environment once at the start of a proof.</p>",
        "id": 240240053,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621971940
    },
    {
        "content": "<p>In lean it is pretty safe to ignore environment changing tactics; most tactics users use directly don't do this (although <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> might have some counterexamples)</p>",
        "id": 240240362,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621972073
    },
    {
        "content": "<p>Here's an example of an environment changing tactic (actually a term constructor) that comes up:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">example</span> <span class=\"o\">:</span> <span class=\"n\">true</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"n\">try</span> <span class=\"o\">{</span> <span class=\"k\">let</span> <span class=\"o\">:</span> <span class=\"n\">ℕ</span> <span class=\"bp\">×</span> <span class=\"n\">ℕ</span> <span class=\"bp\">→</span> <span class=\"n\">ℕ</span> <span class=\"o\">:=</span> <span class=\"bp\">λ</span> <span class=\"o\">⟨</span><span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"n\">y</span><span class=\"o\">⟩,</span> <span class=\"mi\">0</span><span class=\"o\">,</span> <span class=\"n\">revert</span> <span class=\"n\">this</span><span class=\"o\">,</span> <span class=\"n\">done</span> <span class=\"o\">},</span>\n  <span class=\"n\">try</span> <span class=\"o\">{</span> <span class=\"k\">let</span> <span class=\"o\">:</span> <span class=\"n\">ℕ</span> <span class=\"bp\">×</span> <span class=\"n\">ℕ</span> <span class=\"bp\">→</span> <span class=\"n\">ℕ</span> <span class=\"o\">:=</span> <span class=\"bp\">λ</span> <span class=\"o\">⟨</span><span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"n\">y</span><span class=\"o\">⟩,</span> <span class=\"mi\">0</span><span class=\"o\">,</span> <span class=\"n\">revert</span> <span class=\"n\">this</span><span class=\"o\">,</span> <span class=\"n\">done</span> <span class=\"o\">},</span>\n  <span class=\"n\">trivial</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>The <code>λ ⟨x, y⟩, _</code>syntax, as well as any use of <code>match</code>, will create an auxiliary definition so it can use the equation compiler, which only works on top level defs. If you look at the intermediate states you will notice that the first one creates <code>_example._match_1</code> and the second one creates <code>_example._match_2</code>, even though the state was rewound after the first <code>let</code>. So this can create some nondeterminism in the names that tactics produce but at least it doesn't spuriously fail after creating <code>_example._match_1</code> twice.</p>",
        "id": 240241863,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621972735
    },
    {
        "content": "<p>Since we're talking about an interface for ML agents to communicate with Lean: what is the current best way of doing this? Is it <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> 's <a href=\"https://github.com/jesse-michael-han/lean-tpe-public\">lean-tpe</a>?</p>",
        "id": 240242660,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621973061
    },
    {
        "content": "<p>Assuming that this is the interface that GPT-f uses (as it appears to be), I would say that's probably the best option that currently exists. I don't think lean C++ has an IPC protocol</p>",
        "id": 240243058,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1621973260
    },
    {
        "content": "<p>currently <code>lean-gptf</code> and <code>lean-tpe</code> assume the proof search is driven from Lean; <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> is currently adapting the code to create a REPL-like interface which maintains a similar protocol to the one <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/240134935\">described</a> by Mario</p>",
        "id": 240249138,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1621976295
    },
    {
        "content": "<p>Also, <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> is cleaning up the dataset generation tools we used for lean-step to make them more accessible to other researchers.  If you are interested, we can ping you when these are ready.</p>",
        "id": 240249695,
        "sender_full_name": "Jason Rute",
        "timestamp": 1621976541
    },
    {
        "content": "<p>Please do, thank you!</p>",
        "id": 240254820,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1621979543
    },
    {
        "content": "<p><span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span>I plan to release the REPL this week. It’s still a prototype but is already usable.</p>",
        "id": 240281428,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1622007261
    },
    {
        "content": "<p>We've just open-sourced our new REPL (based on <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span>'s <a href=\"https://github.com/jesse-michael-han/lean-gptf\">lean-gptf</a>) for Lean 3 loosely following <span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>'s <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/HOList.20or.20Lean.3F/near/240134935\">proposed interface</a>. It exposes a JSON interface easy to integrate from your language of choice:</p>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>$ lean --run src/repl.lean\n</code></pre></div>\n<div class=\"codehilite\" data-code-language=\"JSON\"><pre><span></span><code><span class=\"p\">[</span><span class=\"s2\">\"init_search\"</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s2\">\"int.prime.dvd_mul\"</span><span class=\"p\">,</span> <span class=\"s2\">\"\"</span><span class=\"p\">]]</span>\n<span class=\"p\">{</span><span class=\"nt\">\"error\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"search_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state\"</span><span class=\"p\">:</span><span class=\"s2\">\"⊢ ∀ {m n : ℤ} {p : ℕ}, nat.prime p → ↑p ∣ m * n → p ∣ m.nat_abs ∨ p ∣ n.nat_abs\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"0\"</span><span class=\"p\">}</span>\n\n<span class=\"p\">[</span><span class=\"s2\">\"run_tac\"</span><span class=\"p\">,[</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"s2\">\"intros\"</span><span class=\"p\">]]</span>\n<span class=\"p\">{</span><span class=\"nt\">\"error\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"search_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state\"</span><span class=\"p\">:</span><span class=\"s2\">\"m n : ℤ,\\tp : ℕ,\\thp : nat.prime p,\\th : ↑p ∣ m * n\\t⊢ p ∣ m.nat_abs ∨ p ∣ n.nat_abs\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"1\"</span><span class=\"p\">}</span>\n\n<span class=\"p\">[</span><span class=\"s2\">\"run_tac\"</span><span class=\"p\">,[</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"s2\">\"apply (nat.prime.dvd_mul hp).mp\"</span><span class=\"p\">]]</span>\n<span class=\"p\">{</span><span class=\"nt\">\"error\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"search_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state\"</span><span class=\"p\">:</span><span class=\"s2\">\"m n : ℤ,\\tp : ℕ,\\thp : nat.prime p,\\th : ↑p ∣ m * n\\t⊢ p ∣ m.nat_abs * n.nat_abs\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"2\"</span><span class=\"p\">}</span>\n\n<span class=\"p\">[</span><span class=\"s2\">\"run_tac\"</span><span class=\"p\">,[</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"s2\">\"2\"</span><span class=\"p\">,</span><span class=\"s2\">\"rw ← int.nat_abs_mul\"</span><span class=\"p\">]]</span>\n<span class=\"p\">{</span><span class=\"nt\">\"error\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"search_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state\"</span><span class=\"p\">:</span><span class=\"s2\">\"m n : ℤ,\\tp : ℕ,\\thp : nat.prime p,\\th : ↑p ∣ m * n\\t⊢ p ∣ (m * n).nat_abs\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"3\"</span><span class=\"p\">}</span>\n\n<span class=\"p\">[</span><span class=\"s2\">\"run_tac\"</span><span class=\"p\">,[</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"s2\">\"simp\"</span><span class=\"p\">]]</span>\n<span class=\"p\">{</span><span class=\"nt\">\"error\"</span><span class=\"p\">:</span><span class=\"s2\">\"run_tac_failed: pos=(some ⟨1, 2⟩) msg=simplify tactic failed to simplify\"</span><span class=\"p\">,</span><span class=\"nt\">\"search_id\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state_id\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">}</span>\n\n<span class=\"p\">[</span><span class=\"s2\">\"run_tac\"</span><span class=\"p\">,[</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"s2\">\"exact int.coe_nat_dvd_left.mp h\"</span><span class=\"p\">]]</span>\n<span class=\"p\">{</span><span class=\"nt\">\"error\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nt\">\"search_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"0\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state\"</span><span class=\"p\">:</span><span class=\"s2\">\"no goals\"</span><span class=\"p\">,</span><span class=\"nt\">\"tactic_state_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"4\"</span><span class=\"p\">}</span>\n</code></pre></div>\n<p>It smoothly handles and return tactic application errors and allows multiplexing of proof searches on a single REPL (interleaving of <code>run_tac</code> and <code>init_search</code> commands). It is still subject to a few crashes, but empirically they happen less than 0.01% of the time, which is within bound for a production use at scale. We've successfully run pools of 128+ REPLs each multiplexing 64+ searches in parallel.</p>\n<p>Hopefully, this will help make Lean 3 (and surely 4 very soon) a prime choice for machine learning research.</p>\n<p>The project is hosted here: <a href=\"https://github.com/openai/lean-gym\">https://github.com/openai/lean-gym</a> -- Contributions and feedback welcome!</p>",
        "id": 240914292,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1622538673
    },
    {
        "content": "<p>(<span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> <span class=\"user-mention\" data-user-id=\"230999\">@Daniel Selsam</span> hope it'll be informative and useful for the development of Lean 4!)</p>",
        "id": 240914593,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1622538870
    },
    {
        "content": "<p>(Also hope it'll provide a clear answer to this topic: Lean :p)</p>",
        "id": 240914773,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1622538964
    },
    {
        "content": "<p>Hello. <br>\nW.r.t the transformer approach, how would you handle custom notation and syntax frequenctly used in proof developments?</p>",
        "id": 246292147,
        "sender_full_name": "Eric Bond",
        "timestamp": 1626478884
    },
    {
        "content": "<p>Also I see alot of discussion about premise selection.. but what about proof term selection?</p>",
        "id": 246292227,
        "sender_full_name": "Eric Bond",
        "timestamp": 1626478955
    },
    {
        "content": "<p>Many proofs require induction targeting a specific proof term, unfolding a proof term, or performing a partial substitution of a proof term.</p>",
        "id": 246292301,
        "sender_full_name": "Eric Bond",
        "timestamp": 1626479048
    },
    {
        "content": "<p>As far as syntax, with the gptf approach, it (theoretical) handles syntax in the training data, and also can adapt to syntax in the goal, say if that same syntax would be used in the tactic command.</p>",
        "id": 246313854,
        "sender_full_name": "Jason Rute",
        "timestamp": 1626511463
    },
    {
        "content": "<p>It currently doesn’t know about syntax not found in either location, say earlier in a new file not used for training.</p>",
        "id": 246313869,
        "sender_full_name": "Jason Rute",
        "timestamp": 1626511522
    },
    {
        "content": "<p>Also gptf can generate terms as needed for exact tactics or existentials.</p>",
        "id": 246313943,
        "sender_full_name": "Jason Rute",
        "timestamp": 1626511666
    },
    {
        "content": "<p>None of this works perfectly, but it isn’t a hard barrier like in some systems.</p>",
        "id": 246313984,
        "sender_full_name": "Jason Rute",
        "timestamp": 1626511703
    }
]