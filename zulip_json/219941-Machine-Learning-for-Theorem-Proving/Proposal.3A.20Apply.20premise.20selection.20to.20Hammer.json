[
    {
        "content": "<p>Here is a project which I think is very doable for an enterprising person.  Recently, <span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span>  <a href=\"http://www.andrew.cmu.edu/user/avigad/meetings/fomm2020/slides/fomm_ebner.pdf\" target=\"_blank\" title=\"http://www.andrew.cmu.edu/user/avigad/meetings/fomm2020/slides/fomm_ebner.pdf\">incorporated an SMT-based \"hammer\" into a branch of Lean3</a>.  Right now, it uses TF-IDF (with cosine similarity) to select premises/lemmas.  However, Gabriel showed (on slide 28) that if one used better premise selection one could up-to double the success rate.  I think this is a ripe opportunity to apply the recent advancements in machine learning premise selection for theorem proving.</p>",
        "id": 185556572,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578964966
    },
    {
        "content": "<p>For supervised learning, Gabriel has already found a way to extract both the lemmas used in the original lean proofs as well as those used by the hammer.  This would make a great training set.  Then one could also apply reinforcement learning by using a trained premise selection model, extracting which lemmas have and have not been used, and then using this to train a new model.  Repeat until convergence.  Since the TF-IDF premise selector is already in C++, I imagine one could incorporate a trained TensorFlow model in there.  (And still do the training in Python by using his code to extract the lemmas used.)</p>",
        "id": 185556680,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578965151
    },
    {
        "content": "<p>See here for a discussion on some of this.  <a href=\"#narrow/stream/113488-general/topic/Hammer.20talk\" title=\"#narrow/stream/113488-general/topic/Hammer.20talk\">https://leanprover.zulipchat.com/#narrow/stream/113488-general/topic/Hammer.20talk</a></p>",
        "id": 185556780,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578965257
    },
    {
        "content": "<p>Of course a learned model of premise selection would have trouble with never-seen definitions (unlike TF-IDF).  Google's HOList however showed that sometimes this is ok.  They trained a model on the core HOL Light library, but it did respectably on the Flyspeck library even though there were a number of new definitions.  Moreover, there is <a href=\"https://arxiv.org/pdf/1911.02065.pdf\" target=\"_blank\" title=\"https://arxiv.org/pdf/1911.02065.pdf\">recent research by IBM</a> about this exact problem.</p>",
        "id": 185556961,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578965468
    },
    {
        "content": "<p>Here is <a href=\"https://arxiv.org/pdf/1911.06904.pdf\" target=\"_blank\" title=\"https://arxiv.org/pdf/1911.06904.pdf\">one of the best results in the area of premise selection</a> (also by IBM) at least according to the two standard machine learning benchmarks (HOLStep and the Mizar dataset).  While this might be complicated for a first pass, I could imagine starting with something simpler.  There are a lot of references in that paper (including the original HOLStep paper).  Also, I think Isabelle's Sledgehammer uses naive Bayes for another example.</p>",
        "id": 185557161,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578965739
    },
    {
        "content": "<p>Of course, this might be premature as a project since Gabriel is still improving the hammer (or maybe this was already something he had in mind to add to it).  Either way, I could imagine using the dependency graph of Lean premises as another standard premise selection dataset (this one coming from dependent type theory).</p>",
        "id": 185557477,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578966122
    },
    {
        "content": "<p>Note that our DeepHOL-zero system for HOList <a href=\"https://arxiv.org/abs/1905.10501\" target=\"_blank\" title=\"https://arxiv.org/abs/1905.10501\">https://arxiv.org/abs/1905.10501</a> demonstrates how TF-IDF style similarity metric  can be combined with standard RL to bootstrap premise-selection without imitation learning. DeepHOL-zero achieves results that are almost at the level of systems that were trained with a combination of RL and imitation learning.</p>",
        "id": 185560532,
        "sender_full_name": "Christian Szegedy",
        "timestamp": 1578970167
    },
    {
        "content": "<p>Yes.  I was going to mention your work too.  I forgot you used TF-IDF (or something similar) as a starting point.</p>",
        "id": 185560820,
        "sender_full_name": "Jason Rute",
        "timestamp": 1578970603
    },
    {
        "content": "<p>TF-IDF is not spelled out explicitly, but we use a modified version of it. We have not reported the comparisons with the standard TF-IDF, but our variant was superior in our experiments.</p>",
        "id": 185562598,
        "sender_full_name": "Christian Szegedy",
        "timestamp": 1578973277
    }
]