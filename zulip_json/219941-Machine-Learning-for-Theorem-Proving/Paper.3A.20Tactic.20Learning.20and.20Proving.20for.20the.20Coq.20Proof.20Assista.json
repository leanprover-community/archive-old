[
    {
        "content": "<p>A paper recently came out on applying machine learning to Coq: <a href=\"https://arxiv.org/abs/2003.09140\" title=\"https://arxiv.org/abs/2003.09140\">Tactic Learning and Proving for the Coq Proof Assistant</a> by Lasse Blaauwbroek, Josef Urban, and Herman Geuvers.  <a href=\"https://zenodo.org/record/3693760#.XoXfhi-ZPOQ\" title=\"https://zenodo.org/record/3693760#.XoXfhi-ZPOQ\">Artifacts of Tactic Learning and Proving for the Coq Proof Assistant</a> also includes datasets and Coq configurations for the project (but I don’t think it contains usable code to recreate one’s own Coq AI or to use theirs).</p>",
        "id": 192752855,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882486
    },
    {
        "content": "<p>It has been mentioned on the <a href=\"https://coq.discourse.group/t/machine-learning-and-hammers-for-coq/303/17\" title=\"https://coq.discourse.group/t/machine-learning-and-hammers-for-coq/303/17\">Coq discourse</a>, but there is no discussion so far, so I’d like to start a discussion here.</p>",
        "id": 192752858,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882494
    },
    {
        "content": "<p>In short, the paper is <a href=\"https://arxiv.org/abs/1804.00596\" title=\"https://arxiv.org/abs/1804.00596\">TacticToe</a>, but for Coq.  (TacticToe was one of the first competitive applications of machine learning for ITP and was originally for HOL4.  See <a href=\"#narrow/stream/113488-general/topic/AI.20and.20theorem.20proving/near/166056603\" title=\"#narrow/stream/113488-general/topic/AI.20and.20theorem.20proving/near/166056603\">my notes on it here</a>.)  This project follows a similar methodology to TacticToe, and since they don’t give it a name, I’m going to call it TacticToe for Coq.  Let me mention some of the highlights of the paper.</p>",
        "id": 192752864,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882502
    },
    {
        "content": "<p>TacticToe for Coq is actually the third machine learning based theorem prover for Coq.  The other two projects are <a href=\"https://arxiv.org/abs/1905.09381\" title=\"https://arxiv.org/abs/1905.09381\">CoqGym/ASTactic</a> and <a href=\"https://arxiv.org/abs/1907.07794\" title=\"https://arxiv.org/abs/1907.07794\">Proverbot9001</a>.  (See some of my notes on the two projects <a href=\"#narrow/stream/113488-general/topic/AI.20and.20theorem.20proving/near/171889226\" title=\"#narrow/stream/113488-general/topic/AI.20and.20theorem.20proving/near/171889226\">here</a>.)  Annoyingly, they don’t mention Proverbot9001 even though they cite earlier papers and Proverbot9001 seems to be far and away the state of the art in machine learning applied to Coq.  I’ll mention how this project compares to the others in a bit, but as usual, the lack of standard benchmarks makes such a comparison difficult.</p>",
        "id": 192752868,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882510
    },
    {
        "content": "<p>They make a big deal out of the fact that TacticToe for Coq is built using OCaml (the language Coq is implemented in).  I agree.  While Python is better for rapid prototyping and getting ML researchers involved, OCaml makes it more likely that this tool will eventually be available to the working proof engineer.  So even if this project isn’t the state of the art, it may be the most likely tool that Coq engineers will actually use.  However, I don’t think it is publicly available yet.</p>",
        "id": 192752875,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882519
    },
    {
        "content": "<p>They implement their own proof recording.  I think it is completely separate to the proof recording used for the other two Coq projects.  The techniques are similar to <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/ML.20for.20Lean.3A.20How.20to.20do.20it.3F\" title=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/ML.20for.20Lean.3A.20How.20to.20do.20it.3F\">what I’m playing around with in Lean</a>.  They record all goal-state-tactic pairs and use those as a dataset to train a supervised algorithm.</p>",
        "id": 192752879,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882530
    },
    {
        "content": "<p>Unlike other Coq provers, this project doesn’t use neural networks.  Instead they predict tactics by using k-nearest neighbors (k-NN) similar to the original TacticToe.  If you are not familiar, the idea of k-NN that they take all goal-tactic pairs in the training set and encode the goals as vectors.  Then for any new goal one finds the goal (in the training set) closest (in some metric) to the new goal.  From that they can figure out which tactic to apply.  Some more technical details about this part:</p>\n<ul>\n<li>Since pure k-NN requires searching through all the training data every time, they use a faster approximate version called LSHF.</li>\n<li>They use a variation of TF-IDF for formulas to encode the vectors.  The distance metric is similar to cosine distance, but a bit different.</li>\n<li>They don’t mention tactic argument selection (e.g. premise selection), but if it is similar to TacticToe for HOL4, then I think they find premises which are closest to the current goal in some metric (probably similar to the metric used for tactic selection).</li>\n<li>They also don’t mention the list of tactics they use, but I’m sure it is pared down from the full list.</li>\n</ul>",
        "id": 192752882,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882539
    },
    {
        "content": "<p>They also use a proof search where the most likely tactics are explored more than less likely tactics.  It seems similar to beam search or some tree searches which use a notion of “fuel”.</p>",
        "id": 192752888,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882547
    },
    {
        "content": "<p>Ok, now onto results!  I’ve learned by now that pure percentages of solved theorems don’t mean a lot in isolation and it is difficult to compare results across theorem provers—and even within the same prover.  However, TacticToe for Coq solves ~39% of the theorems in the Coq standard library.  (I would have liked more discussion about test/train split, but I’ll give them the benefit of the doubt that they did this sufficiently.)  On the other hand, CoqHammer solves about ~40-42% of theorems in the standard library (when you combine Vampire, E, and Z3).</p>",
        "id": 192752891,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882555
    },
    {
        "content": "<p>First, let’s compare this to TacticToe for HOL4 which solves 66.4% of the theorems in the HOL4 standard library (twice as much as the E prover on HOL4).  I think this shows how similar ML strategies can have very different numbers for different theorem proving libraries.  I don’t know if this means that HOL-based provers are more amenable to automation than dependent type theorem logics, or if the HOL4 tactics are more powerful, or if just the libraries are different.</p>",
        "id": 192752928,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882563
    },
    {
        "content": "<p>Second, let’s compare it to CoqGym/ASTactic and Proverbot9001.  One problem is that all three projects use different test and training sets.  TacticToe for Coq uses the Coq standard library.  CoqGym uses the standard library and the packages listed on the Coq Package Index.  Proverbot9001 trains on CompCert and test on different projects (they were also kind enough to retrain and retest CoqGym/ASTactic on their dataset).  The saving grace of these three projects is that they all compare their results to CoqHammer.  Assuming that the CoqHammer configurations are similar between the three projects (which honestly they might not be), we can hammer-normalize the solve rates.  Specifically, in this paper, TacticToe for Coq solves 3240 theorems while CoqHammer solves 3534 theorems.  Dividing the two numbers gives a hammer-normalized score of 0.92.  Here are the hammer normalized scores for the three provers:</p>\n<ul>\n<li>ASTactic: 0.69 in CoqGym paper, 0.62 in Proverbot9001 paper</li>\n<li>Proverbot9001: 2.62</li>\n<li>TacticToe for Coq: 0.91</li>\n</ul>",
        "id": 192752931,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882575
    },
    {
        "content": "<p>So it seems like Proverbot9001 is (far and away) better than the others, but a lot of caution is needed (especially since the test set for Proverbot9001 was much smaller).  Really, this highlights the needs for uniform benchmarks, not only within a theorem prover, but across theorem provers as well.  (I would love to see the TacticToe algorithm applied to HOL Light, Isabelle and Lean.)</p>",
        "id": 192752934,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882584
    },
    {
        "content": "<p>Finally, if the authors are reading this (e.g. <span class=\"user-mention\" data-user-id=\"223577\">@Josef Urban</span>), I think I found a small error/typo.  I don’t see where <code>eq_z</code> and <code>eq_x</code> come from in the feature set example on the bottom of page 3. I might not understand what a 2-shingle is, but if I do, <code>x</code> and <code>z</code> are not next to <code>=</code> in the syntax tree of the example formula.</p>",
        "id": 192752943,
        "sender_full_name": "Jason Rute",
        "timestamp": 1585882610
    },
    {
        "content": "<p>Thanks for the great summary!</p>",
        "id": 192886288,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1585969523
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 192991265,
        "sender_full_name": "Rongmin Lu",
        "timestamp": 1586142170
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> thanks for the summary. There is a detail that you didn't mention (as far as I could tell) that in the ASTactic/CoqGym paper they actually generate tactics by doing a tree expansion on the tactic grammar. This is not how Proverbot9001 does it (as I understand from your summary Proverbot9001 is mostly a tactic selection + premise selection paper). </p>\n<p>Do people think this comment is an accurate representation of the main differences in proving methodology between Proverbot9001 &amp; ASTactic/CoqGym?</p>",
        "id": 196996556,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1589037730
    },
    {
        "content": "<p>As side comment relevant to the tactic generation via tree expansion, I believe ASTactic is the only paper I am aware of that generates tactics this way (instead of tactic selection + premise selection). Is this a correct (with high probability) statement? Can someone shed me some light on why generation of tactics via tree expansion is not a popular method? Do people know what are it's drawbacks?</p>",
        "id": 196996740,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1589037987
    },
    {
        "content": "<p>I have to admit that there is a lot I'm unfamiliar with (or forgot) in the various papers.  (It's on my TODO list to look over these papers again and record more of the fine details.)  In this case, what does it mean to \"do a tree expansion on the tactic grammar\"?  Most of the current algorithms (DeepHOL, TacticToe, ProverBot, and this one) do some sort of tree search (MCTS, beam search, etc).  Is doing a \"tree expansion\" a fundamentally different operation?</p>",
        "id": 197014319,
        "sender_full_name": "Jason Rute",
        "timestamp": 1589059947
    },
    {
        "content": "<p>Actually, I should be careful.  There are two \"tree\"s in these algorithms.  First is the search tree (trying tactics and backtracking if necessary).  In this tree, each node is a goal stack (or something similar).  The second tree is the proof.  This gets a bit tricky.  Let's for the moment assume tactics only apply to one goal at a time (which in Lean at least is not always true).  Then sometimes when you apply a tactic to a goal it produces multiple new goals.  The if this is the right set of goals, it doesn't matter which goal you prove first since you have to prove all of them.  Some algorithms like DeepHOL make the simplifying assumption that you are always working on the first goal in the stack.  Then your proof tree can be written as a flat tactic script since it is just a depth-first search through the proof tree you are building.  Of course, since we don't know we are working on the right set of goals, and may need to backtrack, there could be a significant advantage of working on the goals out of order.  I don't remember which algorithms allow this.  I wonder (and I should really look more at the paper before I speculate too much) if by \"tree expansion\" they mean expanding the tactic tree by choosing which goal to work on instead of working on the first goal each time.  If so, that could have some advantages, but I think it is still a matter of tactic selection and premise selection, right?  Do they just use different terminology?</p>",
        "id": 197015700,
        "sender_full_name": "Jason Rute",
        "timestamp": 1589061331
    },
    {
        "content": "<p>Ok, I'm going to go read the paper again before wildly speculating more...</p>",
        "id": 197015707,
        "sender_full_name": "Jason Rute",
        "timestamp": 1589061356
    },
    {
        "content": "<p>I'm moving this conversation to <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof\" title=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Paper.3A.20Learning.20to.20Prove.20Theorems.20via.20Interacting.20with.20Proof\">Paper: Learning to Prove Theorems via Interacting with Proof</a> so that we have a good place to discuss ASTactic and CoqGym.</p>",
        "id": 197021285,
        "sender_full_name": "Jason Rute",
        "timestamp": 1589068280
    }
]