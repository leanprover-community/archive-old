[
    {
        "content": "<p>Hi Everyone! Today we're releasing <a href=\"https://github.com/openai/miniF2F\">MiniF2F</a>. A cross-system formal to formal benchmark consisting of olympiads, undergraduate and high-school maths exercises. The goal of MiniF2F is to provide a shared benchmark to evaluate deep-learning approaches across formal systems. It currently targets Lean and Metamath, with an eye on Hol Light and Isabelle.</p>\n<p>We see MiniF2F as a stepping stone towards the IMO Grand Challenge, as its statements are typically much easier than IMOs, but generally still out of reach of deep-learning based automated theorem provers. We also hope that it will help foster innovation in that space by allowing direct and fair comparison between the existing methods.</p>\n<p>The benchmark is still very early and there's still a lot of work to do. Contributions are more than welcome (there's an indicative TODO list in the README). The Lean statements are released under the Apache license to make sure they can freely float from and back to mathlib.</p>",
        "id": 238117242,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1620642197
    },
    {
        "content": "<p>Thanks for putting this together Stan and team!  I think overall this will be a good project.  I’ve said for a while that we need standard benchmarks.  I look forward to seeing the various neural, ML, decision procedure, and hammer-based systems put through this test.  Right now there is no benchmark that I’m aware of that can compare DeepHOL to lean GPT-f to Isabelle Hammer, and it is great that we are creating such benchmarks.</p>",
        "id": 238221765,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620685902
    },
    {
        "content": "<p>I could imagine this would be a good dataset to put on Papers with Code.  (I used to keep the Paper With Codes SoTA up-to-date for automated reasoning, but I haven’t touched it in a long time.)  Although, I think it would even be better just to maintain a list on this GitHub repo (maybe with some detailed CSVs in the repo too).  This way one can not only see the results, but also all the caveats that comes with each method.</p>",
        "id": 238221829,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620685932
    },
    {
        "content": "<p>For the name MiniF2F, I assume it is a reference to Daniel Selsam calling the IMO Grand Challenge a Formal to Formal (F2F) variant of the IMO.  That is, a formal statement is given to the agent as input, and the agent produces a formal proof as output.  This project is similar, but contains simpler problems as well, hence “Mini” in the name.</p>",
        "id": 238221847,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620685942
    },
    {
        "content": "<p>You mention HOL-Light and Isabelle.  I wonder if you would be able to recruit some Coq, HOL4, and Mizar folks as well to help with translation to those systems.  Then you can target CoqGym/ASTactic, Tactician, TacticToe, ProverBot9001, CoqHammer, ENIGMA, and rlCoP.</p>",
        "id": 238221858,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620685954
    },
    {
        "content": "<p>As for your initial baseline tests with Lean, I’d love to see you test a variety of Lean tactics including <code>tidy</code>, <code>refl</code>, <code>simp</code>, <code>finish</code>, <code>suggest</code>, <code>library_search</code>, <code>omega</code>, <code>norm_num</code> and more.  I think this would be a good place ask the Lean community what sort of tactics would be make good baselines.</p>",
        "id": 238221884,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620685974
    },
    {
        "content": "<p>I think it would be nice to have a paragraph (or a whole paper or tech report) detailing what sort of problems you included in your dataset and why.  Although, it should be noted to anyone reading here that right now there are only 122 Lean problems, so it is easy to look over manually.</p>",
        "id": 238221912,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620685987
    },
    {
        "content": "<p>Every benchmark is flawed, including this one, and we should be careful not to read too much into a good or bad score on this metric.  (Although, I’m sure that will be impossible in practice.)  Right away, I think it should be noted strongly that good performance on this benchmark isn’t necessarily indicative how useful a given system is to a Lean or Metamath user.  One reason is that, after a quick glance of the theorems, I don’t think they are typical theorems that a Lean user would put into Lean.  Moreover, when comparing across systems it is also just as much a comparison of the library and tools in each system as it is of the training methods used.  A theorem might be easier for Lean GPT-f instead of MetaMath GPT-f not because of the subtle differences in training the two projects, but because of the libraries and automation differences.  The same can be said of comparing either GPT-f project to DeepHOL.</p>",
        "id": 238221987,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620686031
    },
    {
        "content": "<p>Also, for any system designed specifically against this benchmark, I’m afraid that there could be significant issues with with data leakage and overfitting to the test set.  If someone uses a pure machine learning approach where they train on a separate dataset and test on the test theorems, I think the issues will be minimal (at least no worse than current leakage and over fitting issues with the ImageNet and Atari datasets).  (I may however be too optimistic here.  With only about 50 test examples, even casually looking at the test set could heavily influence the design choices of the prover and lead to a better score.)  However, the larger issue is if someone writes a tactic to solve these types of problems, similar to Daniel’s IMO Grand Challenge project.  One has to have a lot of self-restraint not to look too much at the test theorems.</p>",
        "id": 238222109,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620686068
    },
    {
        "content": "<p>When talking about these results we really need to take this benchmark with a grain of salt.  I hop to write up a longer document on the issues and challenges of testing and benchmarks, but the short of it is this:  There are a lot of theorem provers.  Comparing across them is hard.  Also there are a lot of subtly different goals and it is not clear that all theorem provers are optimizing for the same goals.  Apples-to-apples comparisons are likely impossible.  Instead we need better (but not perfect) benchmarks and a lot of humility and self-reflection.</p>",
        "id": 238222200,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620686127
    },
    {
        "content": "<p>Update: Looking back at what I wrote above, it may have come across too negative.  I think this is a great start at a benchmark!  Obviously the more problems it has and the more diversity of problems it has, the better.  I think making datasets and benchmarks is one of the best ways for many people to help with AI research.  Unlike training a large neural network or making an end-to-end AI agent, building datasets and benchmarks is the sort of project that a variety of people can help with.  One could argue that every time you put a theorem into Lean (or any ITP library) that it is contributing to a dataset.  Moreover, we don't even need proofs.  Just theorem statements alone are a great benefit to both training and testing mathematical agents.  One success of making datasets was Lean GPT-f.  It basically came about when Jesse and I were both separately working on Lean related datasets.  When these datasets reached maturity, it was easy to combine them and start collaborating with others.</p>",
        "id": 238294826,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620734608
    },
    {
        "content": "<p>Agreed with all your points. A few quick comments:</p>\n<ul>\n<li>The goal of the benchmark is not to evaluate the usefulness of ATPs for the typical Lean user but rather evaluate the reasoning capabilities of such agents using maths exercise as a metric. Of course we're also evaluating the underlying libraries/formal system/tactics at the same time but I think that's fine. As an example applying GPT-f to Lean and Metamath and eventually other systems with the same methodology and measuring against a benchmark like MiniF2F is a very interesting endeavor IMHO. Usefulness for formalization efforts I think is actually well covered by testing on a temporal split of libraries?</li>\n<li>The benchmark is obviously in a prototype state and we very much hope that people from various labs and formal communities will help grow it into a more finalized state.</li>\n</ul>",
        "id": 238449239,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1620813698
    },
    {
        "content": "<p>In case useful, last week we released a paper to accompany the miniF2F benchmark. It's available here: <a href=\"https://arxiv.org/abs/2109.00110\">https://arxiv.org/abs/2109.00110</a></p>\n<p>MiniF2F now has 488 statements fully translated in Lean and Metamath + WIP on Isabelle.</p>\n<blockquote>\n<p>We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, and Isabelle and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.</p>\n</blockquote>",
        "id": 252979508,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1631445743
    },
    {
        "content": "<p>I'm terrified that <code>tidy</code> is now being used as a baseline. :-)</p>",
        "id": 252980309,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1631446603
    },
    {
        "content": "<p>I find these ML papers very hard to read (nothing to do with your exposition, it's just that it's so far from my area, I don't know even the basic vocabulary in this area). Here's a dumb question. You give a sample theorem on p7:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">data.real.basic</span>\n<span class=\"kn\">import</span> <span class=\"n\">tactic</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">algebra_sqineq_2unitcircatblt1</span>\n  <span class=\"o\">(</span><span class=\"n\">a</span> <span class=\"n\">b</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span>\n  <span class=\"o\">(</span><span class=\"n\">h₀</span> <span class=\"o\">:</span> <span class=\"n\">a</span><span class=\"bp\">^</span><span class=\"mi\">2</span> <span class=\"bp\">+</span> <span class=\"n\">b</span><span class=\"bp\">^</span><span class=\"mi\">2</span> <span class=\"bp\">=</span> <span class=\"mi\">2</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n  <span class=\"n\">a</span> <span class=\"bp\">*</span> <span class=\"n\">b</span> <span class=\"bp\">≤</span> <span class=\"mi\">1</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"n\">nlinarith</span> <span class=\"o\">[</span><span class=\"n\">sq_nonneg</span> <span class=\"n\">a</span><span class=\"o\">,</span><span class=\"n\">sq_nonneg</span> <span class=\"n\">b</span><span class=\"o\">,</span><span class=\"n\">sq_nonneg</span> <span class=\"o\">(</span><span class=\"n\">a</span> <span class=\"bp\">-</span> <span class=\"n\">b</span><span class=\"o\">)]</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>Just to be clear -- does your AI find this one-line proof by itself with absolutely no human input? </p>\n<p>Second (and final) dumb question: more generally, there are 244 theorems formalised in Lean here: <a href=\"https://github.com/openai/miniF2F/blob/v1/lean/src/test.lean\">https://github.com/openai/miniF2F/blob/v1/lean/src/test.lean</a> , some with proofs filled in and some with <code>sorry</code> (I am unclear about when a proof is filled in and when it's not). Using both a GPT-related approach which has not seen solutions to these questions before, and also a <code>tidy</code>-like tactic, and taking as much time as you like, what percentage of these goals can you completely solve with no human input whatsoever? I see 10.5% on p5, 29.2% on p6, and figures nearer 40% on p7, all coming from slightly different methods which I find hard to distinguish between.</p>\n<p>My reason for asking is that my <em>impression</em> of previous results in this area is that sometimes CS researchers sneakily do things which a mathematician would regard as \"cheating\" -- for example they do things like \"well we looked at a human proof and then told the system which tactics to use but not how to use them\" or \"we looked at 100 consecutive theorems in an API and for each theorem we tried to prove it assuming all the previous theorems, thus implicitly but crucially using the human-generated ordering of the theorems\". I'm certainly not claiming that you're doing this! I'm observing that I've seen others doing this in the past, and this is the sort of thing which one would not be able to do in, say, an IMO challenge or a computer attempt at a hard human conjecture for which no route is yet known. Basically I just want to check that there's not any \"small print\" which I'm missing.</p>\n<p>Thanks in advance and sorry for the very basic questions!</p>",
        "id": 252980996,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1631447426
    },
    {
        "content": "<p>PS does the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup><mo>=</mo><mn>2</mn><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext><mi>a</mi><mi>b</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">a^2+b^2=2\\implies ab\\leq 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.897438em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.66844em;vertical-align:-0.024em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">⟹</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83041em;vertical-align:-0.13597em;\"></span><span class=\"mord mathnormal\">ab</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> question show up in some kind of school Olympiad test?</p>",
        "id": 252981199,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1631447681
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/MiniF2F/near/252980309\">said</a>:</p>\n<blockquote>\n<p>I'm terrified that <code>tidy</code> is now being used as a baseline. :-)</p>\n</blockquote>\n<p>It's a shame there's not more category theory on these high school olympiad exams!</p>",
        "id": 252981338,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1631447878
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> the AI find the on-line tactic of <code>algebra_sqineq_2unitcircatblt1</code> indeed with no human input other than training on mathlib. That problem is not part of any olympiad as far as we know but it was one problem we used as a test esp when working with Metamath for quite some time. That's how it landed in miniF2F</p>\n<p>The statements in miniF2F may indeed be accompanied by proofs but it's not a requirement, just a nice to have.</p>\n<ul>\n<li>the 10% on p5 is a reference to the PACT paper. That's the pass rate of the tidy baseline one a test split of statements from mathlib</li>\n<li>29.6% on p6 is the pass rate on miniF2F / Lean of GPT-f pre-trained with PACT. PACT is an automated way to extract low-level artifacts from Lean proofs to train the model on as secondary objective on top of the tactic mode tracing data we train on.</li>\n<li>40% is the pass rate of the same model on the subset of problems that come form the MATH dataset.</li>\n</ul>\n<p>The only bit of \"cheating\" I can think of here is related to the fact that the model was pre-trained on data coming from the web that may include informal proofs of these problems (that dataset does not include any Lean at all though). The model is later finetuned on mathlib data. I strongly believe and we will definitely work on verifying this that the results would remain the same if we were to mine for these and filter them.</p>",
        "id": 253007820,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1631476129
    },
    {
        "content": "<p>Happy to answer more questions on our methodology! To give you a bit more color on the pre-training / presence of informal data, the dataset we used was built up in 2020. As you may have noticed I formalized the AMC A 2021 problems last Friday and our models didn't suprisingly underperform on them in any way despite the guarantee that we never saw these statements or proofs even in their informal form.</p>",
        "id": 253007926,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1631476242
    },
    {
        "content": "<p>Also the goal of the paper is really to present the benchmark. It's common use to provide some baseline numbers as well so that groups working on the benchmark have some numbers to compare to. That's why we built that modified tidy + applied the methodologies we had already published in the past for Metamath and Lean. But destroying these methods is not really the focus of the paper.</p>",
        "id": 253008069,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1631476388
    },
    {
        "content": "<p>(This is the PACT paper: <a href=\"https://arxiv.org/abs/2102.06203\">https://arxiv.org/abs/2102.06203</a>)</p>",
        "id": 253008391,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1631476704
    },
    {
        "content": "<p>Thanks for the further explanations!</p>",
        "id": 253009482,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1631477963
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> (or other coauthors), I'm curious about your experience modifying <code>tidy</code>. Do you think you learnt anything in modifying it that could be contributed back to the mathlib version?</p>\n<p>I ask in part because I know <code>tidy</code> is embarrassingly bad. I started writing it before mathlib existed, and it was only every really \"trained\" (tuned, perhaps) on basic theorems from category theory. We now use it on other problems, but as far as I'm aware no one has every really gone through any list of plausible targets for it and analysed failure modes, outside of that category theory domain. I am quite certain it could be made much better! (And likely much less slow.)</p>\n<p>Having good baselines is important, so your use of a variant of it adds to the motivation for this to happen.</p>",
        "id": 253015863,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1631485194
    },
    {
        "content": "<p>(I realise, and apologise, that this question is not about the real point of your paper. :-)</p>",
        "id": 253015915,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1631485217
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> (cc <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> <span class=\"user-mention\" data-user-id=\"408484\">@Kunhao Zheng</span>) To be very honest we didn't thoroughly study the dynamics here. As mentioned in the paper we simply added a few tactics to tidy's list (namely <code>linarith</code> <code>nlinarith</code> <code>ring_nf</code> and <code>norm_num</code>) that are empirically useful to the kind of problems in miniF2F. The goal was really to get a decent non neural baselines and the result we got with this definitely matched that objective.</p>\n<p>I presume there are many things we could do to improve the state of affair here. Some ideas:</p>\n<ul>\n<li>Some pattern matching to select form the tactic list in a bit more opinionated way?</li>\n<li>Upweight shorter goal states?</li>\n<li>Attempts to pass as argument hypotheses when applicable?</li>\n<li>Whatever sledgehammer does in Isabelle <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span> </li>\n</ul>\n<p>Happy to explore this further. Our immediate plan wrt to non neural baselines are the following:</p>\n<ul>\n<li>Run sledgehammer once the Isabelle effort is complete and compare.</li>\n<li>(aspirational) add TPTP as a supported format (not sure we can translate all problems easily) and run E prover on them.</li>\n</ul>\n<p>In any case, very happy to collaborate and help on that front, I think it would be nice to have a stronger tidy for sure! One simple experiment we could run is to query the model inside the tidy loop to get extra-arguments to tactics and see if we can see emerge patterns that could be easily implemented as non-neural heuristics?</p>",
        "id": 253047201,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1631517288
    },
    {
        "content": "<p>Hi miniF2F authors! <span class=\"user-mention\" data-user-id=\"408484\">@Kunhao Zheng</span> <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> I am trying to evaluate a GPT model (using the <code>lean-gym</code> environment) on the miniF2F benchmark these days, but I am confused about one technical detail: it seems that when attempting the proof of a theorem we would need the decl name and open namespaces of it. For <code>mathlib</code>, these could be obtained from the <code>.names</code> files produced by the proof recording pipeline; but how to obtain these things for <code>miniF2F</code>?</p>",
        "id": 277983844,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1649229578
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"489362\">@Yichen Xu</span>! You indeed need the decl name. We parse it out of the minif2f repo simply matching for the theorem keyword with a rather simple regex <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 278060281,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1649267022
    },
    {
        "content": "<p>THis is what we use</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"k\">from</span> <span class=\"n\">smokey</span> <span class=\"kn\">import</span> <span class=\"n\">Smokey</span>\n<span class=\"k\">from</span> <span class=\"n\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">Optional</span>\n\n<span class=\"s2\">\"\"\"</span>\n<span class=\"s2\">Script for creating benchmarks, allows you to specify either a folder containing a bunch of lean</span>\n<span class=\"s2\">files, or a single lean file for which you would like to create benchmarks for. It also can create</span>\n<span class=\"s2\">fractional splits of that data.</span>\n\n<span class=\"s2\">Usage:</span>\n\n<span class=\"s2\">python -m formal.lean.datasets.create_benchmarks_from_folder --folder_path &lt;FOLDER_PATH&gt;</span>\n\n<span class=\"s2\">or</span>\n\n<span class=\"s2\">python -m formal.lean.datasets.create_benchmarks_from_folder --lean_path &lt;LEAN_FILE_PATH&gt;</span>\n\n<span class=\"s2\">\"\"\"</span>\n\n\n<span class=\"kd\">def</span> <span class=\"n\">recursively_grab_lean_files</span><span class=\"o\">(</span><span class=\"n\">folder_path</span><span class=\"o\">:</span> <span class=\"n\">str</span><span class=\"o\">):</span>\n    <span class=\"n\">files</span> <span class=\"bp\">=</span> <span class=\"o\">[]</span>\n    <span class=\"n\">for</span> <span class=\"n\">r</span><span class=\"o\">,</span> <span class=\"n\">_</span><span class=\"o\">,</span> <span class=\"n\">f</span> <span class=\"k\">in</span> <span class=\"n\">os.walk</span><span class=\"o\">(</span><span class=\"n\">folder_path</span><span class=\"o\">):</span>\n        <span class=\"n\">for</span> <span class=\"n\">file</span> <span class=\"k\">in</span> <span class=\"n\">f</span><span class=\"o\">:</span>\n            <span class=\"k\">if</span> <span class=\"s2\">\".lean\"</span> <span class=\"k\">in</span> <span class=\"n\">file</span><span class=\"o\">:</span>\n                <span class=\"n\">files.append</span><span class=\"o\">(</span><span class=\"n\">os.path.join</span><span class=\"o\">(</span><span class=\"n\">r</span><span class=\"o\">,</span> <span class=\"n\">file</span><span class=\"o\">))</span>\n    <span class=\"n\">return</span> <span class=\"n\">files</span>\n\n\n<span class=\"kd\">def</span> <span class=\"n\">run</span><span class=\"o\">(</span><span class=\"n\">lean_path</span><span class=\"o\">:</span> <span class=\"n\">Optional</span><span class=\"o\">[</span><span class=\"n\">str</span><span class=\"o\">]</span> <span class=\"bp\">=</span> <span class=\"n\">None</span><span class=\"o\">,</span> <span class=\"n\">folder_path</span><span class=\"o\">:</span> <span class=\"n\">Optional</span><span class=\"o\">[</span><span class=\"n\">str</span><span class=\"o\">]</span> <span class=\"bp\">=</span> <span class=\"n\">None</span><span class=\"o\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">folder_path</span> <span class=\"n\">and</span> <span class=\"n\">lean_path</span><span class=\"o\">:</span>\n        <span class=\"n\">err_msg</span> <span class=\"bp\">=</span> <span class=\"s2\">\"Please specify *only* either a folder_path or a path to a specific lean file\"</span>\n        <span class=\"n\">raise</span> <span class=\"n\">ValueError</span><span class=\"o\">(</span><span class=\"n\">err_msg</span><span class=\"o\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">lean_path</span><span class=\"o\">:</span>\n        <span class=\"n\">file_paths</span> <span class=\"bp\">=</span> <span class=\"o\">[</span><span class=\"n\">os.path.expanduser</span><span class=\"o\">(</span><span class=\"n\">lean_path</span><span class=\"o\">)]</span>\n    <span class=\"n\">elif</span> <span class=\"n\">folder_path</span><span class=\"o\">:</span>\n        <span class=\"n\">file_paths</span> <span class=\"bp\">=</span> <span class=\"n\">recursively_grab_lean_files</span><span class=\"o\">(</span><span class=\"n\">folder_path</span><span class=\"o\">)</span>\n\n    <span class=\"n\">data</span> <span class=\"bp\">=</span> <span class=\"o\">[]</span>\n\n    <span class=\"bp\">#</span> <span class=\"n\">Create</span> <span class=\"n\">benchmark</span> <span class=\"k\">from</span> <span class=\"n\">list</span> <span class=\"n\">of</span> <span class=\"n\">lean</span> <span class=\"n\">file_paths</span>\n    <span class=\"n\">for</span> <span class=\"n\">benchmark_path</span> <span class=\"k\">in</span> <span class=\"n\">file_paths</span><span class=\"o\">:</span>\n        <span class=\"k\">with</span> <span class=\"kn\">open</span><span class=\"o\">(</span><span class=\"n\">benchmark_path</span><span class=\"o\">,</span> <span class=\"s2\">\"r\"</span><span class=\"o\">)</span> <span class=\"n\">as</span> <span class=\"n\">f</span><span class=\"o\">:</span>\n            <span class=\"n\">opens</span> <span class=\"bp\">=</span> <span class=\"o\">[]</span>\n            <span class=\"n\">imports</span> <span class=\"bp\">=</span> <span class=\"o\">[]</span>\n            <span class=\"n\">begin_count</span> <span class=\"bp\">=</span> <span class=\"mi\">0</span>\n\n            <span class=\"n\">for</span> <span class=\"n\">line</span> <span class=\"k\">in</span> <span class=\"n\">f.readlines</span><span class=\"o\">():</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.startswith</span><span class=\"o\">(</span><span class=\"s2\">\"import \"</span><span class=\"o\">):</span>\n                    <span class=\"n\">imports</span> <span class=\"bp\">+=</span> <span class=\"o\">[</span><span class=\"n\">line</span><span class=\"o\">[</span><span class=\"mi\">7</span><span class=\"o\">:]</span><span class=\"bp\">.</span><span class=\"n\">strip</span><span class=\"o\">(</span><span class=\"s2\">\" </span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"o\">)]</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.strip</span><span class=\"o\">()</span> <span class=\"bp\">==</span> <span class=\"s2\">\"begin\"</span><span class=\"o\">:</span>\n                    <span class=\"n\">begin_count</span> <span class=\"bp\">+=</span> <span class=\"mi\">1</span>\n                    <span class=\"n\">continue</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.strip</span><span class=\"o\">()</span> <span class=\"bp\">==</span> <span class=\"s2\">\"end\"</span><span class=\"o\">:</span>\n                    <span class=\"k\">if</span> <span class=\"n\">begin_count</span> <span class=\"bp\">&gt;</span> <span class=\"mi\">0</span><span class=\"o\">:</span>\n                        <span class=\"n\">begin_count</span> <span class=\"bp\">-=</span> <span class=\"mi\">1</span>\n                        <span class=\"n\">continue</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.startswith</span><span class=\"o\">(</span><span class=\"s2\">\"section\"</span><span class=\"o\">):</span>\n                    <span class=\"n\">opens</span> <span class=\"bp\">=</span> <span class=\"o\">[]</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.startswith</span><span class=\"o\">(</span><span class=\"s2\">\"open \"</span><span class=\"o\">):</span>\n                    <span class=\"n\">opens</span> <span class=\"bp\">+=</span> <span class=\"n\">line</span><span class=\"o\">[</span><span class=\"mi\">5</span><span class=\"o\">:]</span><span class=\"bp\">.</span><span class=\"n\">strip</span><span class=\"o\">()</span><span class=\"bp\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s2\">\" \"</span><span class=\"o\">)</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.startswith</span><span class=\"o\">(</span><span class=\"s2\">\"end\"</span><span class=\"o\">):</span>\n                    <span class=\"n\">opens</span> <span class=\"bp\">=</span> <span class=\"o\">[]</span>\n                <span class=\"k\">if</span> <span class=\"n\">line.startswith</span><span class=\"o\">(</span><span class=\"s2\">\"theorem\"</span><span class=\"o\">):</span>\n                    <span class=\"n\">name</span> <span class=\"bp\">=</span> <span class=\"n\">line</span><span class=\"o\">[</span><span class=\"mi\">7</span><span class=\"o\">:]</span><span class=\"bp\">.</span><span class=\"n\">strip</span><span class=\"o\">(</span><span class=\"s2\">\" :</span><span class=\"se\">\\n</span><span class=\"s2\">\"</span><span class=\"o\">)</span>\n                    <span class=\"bp\">#</span> <span class=\"n\">print</span><span class=\"o\">(</span><span class=\"n\">f</span><span class=\"s2\">\"DECL {name}\"</span><span class=\"o\">)</span>\n                    <span class=\"n\">data.append</span><span class=\"o\">({</span><span class=\"s2\">\"name\"</span><span class=\"o\">:</span> <span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"s2\">\"opens\"</span><span class=\"o\">:</span> <span class=\"n\">opens</span><span class=\"o\">,</span> <span class=\"s2\">\"imports\"</span><span class=\"o\">:</span> <span class=\"n\">imports</span><span class=\"o\">})</span>\n\n    <span class=\"n\">random.shuffle</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">)</span>\n    <span class=\"k\">with</span> <span class=\"n\">jsonlines.open</span><span class=\"o\">(</span><span class=\"s2\">\"out_benchmark.jsonl\"</span><span class=\"o\">,</span> <span class=\"s2\">\"w\"</span><span class=\"o\">)</span> <span class=\"n\">as</span> <span class=\"n\">w</span><span class=\"o\">:</span>\n        <span class=\"n\">for</span> <span class=\"n\">d</span> <span class=\"k\">in</span> <span class=\"n\">data</span><span class=\"o\">:</span>\n            <span class=\"n\">entry</span> <span class=\"bp\">=</span> <span class=\"o\">{</span><span class=\"s2\">\"decl\"</span><span class=\"o\">:</span> <span class=\"n\">d</span><span class=\"o\">}</span>\n            <span class=\"n\">w.write</span><span class=\"o\">(</span><span class=\"n\">entry</span><span class=\"o\">)</span>\n\n\n<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"bp\">==</span> <span class=\"s2\">\"__main__\"</span><span class=\"o\">:</span>\n    <span class=\"n\">Smokey</span><span class=\"o\">(</span><span class=\"n\">run</span><span class=\"o\">)</span>\n</code></pre></div>",
        "id": 278060396,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1649267066
    },
    {
        "content": "<p>A lot of thanks!</p>",
        "id": 278113051,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1649301679
    }
]