[
    {
        "content": "<p>Hey there! I recently joined this community and I want to play around with Lean. My main background is in natural language processing. It's my impression that you can use neural networks for theorem proving by training a large language model and using Lean for verification after generating candidate proofs (e. g. <a href=\"https://openai.com/blog/formal-math/\">this</a>). Can anyone point me to how you can actually use Lean for that sort of stuff automatically, by talking to Lean from Python? (A wrapper or a server with an API would be nice...)</p>",
        "id": 312678492,
        "sender_full_name": "Vadim Fomin",
        "timestamp": 1669655971
    },
    {
        "content": "<p>the openai project that did that wrote such a wrapper and published it: <a href=\"https://github.com/openai/lean-gym\">https://github.com/openai/lean-gym</a></p>",
        "id": 312682030,
        "sender_full_name": "Andrés Goens",
        "timestamp": 1669657031
    },
    {
        "content": "<blockquote>\n<p>It's my impression that you can use neural networks for theorem proving by training a large language model and using Lean for verification after generating candidate proofs</p>\n</blockquote>\n<p>wether that's true I guess is <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Meta.20IMO.20result/near/311956563\">up for debate</a></p>",
        "id": 312682130,
        "sender_full_name": "Andrés Goens",
        "timestamp": 1669657070
    },
    {
        "content": "<p>Why do you say it's up for debate? <br>\nI believe the only debate in the post you linked is about the faithfulness of formal statements in minif2f, not about the use of lean to verify proofs :)</p>\n<p>Other useful resources by openai for neural theorem proving and lean : </p>\n<p><a href=\"https://github.com/jesse-michael-han/lean-step-public\">https://github.com/jesse-michael-han/lean-step-public</a><br>\n<a href=\"https://github.com/jasonrute/lean_proof_recording\">https://github.com/jasonrute/lean_proof_recording</a></p>",
        "id": 312682799,
        "sender_full_name": "Timothee Lacroix",
        "timestamp": 1669657269
    },
    {
        "content": "<p>And latest minif2f <a href=\"https://github.com/facebookresearch/miniF2F\">https://github.com/facebookresearch/miniF2F</a></p>",
        "id": 312684577,
        "sender_full_name": "Timothee Lacroix",
        "timestamp": 1669657409
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"359917\">Timothee Lacroix</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.20on.20top.20of.20language.20models.3F/near/312682799\">said</a>:</p>\n<blockquote>\n<p>Why do you say it's up for debate? <br>\nI believe the only debate in the post you linked is about the faithfulness of formal statements in minif2f, not about the use of lean to verify proofs :)<br>\n</p>\n</blockquote>\n<p>oh, sorry, I should have been a bit more precise. I didn't mean that using Lean to verify them was up for debate, but rather how well NLP-based language models transfer over to Lean/formal statements</p>",
        "id": 312684900,
        "sender_full_name": "Andrés Goens",
        "timestamp": 1669657511
    },
    {
        "content": "<p>(thanks for the clarification)</p>",
        "id": 312684981,
        "sender_full_name": "Andrés Goens",
        "timestamp": 1669657542
    },
    {
        "content": "<p>Ah yeah :) some work remains to be done, that's for sure :D</p>",
        "id": 312685916,
        "sender_full_name": "Timothee Lacroix",
        "timestamp": 1669657832
    },
    {
        "content": "<p>First, it should be pointed out that Lean (and most other ITPs) are designed as computer languages and use-cases like this have been ad-hoc-ly added on top, and don't feel very natural.  For lean, there are three many routes of communication:</p>\n<ol>\n<li>Metaprogramming: Lean has an extensive meta-programming framework which lets you look at the internals of Lean objects, execute tactics, and just in general communicate with IO in a variety of ways.  The problem is that you have to know Lean's metaprogramming framework (written in Lean) to get anywhere.  The lean-gym mentioned above as well as written in Lean.  If your usecase fits into that, then you don't need to even touch Lean's metaprogramming code, but the API is limited.   The two datasets mentioned above also are gotten via metaprogramming.  Those are the same datasets used by both OpenAI and Meta Research.</li>\n<li>C++: Lean 3 is written in C++.  (An aside, Lean 3 will probably be the main version of Lean for about another year or less, while the library is ported to Lean 4.  Lean 4 is mostly written in Lean 4 and has better programming features.  My answer is for Lean 3 since that is where all the proof data is currently located.)  Meta Research wrote (proprietary) code to directly hook into the C++ backend.  However, a small piece of that is available for everyone to use, namely, you can hook up any LLM to Lean's infoview to give suggestions.  (<a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/api.20suggestion.20vscode.20integration\">#general &gt; api suggestion vscode integration</a>) This is the easies way to use an existing LLM.  Again, the API is particular to their use case and format.</li>\n<li>Lean communicates with VS Code, Emacs, etc. via a language server.  It is a simple JSON server that can be used directly.  But there is also a light weight python interface (which is not well maintained) at <a href=\"https://github.com/leanprover-community/lean-client-python\">https://github.com/leanprover-community/lean-client-python</a>.  I could imagine a copilot-like API which directly interacts with the editor or language server.</li>\n</ol>",
        "id": 312686361,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669657952
    },
    {
        "content": "<p>As for how to go about training a LLM, it is fairly easy if you have the data (and the GPUs/skills <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span>).  See this talk (<a href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\">https://www.youtube.com/watch?v=EXpmbAfBNnw</a>) or any of the following papers:</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"EXpmbAfBNnw\" href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\"><img src=\"https://uploads.zulipusercontent.net/bd2a9c6a819c867efb19ea439f4a113b891f5dae/68747470733a2f2f692e7974696d672e636f6d2f76692f4558706d624166424e6e772f64656661756c742e6a7067\"></a></div><ul>\n<li><a href=\"https://arxiv.org/abs/2009.03393\">Generative Language Modeling for Automated Theorem Proving</a></li>\n<li><a href=\"https://arxiv.org/abs/2102.06203\">Proof Artifact Co-training for Theorem Proving with Language Models</a></li>\n<li><a href=\"https://arxiv.org/abs/2202.01344\">Formal Mathematics Statement Curriculum Learning</a></li>\n<li><a href=\"https://arxiv.org/abs/2205.10893\">Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers</a></li>\n<li><a href=\"https://arxiv.org/abs/2205.11491\">HyperTree Proof Search for Neural Theorem Proving</a></li>\n</ul>",
        "id": 312687271,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669658225
    },
    {
        "content": "<p>If you aren't ITP specific, there is also a toy environment called INT, which is easier to work with:</p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/2007.02924\">INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving\n</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/albertqjiang/INT\">https://github.com/albertqjiang/INT</a></p>\n</li>\n</ul>",
        "id": 312687804,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669658394
    },
    {
        "content": "<p>And Isabelle also has a publicly available interface for machine learning maintained by <span class=\"user-mention\" data-user-id=\"258175\">@Albert Jiang</span> and others, and used in their papers, including Thor.  I think it is <a href=\"https://github.com/albertqjiang/Portal-to-ISAbelle\">https://github.com/albertqjiang/Portal-to-ISAbelle</a></p>",
        "id": 312688386,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669658543
    },
    {
        "content": "<p>Finally if you want to use pretrained models like GPT-3, Codex, etc.  You could just have that generate whole proofs in one go, with no interaction.   Then you could just interact with Lean via its command line interface, asking it to just check a proof in a file.</p>",
        "id": 312689525,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669658892
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"315434\">Andrés Goens</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Lean.20on.20top.20of.20language.20models.3F/near/312682030\">said</a>:</p>\n<blockquote>\n<p>the openai project that did that wrote such a wrapper and published it: <a href=\"https://github.com/openai/lean-gym\">https://github.com/openai/lean-gym</a></p>\n</blockquote>\n<p>That's specifically what I was looking for, thank you!</p>",
        "id": 312692813,
        "sender_full_name": "Vadim Fomin",
        "timestamp": 1669659865
    },
    {
        "content": "<p>Thank you Jason, very useful!! Will mess around and probably come back with more questions :)</p>",
        "id": 312693372,
        "sender_full_name": "Vadim Fomin",
        "timestamp": 1669660012
    },
    {
        "content": "<p>I think the most reasonable thing for me would be to figure out how to start a server, and dive into its API</p>",
        "id": 312703076,
        "sender_full_name": "Vadim Fomin",
        "timestamp": 1669662998
    }
]