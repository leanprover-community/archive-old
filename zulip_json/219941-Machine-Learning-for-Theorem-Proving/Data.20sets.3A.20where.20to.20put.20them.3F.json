[
    {
        "content": "<p>I'm in the process of creating some clean Lean datasets ready for machine learning prediction, and I'm trying to figure out where to put them.  Any suggestions?</p>",
        "id": 195257662,
        "sender_full_name": "Jason Rute",
        "timestamp": 1587772714
    },
    {
        "content": "<p>One option is GitHub?  Another is Kaggle.  Or I could put it on GitHub and link to it from Kaggle.  I am sure there are many other places.  Any one with familiarity with this sort of thing?  Does anyone else what to see, play with the datasets?</p>",
        "id": 195257669,
        "sender_full_name": "Jason Rute",
        "timestamp": 1587772726
    },
    {
        "content": "<p>One of the datasets will be all (well...most) instances of tactic keywords and the state when they are applied.  (I'm a bit of a perfectionist and there are a number of small issues in this dataset, but I think it is worth releasing anyway to start playing with.  I want to learn more about formula embeddings and I think it will be a great dataset to start with.  I can always create another version later.  (This isn't a state of the art dataset.  HOLStep and the Mizar datasets have been around for a long time, so I don't think it needs to be perfect at the beginning.)</p>",
        "id": 195257697,
        "sender_full_name": "Jason Rute",
        "timestamp": 1587772781
    },
    {
        "content": "<p>Another dataset will be all of the instances of the <code>rw</code> tactic, the exact rewrite formula applied, and the state just before the rewrite is applied.  If it is easy, I might also include all the outputs of <code>rw_hint</code> so that it is a true prediction problem (pick which of these rewrites is the one the human used).</p>",
        "id": 195257782,
        "sender_full_name": "Jason Rute",
        "timestamp": 1587772851
    },
    {
        "content": "<p>I'm also still working on other methods of \"proof recording\" so I can create more and better datasets.</p>",
        "id": 195257804,
        "sender_full_name": "Jason Rute",
        "timestamp": 1587772875
    },
    {
        "content": "<p>This is great! Can't wait to take a look. I'd say github unless you are over the file size limit (100mb per file).</p>",
        "id": 195257880,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1587772985
    },
    {
        "content": "<p>I think I can stay under that.  I can always break up the files.</p>",
        "id": 195258429,
        "sender_full_name": "Jason Rute",
        "timestamp": 1587773553
    },
    {
        "content": "<p>Kaggle is a good place to get some ml folks accidentally interested in ITPs :)</p>",
        "id": 195410751,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1587991363
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> hi! sorry for the long radio silence. Where can I find the latest about your datasets?</p>",
        "id": 204054642,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1594885719
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> Also sorry for taking a few days to get back to you.  I moved a few weeks ago, and haven't got back into the COVID lockdown groove I was in before.  Also, I have some stuff in my personal life which is making me busier right now.  I haven't put any datasets online yet, but let me tell you what sort of things I have been working on (and also convince you why the datasets are a mess right now).</p>\n<ul>\n<li>I figured out how to run the lean server <code>info</code> command on every character.  It takes about half a day on a c4 EC2 instance if I recall.  You can find the tool here: <a href=\"https://github.com/jasonrute/lean_info_scrapper\">Lean Info Scraper</a>.  While I didn't put the raw data online, I did put a visual version of it here: <a href=\"https://github.com/jasonrute/annotated_lean\">Annotated Lean</a>.  As <a href=\"#narrow/stream/113488-general/topic/annotated.20lean.20files\">discussed here</a>, there are a lot of peculiarities in the results.  This has made it hard to turn them into a clean dataset.  I recently realized that some (but certainly not all) of the peculiarities are due to the way I was calling the server.  If one calls the <code>info</code> command twice before the first <code>info</code> command finishes, weird behaviors happen.  That might have been fixed in the most recent version of lean, and it would also wouldn't be an issue if I used asynchronous programming to call the lean server.  That is what Patrick's <a href=\"https://github.com/leanprover-community/lean-client-python\">lean-client for python</a> does.  (I've been working to clean that project up so I can use it.)</li>\n<li>After the discussion <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/rw_hint\">here</a>, on making a machine learned ranking tool for rewrites, I've gathered some data on rewriting in lean.  Instead of using the messy lean-server info command, I just modified lean to record every application of the <code>rw</code> (and similar tactics) during compilation.  I have the details for each <code>rw</code> tactic what was used to rewrite, and what was the goal before the rewrite.  It took a few hours to run.  Also in order to run it I have to manually edit the lean code, but this isn't bad since it is just <code>.lean</code> files.  I don't have to touch the <code>C++</code> files.  I was hoping to clean up the data a bit before posting it.  I was also possibly hoping to integrate with the <code>rw_hint</code>.  That way I would have positive and negative examples.  Right now I just have positive examples (which using hard negative mining and the like would still make it ok data).  Also, I'm very uncertain right now on exactly how one should present goals and expressions for machine learning.  There are a number of approaches: (1) just take the pretty-printed goal (however, no one does ML from the PP goal), (2) take a parsed version of the expression (the problem here is that there are a number of different small issues including the handling of variables and types), (3) run the expression through a standard hard-coded feature extractor for logical expressions (there are a number of standard ones and this is good since no Lean tactic is going to start using GNNs, LSTMs, or Transformers anytime soon).  Of course it is possible that I'm just overthinking this and should just dump what I have.  (The code lives on a branch here: <a href=\"https://github.com/leanprover-community/lean/compare/master...jasonrute:v3.9.0-rw-recording\">https://github.com/leanprover-community/lean/compare/master...jasonrute:v3.9.0-rw-recording</a> .  Anyone could run it.  Ask me if you want instructions.)</li>\n<li>I could also apply something similar to all the standard tactics and get information about every tactic application, including the goal, the tactic, and the parameters used.  Again it involves modifying the source lean files (but not the source C++).</li>\n<li>My big project is a new lean-python interface which doesn't go through the lean server.  Instead, it lets you communicate back and forth between Lean and Python inside a tactic.  This lets one create fast-ish Gym environments for Lean inside Python (where Lean applies the tactics that Python tells it to and keeps tract of the results, including a branching state tree) and where Python runs the machine learning.  Conversely it can also be used to create prototype tactics which run inside Lean but communicate with Python.  Again, Lean tests out various tactics (as Python tells it to do) and keeps track of the results.  I'm close to done with my prototype, but over the last 6 weeks I have got caught up with other things.  After I finish my prototype, I'm planning no soliciting advice from you and others.</li>\n<li>My (too ambitious) goal was to finish the new Lean-Python communication tool and then make a version of TacticToe for Lean.  I will record tactic proofs (as explained above) and then plug them into k-nearest neighbors.  Using my interface, I could create a <code>tactictoe</code> tactic which performs a tree search guided by this k-NN model.  With minimal work, a user, like Kevin Buzzard, could set it up and try it out and let me know what he thinks.  It might also be possible to install it on a web server.  However, I'm just one person without a lot of free time, so we will see...</li>\n</ul>\n<p>Besides the above issues (especially my lack of free time), there are also some others:</p>\n<ul>\n<li>Lean 3 (both the library, the application, and the server) is changing really quickly, and the library is being massively refactored, which makes it hard to build stable datasets (unless I just fix a previous version of Lean like Lean 3.10).</li>\n<li>Lean 4 will theoretically come out in two months.  We will see if they keep this timeline, but again, it makes it unclear if all this data gathering and code will go to waste.</li>\n<li>I don't think there is a lot of interest in Lean and machine learning right now.  I'm hoping after I build some of the above tools, I can create interest, but we'll see.</li>\n</ul>",
        "id": 204324247,
        "sender_full_name": "Jason Rute",
        "timestamp": 1595118406
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> thank you so much for your very detailed response. I really enjoyed the annotated Lean files!! This is really great. We'll dig into the lean-client for python as the demo there seems to include most of what we need to get started with doing exploratory ML work on Lean. Of the issues you mention, I think we can help with the third one obviously :) For the two others I feel like we can simply freeze the version of Lean 3 we use and I'm not too worried that Lean 4 will provide reasonable interface we can adapt to?</p>",
        "id": 204400616,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1595236208
    }
]