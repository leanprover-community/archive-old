[
    {
        "content": "<p>There is a site <a href=\"https://paperswithcode.com\" target=\"_blank\" title=\"https://paperswithcode.com\">https://paperswithcode.com</a>.  It is contains most modern ML papers, especially those with associated open source code bases.  They also maintain lists of leaderboard specifying which models are state of the art.  (Such leaderboards should be taken with a grain of salt, but at the same time I think they are quite useful in guiding the field.)</p>",
        "id": 187742321,
        "sender_full_name": "Jason Rute",
        "timestamp": 1581218462
    },
    {
        "content": "<p>Anyway, the site is broken down by topic, and I've been, for the most part, unofficially maintaining the <a href=\"https://paperswithcode.com/task/automated-theorem-proving\" target=\"_blank\" title=\"https://paperswithcode.com/task/automated-theorem-proving\">Automated Theorem Proving Topic</a> section.  I mostly do the following:</p>\n<ul>\n<li>Add papers which haven't already been added. (Most papers are quickly added by others or by automated tools, but some get left out.)</li>\n<li>Adding links to the code for the papers.  This is really important and one of the main purposes of the site.</li>\n<li>Adding items to the leaderboards and add new leaderboards.  I think leaderboards/benchmarks/datasets are one of the best approaches to advertise theorem proving to the ML community.  However, my approach has been to only add a leaderboard if (1) it is a clear benchmark/dataset that someone could try to compete against, and (2) there have been at least two papers on that particular dataset/benchmark (so at least two data points demonstrating that others think this is a valuable metric). But that is just my approach.  It also helps if that benchmark/dataset has a well-defined name.  Since TP problems are often of the form \"prove as many theorems within time limit T,\" it is hard to sum up the benchmark in a short title if it doesn't have a well-defined name.  (One unfortunately can't add links or a description.)</li>\n</ul>",
        "id": 187742324,
        "sender_full_name": "Jason Rute",
        "timestamp": 1581218468
    },
    {
        "content": "<p><strong>Is anyone interested in helping me maintain that part of the site?</strong>  My role isn't official, and anyone can add stuff to the site, but I think it is important that those with interest and understanding in ML and TP maintain that section.  I've been preoccupied with other stuff and haven't added anything for a few months.</p>",
        "id": 187742326,
        "sender_full_name": "Jason Rute",
        "timestamp": 1581218477
    },
    {
        "content": "<p>I have added a few entries to <a href=\"https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-test\">https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-test</a> (sledgehammer, sledgehammer with heuristics, which are baselines discussed in papers, such that readers of the webpage can have an idea how ATP performs; as well as PACT)</p>",
        "id": 312747227,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669681873
    },
    {
        "content": "<p>Thanks for doing this.  What does <code>Pass@8</code> mean in this context, and more importantly is it helpful or confusing?  For example, all good models (including Hammer) have search.  So it doesn't mean what it means in say AlphaCode, where Pass@8 means one of the top ten results is correct (but the model can't be sure which one), since in theorem proving you can check if a proof is correct and if not, then backtrack.  I think, if I remember correctly that OpenAI started using that term to mean just rerunning the model eight times.  I think it is a useful metric, but I also think it is gets into the difficulties of comparison.  For example, the Minerva solution, while being Pass@100 in some sense also visits fewer proof-states than some other models which do more searching inside a single search tree.  But of course the Minerva model probably uses way more compute (as say measured in FLOPS).</p>",
        "id": 312843183,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669728767
    },
    {
        "content": "<p>It is a sad state of affairs that the best thing to do in a proof search sometimes is to just throw everything away and start from scratch. <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span>  A good model should be able to learn from its mistakes.</p>",
        "id": 312843197,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669728770
    },
    {
        "content": "<p>Also the line:</p>\n<blockquote>\n<p>GPT-f   0.366           HyperTree Proof Search for Neural Theorem Proving   2022</p>\n</blockquote>\n<p>confuses me.  What is this referring to, and should it be 36.6?  (Or should all the numbers be between 0.0 and 1.0?)  Lean GPT-f is confusing.  I think it is referring to the model mentioned above under Lean expert iteration.</p>",
        "id": 312844859,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669729301
    },
    {
        "content": "<p>Actually, I really think <code>Pass@N</code> is confusing.  In the original <a href=\"https://paperswithcode.com/paper/minif2f-a-cross-system-benchmark-for-formal\">Mini-F2F paper</a>, they say:</p>\n<blockquote>\n<p>Pass rates are reported as Pass@N where N is the number of proof search attempts per statement. Pass@N is computed by running more attempts per statement, averaged to get an unbiased, low-variance estimate.</p>\n</blockquote>\n<p>Note, they say <em>averaged</em>.</p>",
        "id": 312845334,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669729448
    },
    {
        "content": "<p>So I think in the OpenAI papers, Pass@N is used to just used to reduce variance in the pass rate estimate.</p>",
        "id": 312846569,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669729782
    },
    {
        "content": "<p>And the HTPS paper also uses average pass rate mostly, except in a few cases where it uses cumulative pass rate.</p>",
        "id": 312847198,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669729952
    },
    {
        "content": "<p>So in general, the results of pass@8 and pass@64 are directly compare-able.  The latter just has lower variance.</p>",
        "id": 312847433,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730029
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> </p>\n<blockquote>\n<p>What does Pass@8 mean in this context, and more importantly is it helpful or confusing?</p>\n</blockquote>\n<p>It also confuses me, and it was there before my modification. Maybe we should delete that pass@8?</p>\n<blockquote>\n<p>GPT-f 0.366</p>\n</blockquote>\n<p>Also not by me and yes that looks buggy ;)</p>",
        "id": 312847973,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730183
    },
    {
        "content": "<p>0.366 modified now.</p>\n<p>Hmm but what about \"Lean Expert Iteration 29.6  34.5    36.6\". More passes looks better. Maybe we should just keep the highest score?</p>",
        "id": 312848199,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730255
    },
    {
        "content": "<p>The Minerva result uses 100 to mean 100 attempts (but again this is just a form of proof search).</p>",
        "id": 312848267,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730280
    },
    {
        "content": "<p>Then I guess maybe we should just pick highest score and merge all those \"pass@N\" columns</p>",
        "id": 312848446,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730334
    },
    {
        "content": "<p>I think it would be good to have one column with the lowest variance number.</p>",
        "id": 312848468,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730340
    },
    {
        "content": "<p>Anyway here is a screenshot backup before further changes<br>\n<a href=\"/user_uploads/3121/_zL28NvUgVmKyI8PUIWL2zK-/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/_zL28NvUgVmKyI8PUIWL2zK-/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/_zL28NvUgVmKyI8PUIWL2zK-/image.png\"></a></div>",
        "id": 312848517,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730353
    },
    {
        "content": "<p>We could also add tags for which ITP.</p>",
        "id": 312848568,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730367
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312848468\">said</a>:</p>\n<blockquote>\n<p>I think it would be good to have one column with the lowest variance number.</p>\n</blockquote>\n<p>Not very sure, so what should we do for \"Lean Expert Iteration\" row?</p>",
        "id": 312848574,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730369
    },
    {
        "content": "<p>The link for PACT is just a repeat the the Lean GPT-f line above I think.  I don't think Thor reproduced it.</p>",
        "id": 312848639,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730395
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312848568\">said</a>:</p>\n<blockquote>\n<p>We could also add tags for which ITP.</p>\n</blockquote>\n<p>done, ATPs are labelled (all others are ITP)</p>",
        "id": 312848841,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730426
    },
    {
        "content": "<p>Before we rush to change too much, we could ask the authors: <span class=\"user-mention\" data-user-id=\"258175\">@Albert Jiang</span> <span class=\"user-mention\" data-user-id=\"359917\">@Timothee Lacroix</span> <span class=\"user-mention\" data-user-id=\"258218\">@Guillaume</span> <span class=\"user-mention\" data-user-id=\"408484\">@Kunhao Zheng</span>, etc ?</p>",
        "id": 312849159,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730496
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312848639\">said</a>:</p>\n<blockquote>\n<p>The link for PACT is just a repeat the the Lean GPT-f line above I think.  I don't think Thor reproduced it.</p>\n</blockquote>\n<p>Sorry do not get it. Thor mentions in its paper: <a href=\"/user_uploads/3121/1CzKSZ_9R4ftxYjyKTj4R1O2/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/1CzKSZ_9R4ftxYjyKTj4R1O2/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/1CzKSZ_9R4ftxYjyKTj4R1O2/image.png\"></a></div>",
        "id": 312849174,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730501
    },
    {
        "content": "<p>Good idea! So I will not modify it further. Anyway previous modifications are harmless: They either add new rows (without touching old data) or labels, or fix explicit bug (0.366 vs 36.6)</p>",
        "id": 312849355,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730542
    },
    {
        "content": "<p>The PACT paper (PACT is the training data methodology) has the Lean GPT-f model.  The original PACT paper didn't test on MiniF2F since it didn't exist.  It was tested on MiniF2F in the miniF2f paper, but using Lean GPT-f model from the PACT paper.</p>",
        "id": 312849474,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730583
    },
    {
        "content": "<p>Some people call the model PACT.  Others call it Lean GPT-f.  It is ambiguous.</p>",
        "id": 312849780,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730672
    },
    {
        "content": "<p>Then looks like we should merge \"Lean GPT-f\" and \"PACT\" into one row and name it something like \"PACT i.e. Lean GPT-f\"</p>",
        "id": 312849975,
        "sender_full_name": "Tom Chen",
        "timestamp": 1669730725
    },
    {
        "content": "<p>Or \"Lean GPT-f (a.k.a. PACT)\"</p>",
        "id": 312850077,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669730753
    },
    {
        "content": "<p>Wait, sorry!  I misread the Pass@N stuff.  It is talking about the number of <em>attempts</em>.  But what they do is run many more than N attempts and average (where I think they split all the attempts into buckets of size N, take the Pass@N for each bucket, and average all the rates across buckets) to get Pass@N.  So it does make it make sense that it is going up with N since that is more attempts.  But at the same time, it is still possible to compare pass@100 for one model and pass@8 for another.  The first uses more compute (all other things being equal), but both models are performing a single, large proof search in the end.</p>",
        "id": 312852008,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669731305
    },
    {
        "content": "<p>Maybe it is easiest to leave it as is.  Or to have a pass rate column and a separate attempts column.</p>",
        "id": 312852374,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669731406
    },
    {
        "content": "<p>For this question I'm not quite sure. Maybe summoning <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> better know the term here. :D</p>",
        "id": 312853494,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669731728
    },
    {
        "content": "<p>My thought is that the rationale behind the pass@N is the indeterminism of sampling of language models. If you set the sampling budget to 128, the 128 proof terms sampled this time/attempt could be different from the next time/attempt, which determines how the proof tree grows in the tactic states that follows. So each attempt you grow a different proof tree.</p>",
        "id": 312854302,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669731956
    },
    {
        "content": "<p>Last nitpick:  I think all the OpenAI models listed there use external data in that the models started from a pre-trained variation of GPT-3 and also incorporate arXiv, Github, and math stackexchange.</p>",
        "id": 312854496,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669732000
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"408484\">Kunhao Zheng</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312854302\">said</a>:</p>\n<blockquote>\n<p>My thought is that the rationale behind the pass@N is the indeterminism of sampling of language models. If you set the sampling budget to 128, the 128 proof terms sampled this time/attempt could be different from the next time/attempt, which determines how the proof tree grows in the tactic states that follows. So each attempt you grow a different proof tree.</p>\n</blockquote>\n<p>And that's why in minif2f paper you would not find other than <code>pass@1</code> for the <code>tidy</code>. Because you can regard <code>tidy</code> as a deterministic sampling, that always output the <code>L+HL</code>, <code>pass@N</code> with N&gt;1 will be the same as <code>pass@1</code>.</p>",
        "id": 312854932,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669732132
    },
    {
        "content": "<p>What is strange about Pass@N for say a MCTS algorithm is that one could either do pass@64 visiting N nodes each time or do pass@1 visiting Nx64 nodes.  It just happens that the first does much better right now.</p>",
        "id": 312855595,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669732290
    },
    {
        "content": "<p>And an ATP like Hammer, your could run it 64 times in parallel for pass@64 (assuming it is stochastic), or run it 64 times longer for pass@1.</p>",
        "id": 312856055,
        "sender_full_name": "Jason Rute",
        "timestamp": 1669732409
    },
    {
        "content": "<p>I give a careful read on the original GPT-f paper <a href=\"https://arxiv.org/abs/2009.03393\">https://arxiv.org/abs/2009.03393</a>, section 4.3.1 and 4.4.</p>\n<p>Note that there are some typos around <code>higher</code> vs <code>lower</code> logprob...</p>\n<blockquote>\n<p>Decreasing the number of expansions per goal e increases the variance as we less systematically explore the action space when expanding each goal. The e value can be taken quite high as each auto-regressive proof step generation uses the same query for a given goal and can therefore be batched together. Increasing e too much may also hurt performance given the breadth first nature of the cumulative logprob ordering. Increasing the number of attempts per proposition a decreases variance and consistently improves performance up to reasonably high values (We use a = 32 attempts for our final benchmarking). We found that a = 4 limited the variance in our evaluations while remaining tractable given the amount of compute we had available. Finally the proof search depth d has little impact on variance but naturally improves performance (we take d = 128 to evaluate our models and d = 256 for our final benchmarking).</p>\n</blockquote>",
        "id": 312861269,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669733872
    },
    {
        "content": "<p>The <code>e</code> here corresponds to <em>the number of new nodes</em> that Jason mentioned. I think the conclusion in this paragraph is under the assumption that the priority queue size is fixed.</p>\n<p>The search setup used in miniF2F <code>tidy</code> is that, we do not grow the search (i.e. insert new tactic states back into the priority queue) once the queue is full OR the tactic state reach the depth limit.</p>",
        "id": 312863014,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669734337
    },
    {
        "content": "<p>Oh also that's under the setting of cumulated logprob for priority queue.  In my experiment of miniF2F <code>tidy</code>, for each new tactic state, its value is computed by the value given by the pre-determined list (and in <em>GPT-f</em> setting it's given by value function) multiplied by the parent's value. So I guess that's different from MCTS where value function is not decaying somehow?</p>",
        "id": 312864762,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669734842
    },
    {
        "content": "<p>That said, even a same proof step in different depth of a tree will have different values due to the decaying we set.</p>",
        "id": 312865210,
        "sender_full_name": "Kunhao Zheng",
        "timestamp": 1669734968
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"563306\">Tom Chen</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312848517\">said</a>:</p>\n<blockquote>\n<p>Anyway here is a screenshot backup before further changes<br>\n<a href=\"/user_uploads/3121/_zL28NvUgVmKyI8PUIWL2zK-/image.png\">image.png</a></p>\n</blockquote>\n<p>Thor + expert iteration on auto-formalised theorems should not be added there, because many of the auto-formalized problems are actually in miniF2F test. So if you do expert iteration on these you already have performed quite some search on test theorems (so this is not a pass@1 at all). Using a similar approach in Evariste we have more than 50% on miniF2F-test with pass@1 when we do the same thing.</p>",
        "id": 312942656,
        "sender_full_name": "Guillaume",
        "timestamp": 1669757824
    },
    {
        "content": "<p>It's not clear what's the best thing to report, but the only way to make this fair is to only allow the model to see the same set of problems during online MCTS / expert iteration (even though it's not the most interesting setting because in practice you want to do your iterations with as many statements as possible)</p>",
        "id": 312943003,
        "sender_full_name": "Guillaume",
        "timestamp": 1669757983
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"258218\">Guillaume</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312942656\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"563306\">Tom Chen</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Maintaining.20Papers.20with.20Code.20for.20Automated.20Theorem.20Proving/near/312848517\">said</a>:</p>\n<blockquote>\n<p>Anyway here is a screenshot backup before further changes<br>\n<a href=\"/user_uploads/3121/_zL28NvUgVmKyI8PUIWL2zK-/image.png\">image.png</a></p>\n</blockquote>\n<p>Thor + expert iteration on auto-formalised theorems should not be added there, because many of the auto-formalized problems are actually in miniF2F test. So if you do expert iteration on these you already have performed quite some search on test theorems (so this is not a pass@1 at all). Using a similar approach in Evariste we have more than 50% on miniF2F-test with pass@1 when we do the same thing.</p>\n</blockquote>\n<p>I agree with Guillaume that Thor + expert iteration is not pass@1. A portion of miniF2F (the MATH dataset ones) might have been seen between passes, if they are formalised correctly. So I'd frame this as an online learning that allows a few tries (few = # expert iteration + 1, in our case few=3) per problem and you can do additional learning in between tries.</p>",
        "id": 312960603,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1669765119
    },
    {
        "content": "<p>It's in the <strong>using extra training data category</strong> though which tells you that it's not magic that it performs better than just Thor</p>",
        "id": 312960687,
        "sender_full_name": "Albert Jiang",
        "timestamp": 1669765187
    },
    {
        "content": "<p>I think I would call that <code>test statements visible during training</code>, or something similar, to highlight the fact that the model is never provided with the proof of test theorems.</p>",
        "id": 312971465,
        "sender_full_name": "Guillaume",
        "timestamp": 1669772376
    },
    {
        "content": "<p>Revisiting the <code>pass@N</code>: In HTPS paper appendix C, they mentioned that there are a lot of hyper-parameters such as decoding temperature, depth penalty, exploration constant, etc. Their approach is to randomly choose those hyper-parameters <em>at the beginning of each proof</em>. Thus, one <em>more</em> reason why <code>pass@N</code> is better when N is larger may be that, the hyper-param happens to be good with more trials.</p>",
        "id": 317278949,
        "sender_full_name": "Tom Chen",
        "timestamp": 1671675132
    }
]