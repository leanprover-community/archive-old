[
    {
        "content": "<p>There is a <a href=\"https://mathai-iclr.github.io/schedule/\">workshop tomorrow on Math and AI</a>.  I won't personally be attending (I have work), but I look forward to <a href=\"https://mathai-iclr.github.io/papers/\">reading the papers and posters</a> and <a href=\"https://mathai-iclr.github.io/schedule/\">watching the talks</a> when they appear on YouTube in about a month.  There are some big names in deep learning there (Bengio, Szegedy), as well as mathematics (Gowers).  Also, a lot of familiar faces giving talks and papers, like Josef Urban, Markus Rabe, <span class=\"user-mention\" data-user-id=\"230999\">@Daniel Selsam</span>, <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span>, <span class=\"user-mention\" data-user-id=\"240875\">@Yuhuai Tony Wu</span>, <span class=\"user-mention\" data-user-id=\"110187\">@Minchao Wu</span>, and <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> (talking about Lean GPT-f or course <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span>).</p>",
        "id": 237742950,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620341790
    },
    {
        "content": "<p>Thanks Jason for helping to advertise the workshop! Yes -- to attend the workshop you need to register at <a href=\"https://iclr.cc/Register/view-registration\">https://iclr.cc/Register/view-registration</a>. You can then follow the program at <a href=\"https://iclr.cc/virtual/2021/workshop/2124\">https://iclr.cc/virtual/2021/workshop/2124</a>.</p>\n<p>I would like to specifically mention the panel discussion tomorrow at <strong>10am PT/ 1pm ET</strong>. It'll be a discussion on general reasoning, the role of math in general intelligence, and the challenges ahead. We will leave 10-20 min in the end for questions from the audience. You can join zoom or leave questions on the rocket chat.</p>\n<p>The panel features Fields Medalist Tim Gowers, Turing Award winner Yoshua Bengio, neural network founding father Jay McClelland, Cambridge AI Professor Mateja Jamnik, neural network guru Christian Szegedy, formal reasoning expert Josef Urban, and math philosopher Alison Pease.</p>\n<p>Hope to see you there!</p>",
        "id": 237745848,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1620343674
    },
    {
        "content": "<p>Some things which look interesting to me in the <a href=\"https://mathai-iclr.github.io/papers/\">papers and posters</a>:</p>\n<ul>\n<li>Proof Artifact Co-training for Theorem Proving With Language Models  of course!  (Although I would recommend looking at the <a href=\"https://arxiv.org/abs/2102.06203\">arXiv version</a> or the <a href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\">Harvard YouTube talk</a>.  Or Jesse's talk when it becomes available.)<div class=\"youtube-video message_inline_image\"><a data-id=\"EXpmbAfBNnw\" href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\"><img src=\"https://uploads.zulipusercontent.net/bd2a9c6a819c867efb19ea439f4a113b891f5dae/68747470733a2f2f692e7974696d672e636f6d2f76692f4558706d624166424e6e772f64656661756c742e6a7067\"></a></div></li>\n<li><span class=\"user-mention\" data-user-id=\"230999\">@Daniel Selsam</span> has a paper and poster about his non-deterministic tactics.  I look forward to learning more about how those actually work (especially now that the examples seem to be written in Lean code).</li>\n<li>I'm particularly interested in the REFACTOR paper where they extract \"Modular and Reusable\" theorems from MetaMath proofs.</li>\n<li>A lot of papers on transformers and mathematical tasks.</li>\n<li>\"Pretrained Transformers as Universal Computation Engines \" is a really interesting paper.  The main idea is that a pertrained Transformer model (an advanced type of language model) are good for all sorts of things having nothing to do with the original task it was trained on.  Also like pretrained image models, one can get away with just fine tuning on a much smaller subset of parameters.  (Here is a <a href=\"https://www.youtube.com/watch?v=Elxn8rS88bI\">video</a> on the paper.) <div class=\"youtube-video message_inline_image\"><a data-id=\"Elxn8rS88bI\" href=\"https://www.youtube.com/watch?v=Elxn8rS88bI\"><img src=\"https://uploads.zulipusercontent.net/546a0e25f52bce758736f9f431828dc1b84dc82d/68747470733a2f2f692e7974696d672e636f6d2f76692f456c786e387253383862492f64656661756c742e6a7067\"></a></div></li>\n<li>Also, in a similar idea is the LIME paper, which is a particular mathematical method of pertaining transformers on a synthetic dataset.  (It reminds me of another paper where they pretrain image models on <a href=\"https://arxiv.org/abs/2103.13023\">synthetic fractals</a>.  Maybe we can get away with having pretrained language models which don't contain data, just synthetically created computational primitives.)</li>\n</ul>\n<p>As for the invited talks, I'm always excited to hear what Google (<span class=\"user-mention\" data-user-id=\"217806\">@Markus Rabe</span>) and OpenAI (<span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span>) have been up to.  The panel discussion also looks pretty interesting.</p>\n<p>Actually, I have to admit all the papers and invited talks look really interesting.  (I'm starting to really regret not taking off work to attend this virtual workshop. <span aria-label=\"frown\" class=\"emoji emoji-1f641\" role=\"img\" title=\"frown\">:frown:</span>.)</p>",
        "id": 237746498,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620344142
    },
    {
        "content": "<p>OpenAI has an interesting one on \"grokking\". The picture says it all: <a href=\"/user_uploads/3121/xHkFCKKOHYPiLFkUw4LQDHcb/grokking.png\">grokking.png</a> Wild.</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/xHkFCKKOHYPiLFkUw4LQDHcb/grokking.png\" title=\"grokking.png\"><img src=\"/user_uploads/3121/xHkFCKKOHYPiLFkUw4LQDHcb/grokking.png\"></a></div>",
        "id": 237748438,
        "sender_full_name": "Daniel Selsam",
        "timestamp": 1620345616
    },
    {
        "content": "<p>Something I'd like to mention about the REFACTOR work <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  mentioned. One of the inspiration of the work also comes from dreamcoder, where we found that the algorithm for extracting reusable components is very hard to scale to realistic problems (it's using something called fragment grammer -- check their supplementary materials for details). We instead went with a data-driven way, by creating synthetic data points to train a GNN agent for extracting reusable components. Hence it might be interesting for people who were discussing the dreamcoder paper on another thread of this stream.</p>",
        "id": 237760741,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1620356017
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"240875\">Yuhuai Tony Wu</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237760741\">said</a>:</p>\n<blockquote>\n<p>One of the inspiration of the work also comes from dreamcoder, where we found that the algorithm for extracting reusable components is very hard to scale to realistic problems</p>\n</blockquote>\n<p>Which variables did you find it scaled poorly in? The beam-search abstraction algorithm of S4.5.3 is at least nominally linear in the number of programs/proofs.</p>",
        "id": 237761489,
        "sender_full_name": "Daniel Selsam",
        "timestamp": 1620356693
    },
    {
        "content": "<p>We did some experiments with his repo: <a href=\"https://github.com/ellisk42/ec/tree/master/dreamcoder\">https://github.com/ellisk42/ec/tree/master/dreamcoder</a>, and I remember the algorithm couldn't cope with large programs/graphs, and also generated poor fragments after all. Also to me the experiments presented in the paper is quite toy-ish, I would guess this is their bottleneck.</p>",
        "id": 237763131,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1620358205
    },
    {
        "content": "<p>I also had trouble with that step in their repo.  I was running out of memory if I had more than 20 examples.  Honestly I didn’t troubleshoot it much and could have went to a higher resource machine, so I wasn’t sure if it was just user error on my part.</p>",
        "id": 237766763,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620361508
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"230999\">@Daniel Selsam</span> I wonder how that Grokking paper is related to the <a href=\"https://openai.com/blog/deep-double-descent/\">double descent phenomenon</a></p>",
        "id": 237767184,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620361956
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"240875\">Yuhuai Tony Wu</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237763131\">said</a>:</p>\n<blockquote>\n<p>We did some experiments with his repo: <a href=\"https://github.com/ellisk42/ec/tree/master/dreamcoder\">https://github.com/ellisk42/ec/tree/master/dreamcoder</a>, and I remember the algorithm couldn't cope with large programs/graphs, and also generated poor fragments after all. Also to me the experiments presented in the paper is quite toy-ish, I would guess this is their bottleneck.</p>\n</blockquote>\n<p>Do you think your REFACTOR approach could fit into a program synthesis engine (similar in purpose to DreamCoder), but one that scales beyond \"toyish\" examples?  I have been collecting problems that I think would be a good test of a DreamCoder-like-system.</p>",
        "id": 237818123,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620392426
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237818123\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"240875\">Yuhuai Tony Wu</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237763131\">said</a>:</p>\n<blockquote>\n<p>We did some experiments with his repo: <a href=\"https://github.com/ellisk42/ec/tree/master/dreamcoder\">https://github.com/ellisk42/ec/tree/master/dreamcoder</a>, and I remember the algorithm couldn't cope with large programs/graphs, and also generated poor fragments after all. Also to me the experiments presented in the paper is quite toy-ish, I would guess this is their bottleneck.</p>\n</blockquote>\n<p>Do you think your REFACTOR approach could fit into a program synthesis engine (similar in purpose to DreamCoder), but one that scales beyond \"toyish\" examples?  I have been collecting problems that I think would be a good test of a DreamCoder-like-system.</p>\n</blockquote>\n<p>Yes definitely, we started doing some early experiments with <span class=\"user-mention\" data-user-id=\"258175\">@Albert Jiang</span>, but we didn't finish the effort -- happy to chat if someone wants to continue our effort.</p>",
        "id": 237831001,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1620397411
    },
    {
        "content": "<p>How did the <del>conference</del> workshop go?  Any highlights?</p>",
        "id": 237899936,
        "sender_full_name": "Jason Rute",
        "timestamp": 1620426788
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning.20for.20Theorem.20Proving/topic/Math-AI.40ICLR/near/237746498\">said</a>:</p>\n<blockquote>\n<p>Although I would recommend looking at the <a href=\"https://arxiv.org/abs/2102.06203\">arXiv version</a> or the <a href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\">Harvard YouTube talk</a>.  Or Jesse's talk when it becomes available.)</p>\n</blockquote>\n<p>I just watched this talk. Very nice. I laughed out loud when I saw that you used my \"Human-written proof\" as the example (c.45:00) of where GPTf crushed it!</p>",
        "id": 240594554,
        "sender_full_name": "Oliver Nash",
        "timestamp": 1622203551
    },
    {
        "content": "<p>The talks for Math-AI are out now.  I haven't had a time to watch them yet.  <a href=\"https://mobile.twitter.com/Yuhu_ai_/status/1402832506177609729\">https://mobile.twitter.com/Yuhu_ai_/status/1402832506177609729</a></p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://mobile.twitter.com/Yuhu_ai_/status/1402832506177609729\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/2b2aea46b32aac6ac3e0b8b147d04b990bfe53bc/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313238313332303237383336333131393631372f64394e67673035545f6e6f726d616c2e6a7067\"></a><p>All talks and the panel discussion is now released at <a href=\"https://t.co/0guE7zh4zR\">https://iclr.cc/virtual/2021/workshop/2124</a>.\n\nThe panel discussion starts at 4:33:47. \n\nEnjoy! <a href=\"https://t.co/nOmN6D5un0\">https://twitter.com/Yuhu_ai_/status/1390649168868245504</a></p><span>- Yuhuai (Tony) Wu (@Yuhu_ai_)</span></div></div>",
        "id": 242471359,
        "sender_full_name": "Jason Rute",
        "timestamp": 1623530285
    }
]