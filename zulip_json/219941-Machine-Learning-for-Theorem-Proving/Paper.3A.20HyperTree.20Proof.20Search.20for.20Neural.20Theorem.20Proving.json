[
    {
        "content": "<p>There is a new neural theorem prover paper by Meta AI (along with Gabriel Ebner and Amaury Hayat).</p>\n<ul>\n<li>Title: HyperTree Proof Search for Neural Theorem Proving</li>\n<li>Authors: Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurélien Rodriguez, Timothée Lacroix</li>\n<li>Twitter: <a href=\"https://twitter.com/GuillaumeLample/status/1529107248013795333?cxt=HHwWioCz1Zi8vbgqAAAA\">https://twitter.com/GuillaumeLample/status/1529107248013795333?cxt=HHwWioCz1Zi8vbgqAAAA</a></li>\n<li>arXiv: <a href=\"https://arxiv.org/abs/2205.11491\">https://arxiv.org/abs/2205.11491</a></li>\n</ul>\n<p>The paper has state of the art results for the Lean MiniF2F benchmark (which you may recall from the recent OpenAI work) and on Metamath.  I notice that a number of the authors are in this Zulip.  Would they be interested in telling us more about the paper? <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/GuillaumeLample/status/1529107248013795333?cxt=HHwWioCz1Zi8vbgqAAAA\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/a77c5db4fcb26731a76147fdcee12dd94482d501/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313230343532393931363032363435383131322f5f6b6354557038735f6e6f726d616c2e6a7067\"></a><p>Excited to release our latest work: <a href=\"https://t.co/E7BdFH72El\">https://arxiv.org/abs/2205.11491</a> We present a new algorithm, HyperTree Proof Search (HTPS) inspired by the recent success of AlphaZero. Our model is able to prove mathematical theorems in a fully automated way and significantly outperforms the SOTA. 1/n <a href=\"https://t.co/KdNjFDifB3\">https://twitter.com/GuillaumeLample/status/1529107248013795333/photo/1</a></p><span>- Guillaume Lample (@GuillaumeLample)</span><div class=\"twitter-image\"><a href=\"https://t.co/KdNjFDifB3\"><img src=\"https://uploads.zulipusercontent.net/a1077e697bf9c5a9aadda8e810fdd0454ff7e8d9/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f465468577a485657414141707753552e6a70673a6d656469756d\"></a></div></div></div>",
        "id": 283824109,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653439695
    },
    {
        "content": "<p>I actually have a comparison question that maybe one of the authors or <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> could address.  In this paper it seems that they are directly comparing the GPT-f Metamath results to this paper's Metamath results, but it is not clear to me that the numbers are comparable.</p>\n<p>The comparison I'm questioning is this from the abstract:</p>\n<blockquote>\n<p>In particular, we show that with HTPS alone, a model trained on annotated proofs manages to prove 65.4% of a held-out set of Metamath theorems, significantly outperforming the previous state of the art of 56.5% by GPT-f.</p>\n</blockquote>\n<p>Here is what I gather from the two papers:</p>\n<ul>\n<li>In <a href=\"https://arxiv.org/pdf/2009.03393.pdf\">Generative Language Modeling for Automated Theorem Proving</a> (with the GPT-f model) they say: \"We split that dataset between a train set and two valid and test sets each containing ∼1k proofs sampled randomly (∼90k proof steps each).\"  The valid set seems to be the one used for expert iteration with a final pass rate of 56.5% after Expert Iteration.  The test set which is not used for expert iteration and only looked at the end has a pass rate of 56.22%.</li>\n<li>In this paper (with the Evariste model), they say: \"We first derive a graph of dependencies between statements, and generate a random train-valid-test split of theorems, with 1000 valid and test theorems. We use the DAG to ensure that each theorem in the valid or test set is not used to prove another theorem.\"  Again, the validation theorems are used for a type of iterative training (which they call \"online training\").  However, the number 65.4% posted above is for supervised learning only (on the validation set), with no online training.  If online training is added, I think it becomes an 82.6% pass rate on the validation set.</li>\n</ul>\n<p>So I'm observing two things:</p>\n<ol>\n<li>The Evariste model using their proof search, even without any form of iterative training, outperforms GPT-f which does expert iteration.  When a form of iteration (online learning) is added, it does significantly better.</li>\n<li>\n<p>However, the validation sets used for measuring success are <em>very different</em>.  The validation set for GPT-f is a random sample of theorems.  The validation set for Evariste if I understand correctly consists entirely of \"leaf theorems\" which are not used in the proof of any other theorem.</p>\n<ul>\n<li>On one hand, these \"leaf theorems\" may consist of newer theorems which have haven't had a chance to be used later and therefore are harder to prove.  Also, by using leaf theorems one is avoiding certain types of data leakage which could come from training on a theorem which comes after a test/validation theorem.</li>\n<li>On the other hand, these \"leaf theorems\" can't be lemmas (since lemmas are used to prove another theorem), and since Metamath has no automation, I assume Metamath users break up theorems into lots of lemmas.  This means (if I understand correctly), that the validation and test sets consist of lots of easier proofs which just combine all the lemmas together to get the final result.  If so, it could be that this validation set is much easier than the one used by GPT-f and that could account for the much higher pass rate.</li>\n</ul>\n</li>\n</ol>",
        "id": 283895454,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653491110
    },
    {
        "content": "<p>That seems like a fair point. But I think the Lean / miniF2F numbers are what matters really here :P I don't think they have any Metamath miniF2F numbers shared in the paper do they? That would be a good way to compare the two approaches on Metamath I presume.</p>",
        "id": 283896085,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1653491373
    },
    {
        "content": "<p>Another small typo is that there are a number of references to \"Table 6.1\" which doesn't exist.</p>",
        "id": 283896497,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653491529
    },
    {
        "content": "<p>Ah yes, the Table 6.1 is in fact the Table 1 in Section 6, not sure what happened here.. Thank you for pointing this out!</p>\n<p>Regarding the split, we experimented two different ways:<br>\n1) make the train-valid-test split fully random<br>\n2) make it with exclusively with leaves from the DAG in the valid and test</p>\n<p>We did not mention it in the paper, but we also had an extra constraint to guarantee (in both cases) that theorems in the valid/test do not have a theorem in the training set with the same statement. This can happen in Metamath as you have many theorems called xxxx and xxxxOLD with the same statement but a different proof.</p>\n<p>In early experiments, we did not see any difference of performance between the two splits, so we stuck the second one, as it simplifies things a bit at evaluation time (e.g. we never need to worry about specifying forbidden labels with the DAG). However, we did not re-evaluate this at the end as the focus was indeed to work with Lean.</p>",
        "id": 283900764,
        "sender_full_name": "Guillaume",
        "timestamp": 1653493411
    },
    {
        "content": "<p>In this paper, we feel like we pushed the numbers reasonably far, but the gap to truly \"insightful\" imo proofs in lean remains quite large despite our efforts ! Something amazing needs to happen to allow exploration as efficient as the one found in two player games. We also strongly believed in OpenAI's \"manual\" curriculum approach, but it seems statements only isn't enough...</p>",
        "id": 283902894,
        "sender_full_name": "Timothee Lacroix",
        "timestamp": 1653494295
    },
    {
        "content": "<p>Agreed &lt;3</p>",
        "id": 283903037,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1653494359
    },
    {
        "content": "<p>I'm having some trouble parsing some of the details in appendix A1:</p>\n<blockquote>\n<p>Conversely, let U ⊂ G be the set of invalid nodes. A node g ∈ G \\ U is invalid if it has been<br>\nexpanded but has no tactics in the hypergraph, or all of its tactics have an invalid child. Formally:<br>\n{(g, t, c) ∈ T } = ∅ or ∀(g, t, c) ∈ T , c ∩ I != ∅</p>\n</blockquote>\n<p>What here is <code>U</code>? It seems that this paragraph mandates that <code>U</code> is always the empty set. Is this a typo? Additionally, what is <code>I</code>? It does not appear to be defined anywhere.</p>",
        "id": 284270303,
        "sender_full_name": "Lasse",
        "timestamp": 1653808288
    },
    {
        "content": "<p>I am not an author, but yes, I assume there are some typos there. <br>\nI think <code>U</code> and <code>I</code> are supposed to be the same and refer to the set of invalid nodes, and invalidity is defined for nodes <code>g \\in G</code>.<br>\nNote that the first formula of \"Formally:\" means that the intersection of <code>T</code> with the preimage of <code>{g}</code> under the first projection is empty, and the second one makes the definition recursive (but well-foundedly).</p>",
        "id": 284302538,
        "sender_full_name": "Fabian Glöckle",
        "timestamp": 1653854172
    },
    {
        "content": "<p>(Say, the minimal set with these two properties)</p>",
        "id": 284302632,
        "sender_full_name": "Fabian Glöckle",
        "timestamp": 1653854370
    },
    {
        "content": "<p>Since we are mentioning typos:</p>\n<blockquote>\n<p>Subgoals sharing a metavariable cannot be solved in isolation. This is addressed in Polu and Sutskever [16] by using as input the entire tactic state.</p>\n</blockquote>\n<p>I think the reference you wanted to mention is the PACT paper since you are referring to Lean and tactic states.  (Disclosure, I’m an author on the PACT paper.)</p>",
        "id": 284310616,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653866205
    },
    {
        "content": "<p>Also, thanks to the authors for the response to my earlier questions.</p>",
        "id": 284312018,
        "sender_full_name": "Jason Rute",
        "timestamp": 1653868755
    },
    {
        "content": "<p>Ah, that is probably indeed how it needs to be interpreted. Thanks!</p>",
        "id": 284340055,
        "sender_full_name": "Lasse",
        "timestamp": 1653900730
    }
]