[
    {
        "content": "<p>Hi, I'm reviewing the <a href=\"https://arxiv.org/abs/2102.06203\">PACT paper</a> and I'd love to clarify a few questions I have. <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> </p>\n<p>1 - For the diagram in Figure 1, how are the expressions in the boxes generated? Also I understand the tactic proof and I can see how the proof term it generates corresponds to it and proves the theorem, but what exactly are the arrows in the syntax tree pointing at? </p>\n<p>2 - Section 3.2 describes a recursion into the structure of each proof term. Are the sub-terms simply exprs within the expr of the whole proof term? i.e. this would imply that it is a recursion into an expr-tree as defined <a href=\"https://github.com/leanprover-community/lean/blob/65ad4ffdb3abac75be748554e3cbe990fb1c6500/library/init/meta/expr.lean#L85\">here</a>?</p>",
        "id": 235899462,
        "sender_full_name": "Joe Palermo",
        "timestamp": 1619207659
    },
    {
        "content": "<p>re: 2, it's a recursion through the proofterm <code>expr</code>, but it's not quite <code>expr.mfold</code> --- we skip lambda-abstractions and instead accumulate them in a list of binders for creating synthetic tactic states later on</p>\n<p>in the <code>lean-step</code> codebase the core function is <code>expr.mfold_with_binders</code></p>",
        "id": 235907043,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1619211346
    },
    {
        "content": "<p>re 1: I agree it is a confusing picture.   Each edge represents a context + goal and the child tree of that edge is the term which completes that goal.  So let's work from the top.  Here is the tactic proof:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kd\">lemma</span> <span class=\"n\">sub_ne_zero_of_ne</span> <span class=\"o\">(</span><span class=\"n\">a</span> <span class=\"n\">b</span> <span class=\"o\">:</span> <span class=\"n\">int</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">h</span> <span class=\"o\">:</span> <span class=\"n\">a</span> <span class=\"bp\">≠</span> <span class=\"n\">b</span><span class=\"o\">)</span> <span class=\"o\">:</span> <span class=\"n\">a</span> <span class=\"bp\">-</span> <span class=\"n\">b</span> <span class=\"bp\">≠</span> <span class=\"mi\">0</span> <span class=\"o\">:=</span>\n<span class=\"kd\">begin</span>\n  <span class=\"n\">intro</span> <span class=\"n\">hab</span><span class=\"o\">,</span>\n  <span class=\"n\">apply</span> <span class=\"n\">h</span><span class=\"o\">,</span>\n  <span class=\"n\">apply</span> <span class=\"n\">int.eq_of_sub_eq_zero</span> <span class=\"n\">hab</span><span class=\"o\">,</span>\n  <span class=\"n\">tactic.trace_result</span>\n<span class=\"kd\">end</span>\n</code></pre></div>\n<p>I add the <code>tactic.trace_result</code> at the end so we can see what the proof being generated is.  It is:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">id</span> <span class=\"o\">(</span><span class=\"bp\">λ</span> <span class=\"o\">(</span><span class=\"n\">hab</span> <span class=\"o\">:</span> <span class=\"n\">a</span> <span class=\"bp\">-</span> <span class=\"n\">b</span> <span class=\"bp\">=</span> <span class=\"mi\">0</span><span class=\"o\">),</span> <span class=\"n\">h</span> <span class=\"o\">(</span><span class=\"n\">int.eq_of_sub_eq_zero</span> <span class=\"n\">hab</span><span class=\"o\">))</span>\n</code></pre></div>\n<p>Ignore the extra <code>id</code> so we get </p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"bp\">λ</span> <span class=\"o\">(</span><span class=\"n\">hab</span> <span class=\"o\">:</span> <span class=\"n\">a</span> <span class=\"bp\">-</span> <span class=\"n\">b</span> <span class=\"bp\">=</span> <span class=\"mi\">0</span><span class=\"o\">),</span> <span class=\"n\">h</span> <span class=\"o\">(</span><span class=\"n\">int.eq_of_sub_eq_zero</span> <span class=\"n\">hab</span><span class=\"o\">)</span>\n</code></pre></div>\n<p>Now, like in any mathematical formalism, this expression is a tree.  That is the tree drawn in figure one.  Specifically, the constructors are <code>lam</code> for  lambda abstraction and <code>app</code> for function application, so we can write this expression as something like</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"o\">(</span> <span class=\"n\">lam</span> <span class=\"o\">(</span><span class=\"n\">hab</span> <span class=\"o\">:</span> <span class=\"n\">a</span> <span class=\"bp\">-</span> <span class=\"n\">b</span> <span class=\"bp\">=</span> <span class=\"mi\">0</span><span class=\"o\">)</span> <span class=\"o\">(</span> <span class=\"n\">app</span> <span class=\"n\">h</span> <span class=\"o\">(</span> <span class=\"n\">app</span> <span class=\"n\">int.eq_of_sub_eq_zero</span> <span class=\"n\">hab</span> <span class=\"o\">)</span> <span class=\"o\">)</span> <span class=\"o\">)</span>\n</code></pre></div>\n<p>Now, for every subexpression (or at least the ones in the body of the lambda and on both sides of the application), we have a context and goal.  The context is what variables have occurred in outer lambda binders and the goal is the type of the subexpression.  These form the edges of the tree.</p>",
        "id": 235966494,
        "sender_full_name": "Jason Rute",
        "timestamp": 1619267102
    },
    {
        "content": "<p>Does that make sense?</p>",
        "id": 235966502,
        "sender_full_name": "Jason Rute",
        "timestamp": 1619267109
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> Thanks - I'm going to dig into the code</p>\n<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Ahh yes that makes sense. Thank you for the clarification.</p>",
        "id": 235972386,
        "sender_full_name": "Joe Palermo",
        "timestamp": 1619273102
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span> Is it fair to say that the PACT data pipeline as described in Section 3.2 is best described by <code>lean_step_main_core_aux</code>? I've been reading the code and I noticed that <code>mfold_with_binders</code> is used only in <code>gather_used_premises</code> as a step before starting <code>lean_step_main_core</code>.</p>",
        "id": 236398153,
        "sender_full_name": "Joe Palermo",
        "timestamp": 1619553272
    },
    {
        "content": "<p>oops yeah, <code>mfold_with_binders</code> was the first implementation of the idea but <code>lean_step_main_core_aux </code> is the more general version that gathers all the data i needed at once</p>",
        "id": 236415445,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1619559831
    },
    {
        "content": "<p>The paper mentions the use of the fairseq API:</p>\n<p>\"The fairseq backend supports querying a locally hosted Transformer via the Fairseq CLI (Ott et al., 2019), returning a list of candidates found by beam search, ranked by cumulative log-probabilities.\"</p>\n<p>I'm assuming you trained some models using the fairseq API as a quick baseline to compare against OpenAI's models. I'm assessing different libraries for training LMs, and I'm getting mixed signals about fairseq. I'm curious if you had problems with it or if you would recommend it. Thanks!</p>",
        "id": 236551084,
        "sender_full_name": "Joe Palermo",
        "timestamp": 1619632627
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"240875\">@Yuhuai Tony Wu</span> would know best about FairSeq.</p>",
        "id": 236660014,
        "sender_full_name": "Jason Rute",
        "timestamp": 1619697155
    },
    {
        "content": "<p>If your question is more generally which LM library to use for generating Lean term proofs, I generally don’t know enough from the practical side to be super helpful, but one concern I think you should consider is whether to start with a pretrained model.  I think it is fairly clear now that pretrained models, especially if they contain math, but even if they don’t, are good starting points.  This will of course limit the options of models and libraries you can use and limit your ability to do domain specific encoding.  Also, if you work with a model trained on the Pile or another dataset containing GitHub, you should double check if any Lean repos are in that dataset.</p>",
        "id": 236660703,
        "sender_full_name": "Jason Rute",
        "timestamp": 1619697579
    },
    {
        "content": "<p>Thanks, definitely planning to use a pre-trained model. Was planning to pre-train on LIME (<a href=\"https://arxiv.org/abs/2101.06223\">https://arxiv.org/abs/2101.06223</a>) also.</p>",
        "id": 236669788,
        "sender_full_name": "Joe Palermo",
        "timestamp": 1619701915
    },
    {
        "content": "<p>hi <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span>!  I was also reading the paper and saw one term that was used that had no reference I could use to figure out what it meant. In particular, it said: </p>\n<blockquote>\n<p>...it is possible to augment a seed dataset of human proofs with new successful trajectories using rein-forcement learning or expert iteration.<br>\nWhat is expert iteration? Is there a paper or reference to learn about it? Thanks for time in advance</p>\n</blockquote>",
        "id": 239632988,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1621533169
    },
    {
        "content": "<p>expert iteration is described in this paper: <a href=\"https://arxiv.org/abs/1705.08439\">https://arxiv.org/abs/1705.08439</a></p>",
        "id": 239633331,
        "sender_full_name": "Jesse Michael Han",
        "timestamp": 1621533291
    },
    {
        "content": "<p>I'm curious, is expert iteration not very popular? (or does not work very well) I've never heard of it before and have not seen it before mentioned by name before PACT. Does someone know? (Thanks for the reference!)</p>",
        "id": 239641818,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1621536933
    },
    {
        "content": "<p>Expert iteration can be understood as a very general concept, also known as iterated amplification and distillation.  Essentially the idea is that you start with a weak agent, and apply some amplifier (usually some search method) to obtain better data (\"expert trajectories\"), and then you distill the data back to the agent by supervised learning. AlphaGo/AlphaZero also relies on the same idea .</p>",
        "id": 239644927,
        "sender_full_name": "Yuhuai Tony Wu",
        "timestamp": 1621538294
    },
    {
        "content": "<p>Hi PACT authors! I was curious about the details of the optimizer you used. What type of optimizer did you use (+ scheduler) and what hyperparameter search did you do to find them (if any)?</p>",
        "id": 248914306,
        "sender_full_name": "Brando Miranda",
        "timestamp": 1628546500
    },
    {
        "content": "<p>The optimization strategy is described on page 6 of the <a href=\"https://arxiv.org/pdf/2102.06203.pdf\">PACT paper</a><br>\n(I am not an author of the work)</p>",
        "id": 248920590,
        "sender_full_name": "Aidan Swope",
        "timestamp": 1628550261
    },
    {
        "content": "<p>If you still have questions after looking at the paper, I think <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> would know best.</p>",
        "id": 248920920,
        "sender_full_name": "Jason Rute",
        "timestamp": 1628550518
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>  <span class=\"user-mention\" data-user-id=\"116045\">@Jesse Michael Han</span>! I was curious about the details of co-training when reading the paper. When training the model on the <code>tactic</code> and <code>mix1</code> <code>mix2</code> datasets, will the language modeling loss of the prompt part (for instace, the lm losses computed from <code>GOAL &lt;TacticState&gt; PROOFSTEP</code> part in proof step samples) get masked? Or would the model be optimized from the loss calculated on the whole sample?</p>",
        "id": 277123431,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1648636243
    },
    {
        "content": "<p>(deleted)</p>",
        "id": 277123906,
        "sender_full_name": "Jason Rute",
        "timestamp": 1648636565
    },
    {
        "content": "<p>We used the whole sample, no masking at training <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 277123971,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1648636619
    },
    {
        "content": "<p>Is it also the same for the validation loss we report in the charts?  Whole string, no masking?  <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span></p>",
        "id": 277124277,
        "sender_full_name": "Jason Rute",
        "timestamp": 1648636809
    },
    {
        "content": "<p>Validation loss is masked <span aria-label=\"+1\" class=\"emoji emoji-1f44d\" role=\"img\" title=\"+1\">:+1:</span></p>",
        "id": 277124561,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1648637015
    },
    {
        "content": "<p>A lot of thanks!</p>",
        "id": 277125930,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1648637959
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span> <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>! Thanks again for the clarification. I have another small question about the <code>tactic</code> dataset: I followed the instructions from <a href=\"https://github.com/jasonrute/lean_proof_recording\">lean_proof_recording</a> and run the full pipeline to extract the dataset. While the B.2 section of the paper said that the dataset has around 128K examples, I got over 200K examples finally. I am wondering is the difference of runtime environment causing this? Or would PACT clean/filter the generated samples before training the model on them?</p>",
        "id": 277181151,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1648662055
    },
    {
        "content": "<p>mathlib has grown since then. That's probably why</p>",
        "id": 277182536,
        "sender_full_name": "Arthur Paulino",
        "timestamp": 1648662739
    },
    {
        "content": "<p>And it seems that the local context classification task is missing from the <a href=\"https://github.com/jesse-michael-han/lean-step-public\">lean-step-public</a> repo. Is it because the released extraction script for <code>mix1</code> and <code>mix2</code> is not that up-to-date? :D</p>\n<p><em>EDIT: I realised that the <code>Proof step classification</code> task is actually the <code>local context classifcation</code> task. Sorry for the noise!</em></p>",
        "id": 277182812,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1648662866
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"451983\">Arthur Paulino</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/PACT.20Paper/near/277182536\">said</a>:</p>\n<blockquote>\n<p>mathlib has grown since then. That's probably why</p>\n</blockquote>\n<p>Ah yes! That looks reasonable.</p>",
        "id": 277182846,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1648662888
    },
    {
        "content": "<p>So the dataset size nearly doubled after one year, which may result in better model performance even without the changes in algorithms! It is exciting to see that the machine learning models somewhat grow together with humans. :D</p>",
        "id": 277183669,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1648663217
    },
    {
        "content": "<p>Hey <span class=\"user-mention\" data-user-id=\"249373\">@Stanislas Polu</span>! Recently I have been trying to reproduce the results of PACT on a GPT model trained by our lab. I am curious about the technical details about decoding strategy: is PACT using nuclear sampling? and how would temperature affect the performance?</p>",
        "id": 279490998,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1650420909
    },
    {
        "content": "<p>And we found that our model is really vulnerable to overfitting on mix1 and mix2. Did this also happen in your settings?</p>",
        "id": 279491632,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1650421571
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"489362\">@Yichen Xu</span> we generally simply sample at T=1.0; which is rather high but the diversity of samples overall helps performance we found.</p>",
        "id": 279521594,
        "sender_full_name": "Stanislas Polu",
        "timestamp": 1650448588
    },
    {
        "content": "<p>Tons of thanks! :^)</p>",
        "id": 279541638,
        "sender_full_name": "Yichen Xu",
        "timestamp": 1650460578
    }
]