[
    {
        "content": "<p>Hi all, I'm a theoretical physicist, having recently finished my PhD in quantum information theory, with a particular focus on thermodynamics/statistical mechanics and generalized entropies/the theory of majorization (<span class=\"user-mention\" data-user-id=\"311453\">@Frédéric Dupuis</span> and <span class=\"user-mention\" data-user-id=\"252300\">@Jalex Stark</span>  are two names here whose work is <em>relatively</em> close to mine). I also have an academic background in philosophy of science/physics and economics.</p>\n<p>I'm here because I'm worried about a \"replication crisis\" in my field. For my own \"proofs\" I have utilized all kinds of statements whose proof I didn't check/couldn't fully understand and peer reviewers didn't spot mistakes that I had made in my derivations. I'm not sure about the extent of this problem in different fields and the damage it incurs (there are several dynamics that should lead to a quick identification of errors in published results), but I'm intrigued by the possibility of \"simply\" attaching executable proofs in Lean to papers to reduce the chance of erroneous research being dispersed. <br>\nI'm not sure where on this forum members discussing the above gather and would be thankful about any pointers. Also happy to start a new topic/stream about more general meta-science topics.</p>",
        "id": 207499522,
        "sender_full_name": "Paul Boes",
        "timestamp": 1597912015
    },
    {
        "content": "<p>We haven't had too many discussion here about the replication crisis per se, although it's certainly something that lots of us have thought about. Kevin Buzzard (formerly a famous number theorist, now active here :-) has given a number of thought- (and controversy-!) provoking talks about it the idea of a replication crisis in pure mathematics, and you can find his talks on youtube.</p>",
        "id": 207503076,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1597914635
    },
    {
        "content": "<p>But I think a lot of people here are at least as interested in other motivations for interactive theorem provers:</p>\n<ul>\n<li>the prospect of gaining superpowers by doing mathematics with a computer as an assistant</li>\n<li>pedagogy, and teaching proofs to undergraduates</li>\n<li>a playground for machine learning research</li>\n</ul>",
        "id": 207503131,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1597914720
    },
    {
        "content": "<p>I think that in pure mathematics most people are completely unconcerned with the replication crisis to the extent that I have basically stopped going on about it now.</p>",
        "id": 207505288,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1597916575
    },
    {
        "content": "<p>I think us CS theory / physics folks have a worse replication crisis than the pure maths folks because, well, our people and our publication venues are just less skilled at proof-checking. I gave a talk at QIP 2018 where the main theorem was false. The error could be spotted as early as page 1 of the preprint, where I made a claim whose counterexample could be checked by multiplying some 9x9 matrices together in sage.</p>",
        "id": 207515998,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597926297
    },
    {
        "content": "<p>Welcome! It's always nice to see a fellow quantum information theorist here.</p>\n<p>I think in our field the problem might be bigger than in pure math, because of the different publication culture. On the CS side of the field, most results end up published as conference proceedings and not journal papers, which means that reviewers are working on tight deadlines, so if there's a mistake in say, Lemma 37 in Appendix B, the odds that it will be caught be a reviewer are pretty low. On the physics side, the most prestigious journals (Nature and friends) tend to put a lot of emphasis on marketing the result, and with more mathematical papers the main part of the text ends up being some sort of advertisement for the real thing which is relegated to the \"supplementary material\" that doesn't seem to be as carefully read by reviewers. Other journals like Physical Review Letters have extremely short page limits which also encourages corner-cutting.</p>",
        "id": 207516008,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1597926305
    },
    {
        "content": "<p>Right -- it can also happen with Theorem 1 on page 1 :-)</p>",
        "id": 207516113,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1597926398
    },
    {
        "content": "<p>I'll agree with Frédéric's point, too. I recently got a paper through nature communications, and I don't feel any more confident that it's bug-free now than before we submitted. It just didn't seem like that's what we were being evaluated on.</p>",
        "id": 207516405,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597926632
    },
    {
        "content": "<p>Yeah, papers are mostly evaluated on their importance and novelty. Officially also on correctness, but reviewers just don't have the time to do it properly. For a conference like QIP, PC members have something like 20 papers to evaluate in about one month. Some of that work can be delegated to subreviewers, but this is still a very tight deadline.</p>",
        "id": 207516866,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1597927004
    },
    {
        "content": "<p>I'm pretty sure the most efficient path to \"just attaching verifications\" to results in quantum information theory is</p>\n<ul>\n<li>get probability theory and finite-dimensional hilbert spaces into mathlib</li>\n<li>write a document that looks like the \"preliminaries\" section of a good paper, where the proofs are all \"apply a special case of something that's in mathlib\". </li>\n<li>state some of your colleagues' results in Lean, show them the statements + your preliminaries, convince them to take a stab at filling in some <code>sorry</code>s.</li>\n</ul>",
        "id": 207516977,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597927090
    },
    {
        "content": "<p>I think this is similar to Frédéric's current approach (at least, they've made some progress on the first bullet point), though I don't want to put words in their mouth.</p>",
        "id": 207517043,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597927142
    },
    {
        "content": "<p>I agree! In fact I think the \"just do it\" approach has a decent chance of working: once we're through with the first bullet point, it will become possible to formalize at least parts of the papers that we publish (i.e. self-contained lemmas, that sort of thing). Then, we can just start including them in our papers when submitting. And then you gain an advantage in the reviewing process: the paper instantly looks more credible and the reviewer has less work to do. Hopefully that will make readers curious and we'll see some of them appear here.</p>",
        "id": 207518046,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1597927762
    },
    {
        "content": "<p>I'm still quite new to cryptography but some of these issues are sounding remarkably similar in that context too...</p>",
        "id": 207537534,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1597938158
    },
    {
        "content": "<p>Certainly I have literally heard conversations essentially along the lines of \"what is the purpose of provable security if nobody reads the proofs\".</p>",
        "id": 207537594,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1597938202
    },
    {
        "content": "<p>Thanks everybody for your replies. <br>\nI agree with your reasons, Jalex and Frédéric, for why quantum info (and other more applied fields) might suffer from more of a replication crisis than than pure mathematics. At the same time, I believe that the bottom-line reaction by people in the community might be the same - lack of concern - but for different reasons: While physicists are less-skilled at proof checking, the demands on proofs are also lower. Most of the errors in proofs might never actually do any harm, since where they go wrong is not where physicists care for them to be correct, etc. I am also not aware of a major castle of sand having been burst and pulling a large chunk of papers down with it, the kind of incidence that could be used to motivate people to invest the time into coding up their proofs and attach them to their papers (I'm currently preparing a survey to be send around, where I want to get some feedback on this). </p>\n<p>This makes me believe that the possibility of a crisis itself may not be the most compelling argument for attention towards Lean et al. Potentially it would be better to focus on the notion of a search engine: \"Does there exists a theorem in the literature that shows something similar to X?\" (where X is a Lean statement). This is because a database of published theorems could be filled without the proof - just the statements of the <em>asserted</em> theorems. Collecting those statements in crowd-sourced fashion (maybe in form of a Preliminary doc accompanying the paper, as suggested by Jalex) could be significantly easier than getting people to write up the proofs. Then, once people use the search engine, they'll (hopefully) want to know whether a theorem actually holds, etc. I'm suggesting this being fully ignorant about how a useful search engine based off of Lean statements could be built. </p>\n<p>In that sense, while the \"just do it\"approach might have a chance of working, running additional strategies alongside (such as guessing theorem statements from their latex and then proactively asking authors to confirm the guess/modify it to populate a database that forms the basis of a search engine) might be viable as well. Obviously, already for this much simpler strategy, the first bullet point would have to be taken care of.</p>\n<p>Finally, as an additional note: I think another difference from applied fields to pure mathematics is that in the former the real value of prover systems could be in the \"long tail\" of theorems and lemmata that don't really show anything genuinely novel or original but rather apply a general theorem to a special case or generalize existing knowledge by an epsilon: Such proofs could be quite simple to code up in Lean, since they mostly rely on very basic building blocks, but they'd prevent the accumulation of many small errors, leading to cheap confidence. This seems to be very different from pure math, where maybe the long tail is much shorter/there's more confidence in that tail not containing errors?!</p>",
        "id": 207578240,
        "sender_full_name": "Paul Boes",
        "timestamp": 1597960001
    },
    {
        "content": "<p>For me one of the reasons to start looking at Coq, then at Lean was a week when I found mistakes in 3 papers I wanted to use: one had a typo in a non-essential lemma (I wanted to cite this lemma, had to prove a correct version instead), one paper claimed a result for <code>n≥2</code> but the proof for <code>n=2</code> was wrong, and one had a hole I still don't know how to fix (though now I have a simpler proof of the theorem). Another reason was a hole in my own master's thesis.</p>",
        "id": 207586335,
        "sender_full_name": "Yury G. Kudryashov",
        "timestamp": 1597966238
    },
    {
        "content": "<p>I guess the idea of formalizing just the abstracts of papers so you can make a search engine has been considered by thomas hales, but they haven't gotten very far in terms of public-facing work.</p>",
        "id": 207590724,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597970323
    },
    {
        "content": "<p>We talk about writing more \"formal roadmaps\" around here sometimes, this is a scaled down version of the same idea.</p>",
        "id": 207590737,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597970345
    },
    {
        "content": "<p>The issue is that you have to be really confident in your ability to write good definitions in order to write a good formalization of just the statements without the proofs.  If your definitions are extensible, play well with each other, and minimize the trivial inconveniences for proving stuff about them, they'll see use. If they're not, they won't.</p>",
        "id": 207590837,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597970458
    },
    {
        "content": "<p>In 2020, contributing directly to mathlib is probably the most efficient way to become very skilled at writing formal definitions.</p>",
        "id": 207590985,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1597970629
    },
    {
        "content": "<p>I have to say I'm a bit puzzled by this notion of having a database of formalized statements. It seems to me that to be able to state the theorems, you need the big pile of definitions that they rely on to be formalized, and once that is done, the actual proofs cannot be very far behind. Figuring out the structure of the library seems to be the tough part in what we're doing here, the actual proofs are much easier.</p>",
        "id": 207597004,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1597979738
    },
    {
        "content": "<p>\"the actual proofs cannot be very far behind\" -- this is nowhere near the truth in advanced mathematics.</p>",
        "id": 207606207,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1597994513
    },
    {
        "content": "<p>I supposed Jalex was referring to <a href=\"https://formalabstracts.github.io/\">https://formalabstracts.github.io/</a></p>",
        "id": 207608401,
        "sender_full_name": "Ruben Van de Velde",
        "timestamp": 1597996625
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span>  Yeah, maybe I went a bit far here -- of course you have things like Fermat's last theorem where the statement doesn't refer to the definitions that are relevant in the proof. But at least in the areas I work in, those statements feel very much like the exception rather than the rule.</p>",
        "id": 207633287,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1598016254
    },
    {
        "content": "<p>Yes, I've seen stabs at both modular forms and elliptic curves in Lean, but we knew what these things were 50 years before we proved FLT using them.</p>",
        "id": 207633479,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1598016363
    },
    {
        "content": "<p>Out of interest (just because this is the area of maths I care about) what are the challenges to defining elliptic curves? Or is it more that it is just simply a large amount of work to do and nobody's done it yet?</p>",
        "id": 207635402,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1598017457
    },
    {
        "content": "<p>The challenge is the group law. Depending on what generality one works in there end up being a lot of pretty huge algebraic identities to verify to show it is well defined and associative. I think Kevin and John Cremona got a version defined at lean for the curious mathematician a couple of months back for a bit of fun.</p>",
        "id": 207636465,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1598018026
    },
    {
        "content": "<p>We defined the group law, but it took a long time because the equations are too long to be easily manipulated in Lean</p>",
        "id": 207636607,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1598018100
    },
    {
        "content": "<p>One thing that would really help is a better infrastructure for verifying these identities aka having grobner bases in lean so that's a sort of sticking point. But we could definitely add some definition of elliptic curves and define the j invariant and discriminant and stuff without any of that.</p>",
        "id": 207636668,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1598018139
    },
    {
        "content": "<p>We used Groebner bases when defining addition. We wanted to take short cuts with the algebra to bring it down to a reasonable size so John did some Groebner basis calculations in sage and we copied and pasted the results into our proof.</p>",
        "id": 207639210,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1598019500
    },
    {
        "content": "<p>I'm assuming that e.g. defining isogenies would have similar issues in terms of the scale of large algebra.</p>",
        "id": 207640725,
        "sender_full_name": "Wrenna Robson",
        "timestamp": 1598020263
    },
    {
        "content": "<p>Unless you do it properly, and define an elliptic curve to be a scheme ... etc ...</p>",
        "id": 207642479,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1598021139
    },
    {
        "content": "<p>Then you don't need to manipulate any equations</p>",
        "id": 207642497,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1598021148
    },
    {
        "content": "<p>Yes but we are some way from the picard group and things it seems!</p>",
        "id": 207643230,
        "sender_full_name": "Alex J. Best",
        "timestamp": 1598021460
    },
    {
        "content": "<p>We're not far from the Picard group of a ring, and we're making progress on schemes. We'll get there in the end :-)</p>",
        "id": 207648461,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1598024005
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"331560\">Paul Boes</span> <a href=\"#narrow/stream/113489-new-members/topic/Paul.20Boes/near/207499522\">said</a>:</p>\n<blockquote>\n<p>Hi all, I'm a theoretical physicist, having recently finished my PhD in quantum information theory, with a particular focus on thermodynamics/statistical mechanics and generalized entropies/the theory of majorization (<span class=\"user-mention silent\" data-user-id=\"311453\">Frédéric Dupuis</span> and <span class=\"user-mention silent\" data-user-id=\"252300\">Jalex Stark</span>  are two names here whose work is <em>relatively</em> close to mine). I also have an academic background in philosophy of science/physics and economics.</p>\n<p>I'm here because I'm worried about a \"replication crisis\" in my field. For my own \"proofs\" I have utilized all kinds of statements whose proof I didn't check/couldn't fully understand and peer reviewers didn't spot mistakes that I had made in my derivations. I'm not sure about the extent of this problem in different fields and the damage it incurs (there are several dynamics that should lead to a quick identification of errors in published results), but I'm intrigued by the possibility of \"simply\" attaching executable proofs in Lean to papers to reduce the chance of erroneous research being dispersed. <br>\nI'm not sure where on this forum members discussing the above gather and would be thankful about any pointers. Also happy to start a new topic/stream about more general meta-science topics.</p>\n</blockquote>\n<p>Hi <span class=\"user-mention\" data-user-id=\"331560\">@Paul Boes</span> !</p>\n<p>If you ever think about building a quantum information theory Lean library and find yourself in need of an extra pair of hands please let me know! I am interested in quantum information theory, and am trying to build a Lean library up enough to start working on research questions.</p>",
        "id": 209927481,
        "sender_full_name": "Bassem Safieldeen",
        "timestamp": 1599997981
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"304606\">@Bassem Safieldeen</span>, I think the beginnings of the quantum info library are the <a href=\"https://github.com/leanprover-community/mathlib/pulls?q=is%3Apr+author%3Adupuisf\">recent PRs by Frédéric to mathlib</a>. If you want to be handy, that's probably the right place to chip in.</p>",
        "id": 209939942,
        "sender_full_name": "Jalex Stark",
        "timestamp": 1600016602
    },
    {
        "content": "<p>Will check them out, thanks!</p>",
        "id": 209940204,
        "sender_full_name": "Bassem Safieldeen",
        "timestamp": 1600016974
    },
    {
        "content": "<p>Welcome aboard, <span class=\"user-mention\" data-user-id=\"304606\">@Bassem Safieldeen</span> ! Indeed I'm interested in getting mathlib to the point where current research in quantum information can be formalized. To give you an idea of where we are at right now, we are now in process of merging complex inner product spaces (<a href=\"https://github.com/leanprover-community/mathlib/issues/4057\">#4057</a>) and eigenvalues/vectors (<a href=\"https://github.com/leanprover-community/mathlib/issues/4111\">#4111</a> and several other PRs by Alexander Bentkamp). The next step would be to get the spectral theorem based on these, and then start doing Hermitian operators, unitaries, and so on. So for the time being at least, it doesn't really make sense to do things that are specific to quantum information, we're still formalizing the basic math needed to get started.</p>",
        "id": 209942944,
        "sender_full_name": "Frédéric Dupuis",
        "timestamp": 1600020798
    },
    {
        "content": "<p>Awesome! I will try to help!</p>",
        "id": 209946575,
        "sender_full_name": "Bassem Safieldeen",
        "timestamp": 1600026026
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"304606\">@Bassem Safieldeen</span> , will definitely do (although right now it seems very unlikely). I think you've received the most promising pointers already :)</p>",
        "id": 210062983,
        "sender_full_name": "Paul Boes",
        "timestamp": 1600117397
    }
]