[
    {
        "content": "<p>Hi everyone, my name is John Burnham and I'm one of the authors of the Yatima language, <a href=\"https://github.com/yatima-inc/yatima\">https://github.com/yatima-inc/yatima</a>, a dependently-typed language featuring IPFS-based content-addressing (somewhat like the <a href=\"http://unisonweb.org\">unisonweb.org</a> language). Really interested in what you guys are doing with Lean, and I wrote a parser and IPFS content addresser for the Lean kernel (using the Lean3 export format), that I'm hoping to develop into a full independent checker in Rust (like <a href=\"https://github.com/ammkrn/nanoda_lib\">https://github.com/ammkrn/nanoda_lib</a>).  We've implemented two interesting reducers for Yatima that can likely be adapted to the Lean kernel, one based on lambda-DAG pointer graphs (Bottom-up Beta reduction: <a href=\"https://www.ccs.neu.edu/home/wand/papers/shivers-wand-10.pdf\">https://www.ccs.neu.edu/home/wand/papers/shivers-wand-10.pdf</a>), and another based on the G-machine (<a href=\"https://github.com/yatima-inc/yatima/blob/gb/typechecker-machine/core/src/machine/machine.rs\">https://github.com/yatima-inc/yatima/blob/gb/typechecker-machine/core/src/machine/machine.rs</a>).  We're curious to see how these techniques might compare performance-wise to the existing checkers.</p>\n<p>Also, we're looking at potentially modifying Yatima to align with the Lean kernel. We've received a grant from the Web3 foundation (<a href=\"https://github.com/w3f/Grants-Program/blob/master/applications/yatima.md\">https://github.com/w3f/Grants-Program/blob/master/applications/yatima.md</a>) to implement our language core on their Substrate blockchain platform (so that proofs can be checked trustlessly), and it seems like being able to use Lean in this context would be really interesting. Also, currently Yatima's inductive datatypes use self-types and lambda encodings, based on <a href=\"https://homepage.divms.uiowa.edu/~astump/papers/fu-stump-rta-tlca-14.pdf\">https://homepage.divms.uiowa.edu/~astump/papers/fu-stump-rta-tlca-14.pdf</a>, but these have some odd behavior, and Lean's approach to implementing inductive families seems much safer and better understood.</p>",
        "id": 254281574,
        "sender_full_name": "John Burnham",
        "timestamp": 1632259884
    },
    {
        "content": "<blockquote>\n<p>to implement our language core on their Substrate blockchain platform (so that proofs can be checked trustlessly)</p>\n</blockquote>\n<p>There was a discussion about block chain in <a href=\"#narrow/stream/113489-new-members/topic/Lean.20and.20blockchain/near/224006181\">a previous thread</a> that you may be interested in. Regarding \"checked trustlessly\" It's not entirely clear why its better to trust a blockchain platform instead of just trusting nanoda / trepplein / leanchecker...</p>",
        "id": 254334778,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1632300575
    },
    {
        "content": "<p>I guess one reason is that once a particular proof has been checked on the blockchain, rather than others rechecking the proof on their own computers, one could just look back at the blockchain to see that the proof was already carried out.</p>",
        "id": 254433281,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1632342353
    },
    {
        "content": "<p>I can see from the thread that there are a lot of strong opinions about blockchains here! Frankly, I think the skepticism is deserved, the field is full of specious and badly implemented projects. Here's a leaderboard of worst DeFi smart contract failures, for example: <a href=\"https://rekt.news/leaderboard/\">https://rekt.news/leaderboard/</a>. To me, though, that's what makes the idea of using formal methods interesting here. In blockchain there are programs with extremely high ratios of capital to code complexity, where tens of millions of dollars relies on the correctness of a few hundred lines.</p>",
        "id": 254440467,
        "sender_full_name": "John Burnham",
        "timestamp": 1632345194
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span>, the idea of having a global persistent cache of checked results is also really interesting to me. Of course, this is really trading the cost of typechecking for the cost of searching through a big database, but for many common/expensive theorems it might be worth it</p>",
        "id": 254448236,
        "sender_full_name": "John Burnham",
        "timestamp": 1632349124
    },
    {
        "content": "<p>One thing that is (for me) perhaps the biggest pain point with lean and mathlib is so called \"orange bar hell\". When you are editing a file in mathlib, and you realize a lemma is missing, you may want to quickly insert that lemma in a lower level file and go back to work. But because you have edited a file downstream, you must wait a long time for all the intermediate files to recompile. It doesn't necessarily have to be done with blockchain, but it would be nice if there was some kind of isolation where lean would realize that all of the lemmas you already used in your high-level file are still valid when you do this.</p>",
        "id": 254449847,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1632349838
    },
    {
        "content": "<p>Oh, does Lean not have incremental rechecking/recompilation on a declaration-by-declaration level? I think part of what I'm working on maybe could help with that. Not blockchain per se, but in Yatima we serialize and hash individual definitions (via <a href=\"http://ipld.io\">ipld.io</a>, which is json-like). If we do that in Lean and then persist a a store of \"already checked hashes\", then we could maybe skip rechecking declarations in downstream files that don't depend on the changes made</p>",
        "id": 254452273,
        "sender_full_name": "John Burnham",
        "timestamp": 1632351203
    },
    {
        "content": "<p>Rechecking can be done on a declaration-by-declaration level, but not proof construction, which uses tactics that might change their behavior depending on intermediate declarations that are not obviously referenced</p>",
        "id": 254452636,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632351391
    },
    {
        "content": "<p>But hash collision will definitely cause everything to come tumbling down. It's not that hard to prove that a hash based system is unsound</p>",
        "id": 254452825,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632351491
    },
    {
        "content": "<p>Oh, I see, it's because of the metaprogramming/elaboration. The stuff I'm doing on the export format probably isn't relevant to that then</p>",
        "id": 254452869,
        "sender_full_name": "John Burnham",
        "timestamp": 1632351514
    },
    {
        "content": "<p>I think the risk of hash collisions is negligible using a <a href=\"https://crypto.stackexchange.com/questions/39641/what-are-the-odds-of-collisions-for-a-hash-function-with-256-bit-output\">256-bit cryptographic hash</a>. Performance might be an issue, but my experience with BLAKE is that it's roughly as fast as e.g. SipHash or MurmurHash</p>",
        "id": 254453276,
        "sender_full_name": "John Burnham",
        "timestamp": 1632351701
    },
    {
        "content": "<p>Eh, I'm not a fan of risks that can't be proven to be small</p>",
        "id": 254453626,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632351869
    },
    {
        "content": "<p>They're better than the current risk that has been proven to happen, where our hashes are only 32-bit!</p>",
        "id": 254455274,
        "sender_full_name": "Eric Wieser",
        "timestamp": 1632352781
    },
    {
        "content": "<p>Assuming a valid cryptographic hash function (which is an assumption), with an N-bit digest, the number of hashes K you would need to make before you would see a collision <a href=\"https://www.solipsys.co.uk/new/TheBirthdayParadox.html?HN2\">with probability P</a> can be estimated with: </p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">K</span> <span class=\"bp\">≈</span> <span class=\"n\">sqrt</span><span class=\"o\">(</span><span class=\"bp\">-</span><span class=\"mi\">2</span> <span class=\"bp\">*</span> <span class=\"o\">(</span><span class=\"mi\">2</span><span class=\"bp\">^</span><span class=\"n\">N</span><span class=\"o\">)</span> <span class=\"bp\">*</span> <span class=\"n\">ln</span> <span class=\"o\">(</span><span class=\"n\">P</span><span class=\"o\">))</span>\n</code></pre></div>\n<p>For <code>N = 256</code> and <code>P = 10^(-10)</code>,  <a href=\"https://www.wolframalpha.com/input/?i=sqrt%28-2+*+%282%5E256%29+*+%28ln+%2810%5E-10%29%29%29\"><code>K ≈ 10^39</code></a>. So at 1 exahash (<code>10^18</code> hashes per second, the Bitcoin network is ~ 100 Terahashes (<code>10^14</code>) by comparison), we would expect to see 1 collision every <code>10^21</code> seconds, which is greater than the expected age of the universe. If this is too high we can use 512-bit hashes. A variable digest-width hash can do this without too much performance loss.</p>\n<p>It's more likely the specific cryptographic hash function we're using is somehow flawed though. This is definitely a concern, and can be mitigated by using multiple hash functions, like SHA-256, SHA-3, BLAKE3, etc, and comparing results. However, we should note that a practical attack on SHA-256 would break a <em>lot</em> of things, including, for example, both Git and Nix.  Actually Git uses SHA-1 by default, for which <a href=\"https://shattered.io/\">practical attacks already exist</a>, so if this is a concern we should make sure we're using git's <a href=\"https://stackoverflow.com/questions/65870508/git-and-sha-256\">experimental sha-256 object format</a></p>",
        "id": 254457926,
        "sender_full_name": "John Burnham",
        "timestamp": 1632354664
    },
    {
        "content": "<p>Yes, my main concern is the possibility that the hash is flawed. The existence of cryptographic hashes in the first place is an open question (which I think implies P != NP). But as a formal mathematician I am aware I am being \"pathologically untrusting\" here</p>",
        "id": 254458490,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632355071
    },
    {
        "content": "<p>If you don't like cryptographic hashes, you'll hate what I'm about to suggest next <span aria-label=\"smiling devil\" class=\"emoji emoji-1f608\" role=\"img\" title=\"smiling devil\">:smiling_devil:</span> .</p>\n<p>I think it could be fun to write a recursive SNARK to verify Lean proofs. This way, we could just throw out all the proofs in mathlib and replace them with ~500 byte, quickly verifiable cryptographic proofs that someone knew a proof at some point. Millions of mathematicians would cry out in horror at the disappearance of all semantic nuance in mathlib.</p>",
        "id": 254459368,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1632355602
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> For sure! I definitely think it's the right attitude to have, especially when it comes to proofs or cryptography. I think you're right about the existence of cryptographic hashes implying P != NP, since if P = NP, then there would be practical ways to compute pre-images. P = NP would be such a surprising result though, and even then there would still need to be practical algorithms for NP-complete problems. I mean, it's possible that that P = NP, with some ludicrously big lower bound on the polynomial.</p>",
        "id": 254459781,
        "sender_full_name": "John Burnham",
        "timestamp": 1632355902
    },
    {
        "content": "<p>I think that the probability that the blake-3 hash is broken before the end of the universe is quite high, but it is very hard to put probabilities on scientific advancement</p>",
        "id": 254459996,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632356047
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"282271\">@Bolton Bailey</span> I like the idea of using zero-knowledge proofs here. If the concept of \"proofs on the blockchain\" turns out to be viable/useful, I think that might be the way to go in the long run, so that the whole chain could be quickly reverified</p>",
        "id": 254460038,
        "sender_full_name": "John Burnham",
        "timestamp": 1632356091
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110049\">Mario Carneiro</span> <a href=\"#narrow/stream/113489-new-members/topic/Introducing.20myself.3A.20John.20Burnham/near/254459996\">said</a>:</p>\n<blockquote>\n<p>I think that the probability that the blake-3 hash is broken before the end of the universe is quite high, but it is very hard to put probabilities on scientific advancement</p>\n</blockquote>\n<p>I agree with you, but if we discovered that blake-3 was broken, couldn't we reverify with e.g. sha-3? I would also say the probability is high that at any future time <code>t</code>, there will exist <em>a</em> hash function that is cryptographic in practice (conditional on P != NP though).</p>",
        "id": 254460319,
        "sender_full_name": "John Burnham",
        "timestamp": 1632356273
    },
    {
        "content": "<p>Of course, if P = NP, we wouldn't need mathlib anyway, because we could just search for a proof in polynomial time whenever we wanted to verify something. The worst case scenario would be Impagliazzo's Pessiland world, where P != NP but one way functions don't exist.</p>",
        "id": 254462977,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1632358221
    },
    {
        "content": "<p>I forget if Impagliazzo had a world where P != NP but we couldn't prove it</p>",
        "id": 254463161,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632358356
    },
    {
        "content": "<p>the relevance of the observation that cryptographic hashes imply P != NP is that it's really hard to prove any hash has good properties</p>",
        "id": 254463218,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632358423
    },
    {
        "content": "<p>(The world even worse than any of Impagliazzo's is where P = NP but we couldn't prove it. I imagine one day Klapaucius handing us a black box that just works, but gives no clues. The rest of the world might be happy, but can you imagine how miserable the mathematicians would be?)</p>",
        "id": 254468290,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1632362433
    },
    {
        "content": "<p>IIRC if P = NP then we already know a polynomial time algorithm for SAT</p>",
        "id": 254468748,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632362705
    },
    {
        "content": "<p>One of my favorite paper titles: <a href=\"https://arxiv.org/abs/cs/0206022\">The Fastest and Shortest Algorithm for All Well-Defined Problems</a></p>",
        "id": 254468850,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1632362773
    },
    {
        "content": "<p>\"What somewhat spoils the practical applicability of M_p is the large additive constant c_p.\" :-)</p>",
        "id": 254475276,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1632368099
    }
]