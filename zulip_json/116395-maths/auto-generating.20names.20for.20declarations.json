[
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110596\">Rob Lewis</span> <a href=\"#narrow/stream/116395-maths/topic/Cayley-Hamilton/near/202184921\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p>What we really need is a linter that automatically flags if a name is wrong, and suggests to replace it with a proper one...</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/abs/2004.07761\">https://arxiv.org/abs/2004.07761</a></p>\n</blockquote>\n<p>Interesting!</p>",
        "id": 202185225,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593263108
    },
    {
        "content": "<p>It would be really nice if we had this for Lean.</p>",
        "id": 202185309,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593263234
    },
    {
        "content": "<p>We could maybe even run it on Azure...</p>",
        "id": 202185312,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593263244
    },
    {
        "content": "<p>And have it auto-fix our library for us</p>",
        "id": 202185316,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593263256
    },
    {
        "content": "<p>But I have no idea about whether this stuff lives up to its promise.</p>",
        "id": 202185322,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593263275
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/116395-maths/topic/auto-generating.20names.20for.20declarations/near/202185322\">said</a>:</p>\n<blockquote>\n<p>But I have no idea about whether this stuff lives up to its promise.</p>\n</blockquote>\n<p>I can only speculate. The challenge adapting this to Lean 3 would be accessing what they call \"syntax trees,\" the input-level pre-expressions. Otherwise I think it's just a matter of hooking things together. Maybe a version would work without the syntax trees. They write</p>\n<blockquote>\n<p>The best overall performance (BLEU = 47.2) is obtained using the multi-input model with lemma statement and chopped kernel tree as inputs</p>\n</blockquote>\n<p>I'm not sure if that's intended to exclude the syntax tree in some way.</p>",
        "id": 202185694,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1593263775
    },
    {
        "content": "<p>This is clearly above what I could work on. But I would very much cheer you on.</p>",
        "id": 202186047,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593264325
    },
    {
        "content": "<p>Not me! Look somewhere else...</p>",
        "id": 202186113,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1593264405
    },
    {
        "content": "<p>I agree with Rob, it should be easy to adapt to mathlib.  But I really can't tell how bad the results are.  It guesses the \"right\" name 10% of the time.  I believe it's enough to feed it only the elaborated expressions, the <code>ChopKnlTree+attn+copy</code> input is the second best perfoming in their paper.</p>",
        "id": 202186142,
        "sender_full_name": "Gabriel Ebner",
        "timestamp": 1593264476
    },
    {
        "content": "<p>10% is a bit sad ... I guess we wouldn't get much out of it in practice.</p>",
        "id": 202186216,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593264559
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"198375\">@Karl Palmskog</span> may want to comment here.</p>",
        "id": 202186289,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1593264636
    },
    {
        "content": "<p>Maybe a more useful application would be as a name accuracy estimator. Don't ask it to name lemmas for you, but try to identify lemmas whose names are obviously wrong. This would fit the linting framework better.</p>",
        "id": 202186419,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1593264799
    },
    {
        "content": "<p>Thing like <code>mul_lt_mul''''</code></p>",
        "id": 202186491,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593264877
    },
    {
        "content": "<div class=\"codehilite\"><pre><span></span><code>git grep &quot;&#39;&#39;&#39;&#39;&quot;\nsrc/algebra/linear_ordered_comm_group_with_zero.lean:lemma mul_lt_mul&#39;&#39;&#39;&#39; (hab : a &lt; b) (hcd : c &lt; d) : a * c &lt; b * d :=\n</code></pre></div>",
        "id": 202186515,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593264920
    },
    {
        "content": "<p>hi everyone.</p>\n<blockquote>\n<p>I'm not sure if that's intended to exclude the syntax tree in some way.</p>\n</blockquote>\n<p>Our best model for name suggestion did indeed not use the Gallina syntax trees (ASTs, \"ChopSynTree\") as input. Likely, there was a lot of overlap between the raw tokens (\"Stmt\") from Coq's lexer and the syntax trees from Coq's parser (\"ChopSynTree\") in terms of information used for naming</p>",
        "id": 202191967,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593272443
    },
    {
        "content": "<blockquote>\n<p>It guesses the \"right\" name 10% of the time. </p>\n</blockquote>\n<p>I'm guessing this is referring to the top-1 score. For perspective, MathComp lemma names are not your average lemma names to get exactly right, they can have 8-9 components and mix underscore and camelcase. To be \"top-1-correct\" every character must be present in exactly the right order.</p>\n<p>A better question to ask may be: how many of the top-1 suggested names are \"good\" or \"reasonable\"? In our manual evaluation on a project outside of our corpus, we found that around 24% of top-1 names were rated as good/useful. We believe this is good enough for top-5 suggestions from our tool to be used in pull requests, editors, and the like.</p>",
        "id": 202192133,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593272730
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"112680\">Johan Commelin</span> <a href=\"#narrow/stream/116395-maths/topic/auto-generating.20names.20for.20declarations/near/202185316\">said</a>:</p>\n<blockquote>\n<p>And have it auto-fix our library for us</p>\n</blockquote>\n<p>The output of our tool can only be as good as the training/validation data. If you want to apply this, you will have to select data with desirable naming conventions for the training, validation and test sets. We chose to train/validate/test on very specific MathComp projects in the paper since they were widely regarded as having high-quality names. If there are no agreed-on conventions for certain training/validation data, having a high top-1 percentage on a certain test set might not have any implications for when the tool is applied in practice.</p>",
        "id": 202192714,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593273607
    },
    {
        "content": "<p>This question are half-hypothetical, I won't have time in the near future to try to adapt this myself. But for someone who wanted to, how much work would it take beyond data preparation to adapt this to Lean? It would be very very easy to produce a list of mathlib lemma names with their types and bodies as trees, in whatever format is convenient. From section 5 of your paper, it looks like you're working directly on Coq datatypes for a while, right? So it's fairly far from plug-and-play?</p>",
        "id": 202192758,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1593273692
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110596\">Rob Lewis</span> <a href=\"#narrow/stream/116395-maths/topic/auto-generating.20names.20for.20declarations/near/202186419\">said</a>:</p>\n<blockquote>\n<p>Maybe a more useful application would be as a name accuracy estimator. Don't ask it to name lemmas for you, but try to identify lemmas whose names are obviously wrong. This would fit the linting framework better.</p>\n</blockquote>\n<p>This is indeed one possible application of this kind of tool. You can basically get a number on how well certain lemma names conform to the lemma names in the training/validation set.</p>",
        "id": 202192827,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593273758
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110596\">Rob Lewis</span> <a href=\"#narrow/stream/116395-maths/topic/auto-generating.20names.20for.20declarations/near/202192758\">said</a>:</p>\n<blockquote>\n<p>But for someone who wanted to, how much work would it take beyond data preparation? It would be very very easy to produce a list of mathlib lemma names with their types and bodies as trees, in whatever format is convenient. From section 5 of your paper, it looks like you're working directly on Coq datatypes for a while, right? So it's fairly far from plug-and-play?</p>\n</blockquote>\n<p>Our toolchain currently assumes input data in the form of S-expressions (since this is what we currently get from Coq). So there will be perhaps a workday or two to make our frontend work with Lean data. We also need a Lean-specific so called \"subtokenizer\" that knows how to break apart a Lean names into their components (e.g., based on underscores). It's likely you want to perform similar chopping we did on Lean kernel terms to make them into a manageable size. This might be 1-2 workdays more. Then training/validation takes a day of machine time or so. So I would pessimistically estimate a workweek or so (assuming data selection and tokens can be straightforwardly obtained)</p>",
        "id": 202193102,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593274105
    },
    {
        "content": "<p>I guess this is a bit off-topic, but since we used the same data and similar techniques: we are also presenting some preliminary work soon in the Coq Workshop July 5 on learning/predicting spacing: <a href=\"https://cozy.ece.utexas.edu/~palmskog/coqws20draft.pdf\">https://cozy.ece.utexas.edu/~palmskog/coqws20draft.pdf</a> <a href=\"https://coq-workshop.gitlab.io/2020/#talk-1-2-3\">https://coq-workshop.gitlab.io/2020/#talk-1-2-3</a></p>",
        "id": 202193320,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593274414
    },
    {
        "content": "<p>Thanks for the analysis, that's very helpful! The subtokenizing should be very easy. The rest sounds like a nice project for someone looking to do Lean+ML.</p>",
        "id": 202193322,
        "sender_full_name": "Rob Lewis",
        "timestamp": 1593274415
    },
    {
        "content": "<p>finally, the name generation tool is publicly available now <a href=\"https://github.com/EngineeringSoftware/roosterize\">https://github.com/EngineeringSoftware/roosterize</a> and so is the dataset: <a href=\"https://github.com/EngineeringSoftware/math-comp-corpus\">https://github.com/EngineeringSoftware/math-comp-corpus</a> and the paper will be presented at IJCAR July 3 in a session chaired by Leo: <a href=\"https://easychair.org/smart-program/IJCAR2020/2020-07-03.html#talk:147084\">https://easychair.org/smart-program/IJCAR2020/2020-07-03.html#talk:147084</a> --- my machine learning focused co-author, Pengyu, will likely be able to better answer ML-related questions there than I can here</p>",
        "id": 202193391,
        "sender_full_name": "Karl Palmskog",
        "timestamp": 1593274494
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"198375\">@Karl Palmskog</span> Thanks a lot for chiming in!</p>",
        "id": 202195162,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1593277068
    }
]